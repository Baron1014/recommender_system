{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d29f8d-13fb-4094-ac45-cf49ef5e22b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from imp import reload\n",
    "from dataaccessframeworks.read_data import get_movielens, user_filter, training_testing, get_yelp, get_douban, training_testing_XY\n",
    "from dataaccessframeworks.data_preprocessing import get_one_hot_feature, generate_eval_array\n",
    "from models.collaborative_filtering import get_user_item_matrix, predict\n",
    "from models.evaluation import recall_k\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "import configparser\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from util.mywandb import WandbLog\n",
    "import util.utility as util\n",
    "import itertools\n",
    "from random import sample\n",
    "from IPython.display import clear_output\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.path.dirname(os.getcwd()), 'config.ini'))\n",
    "LIBFM_PATH = '/home/baron/libfm/bin/'\n",
    "os.environ['LIBFM_PATH'] = LIBFM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1bd60d8-f070-4b2b-b73a-34d25167da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data input:\n",
    "[[user, item, rank], .....]\n",
    "'''\n",
    "def get_uij(data, users, items, sample_rate=1000):\n",
    "    for ii, user in enumerate(users):\n",
    "        items_data = data[data[:, 0]==user]\n",
    "        item_compare = list()\n",
    "        item_neg = list()\n",
    "        neg_count = 0\n",
    "        items_iter =[i for i in itertools.combinations(items, 2)]\n",
    "        items_iter = sample(items_iter, sample_rate)\n",
    "        for i, j in items_iter:\n",
    "            # if i exist items, but j not exsit items, i>j\n",
    "            if i in items_data[:, 1] and j not in items_data[:, 1]:\n",
    "                item_compare.append([user, i, j, 1])\n",
    "            # if j exist items, but i not exsit items, j>i\n",
    "            elif i not in items_data[:, 1] and j in items_data[:, 1]:\n",
    "                item_compare.append([user, j, i, 1])\n",
    "            # if i exist items, and also j exsit items, compare i and j\n",
    "            elif i in items_data[:, 1] and j in items_data[:, 1]:\n",
    "                ri = items_data[(items_data[:, 0]==user) & (items_data[:, 1]==i)][0, 2]\n",
    "                rj = items_data[(items_data[:, 0]==user) & (items_data[:, 1]==j)][0, 2]\n",
    "                if ri > rj:\n",
    "                    item_compare.append([user, i, j, 1])\n",
    "                elif ri < rj:\n",
    "                    item_compare.append([user, j, i, 1])\n",
    "                else:\n",
    "                    if neg_count < len(item_compare)//2:\n",
    "                        item_neg.append([user, j, i, 0])\n",
    "                        item_neg.append([user, i, j, 0])\n",
    "                        neg_count+=1\n",
    "            else:\n",
    "                if neg_count < len(item_compare)//2:\n",
    "                    item_neg.append([user, j, i, 0])\n",
    "                    item_neg.append([user, i, j, 0])\n",
    "                    neg_count+=1\n",
    "        if ii==0:\n",
    "            uij = np.array(item_compare)\n",
    "            uij_neg = np.array(item_neg)\n",
    "        else:\n",
    "            if len(item_compare)!= 0:\n",
    "                uij = np.vstack((uij, np.array(item_compare)))\n",
    "            if len(item_neg)!= 0:\n",
    "                uij_neg = np.vstack((uij_neg, np.array(item_neg)))\n",
    "        \n",
    "        if ii%300==0:\n",
    "            print(\"[{}/{}] uij_pos: {}, uij_neg: {}\".format(ii, len(users), uij.shape, uij_neg.shape))\n",
    "    \n",
    "    return uij, uij_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4011be-f3e9-46b9-a80e-ae726fc3e542",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc7fb39-e567-42b6-b8a5-b5fdb61739b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataaccessframeworks.data_preprocessing import get_feature_map, generate_with_feature, get_norating_data\n",
    "\n",
    "def get_uij_one_hot_feature(data, user_item_col, uij_data, y_col=3, time_col=3, batch_size=10000):\n",
    "    # 取得user及items feature map \n",
    "    users_dict, items_dict, features = get_feature_map(data, user_item_col)\n",
    "\n",
    "    # 將user item 數值轉為integer\n",
    "    # user_items = np.array([list(map(int, data))for data in data[user_item_col]])\n",
    "    # 使用者評分次數小於三筆則剔除\n",
    "    filter_data = user_filter(uij_data, 0)\n",
    "    print(filter_data.shape)\n",
    "    print(filter_data[:5])\n",
    "    # user label encoder\n",
    "    le = LabelEncoder()\n",
    "    filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "    # item label encoder\n",
    "    ile = LabelEncoder()\n",
    "    filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "    filter_data[:, 2] = ile.fit_transform(filter_data[:, 2])\n",
    "    \n",
    "    # 做特徵的onehot encoding \n",
    "    one_hot_encoder_data, y, concat_data = get_uij_onehot_encoding(filter_data, users_dict, items_dict, features, le, ile, batch_size, y_col)\n",
    "\n",
    "    return one_hot_encoder_data, y, concat_data\n",
    "\n",
    "# 取得user及items的one hot encoding map\n",
    "def get_uij_onehot_encoding(data, users_dict, items_dict, features, le, ile, batch_size, y_col):\n",
    "    #users_onehot = get_users_onehot(data)\n",
    "    sparse_, dense = get_uij_feature_onehot(data, users_dict, items_dict, features, le, ile, batch_size)\n",
    "    \n",
    "    # 取得y\n",
    "    y = data[:,y_col].reshape(-1,1)\n",
    "    \n",
    "    # return np.concatenate((sparse_, dense), axis=1), y, concat_data\n",
    "    return sparse.hstack((sparse_, dense), format='csr'), y, data\n",
    "\n",
    "# 取得feature one hot\n",
    "def get_uij_feature_onehot(data, users_feature, items_feature, features_map, le, ile, batch_size):\n",
    "    # 取得user & item個數\n",
    "    user_number = np.max(data[:,0]) + 1\n",
    "    item_number = np.max(data[:,1]) + 1\n",
    "    i_feature = items_feature[1].keys()\n",
    "    # one hot encoding\n",
    "    for b in range(0, data.shape[0], batch_size):\n",
    "        user_one_hot = np.eye(user_number)[data[b:b+batch_size,0]]\n",
    "        itemi_one_hot = np.eye(item_number)[data[b:b+batch_size,1]]\n",
    "        itemj_one_hot = np.eye(item_number)[data[b:b+batch_size,2]]\n",
    "        sparse_ = np.concatenate((user_one_hot, itemi_one_hot, itemj_one_hot), axis=1)\n",
    "        dense = np.empty((user_one_hot.shape[0], 1), int)\n",
    "\n",
    "        # create items feature \n",
    "        i_feature = items_feature[1].keys()\n",
    "        for fe in i_feature:\n",
    "            # sparse\n",
    "            if fe.split(\"_\")[1] != 'year':\n",
    "                f_map = features_map[fe]\n",
    "                feature_lengh = f_map[list(f_map.keys())[0]].shape[1]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), feature_lengh*2))\n",
    "                for i, item_ij in enumerate(data[b:b+batch_size, 1:3]):\n",
    "                    item_i, item_j = item_ij\n",
    "                    item_i = ile.inverse_transform(np.array([item_i])).item()\n",
    "                    item_j = ile.inverse_transform(np.array([item_j])).item()\n",
    "                    if item_i not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, :feature_lengh] = features_map[fe][item_i].toarray()\n",
    "                    if item_j not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, feature_lengh:] = features_map[fe][item_j].toarray()\n",
    "                # sparse_ = np.concatenate((sparse_, tmp), axis=1)\n",
    "                sparse_ = np.hstack((sparse_, tmp))\n",
    "            # dense\n",
    "            else:\n",
    "                # i = 0\n",
    "                f_map = features_map[fe]\n",
    "                feature_lengh = f_map[list(f_map.keys())[0]].shape[1]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), feature_lengh*2))\n",
    "                for i, item_ij in enumerate(data[b:b+batch_size, 1:3]):\n",
    "                    item_i, item_j = item_ij\n",
    "                    item_i = ile.inverse_transform(np.array([item_i])).item()\n",
    "                    item_j = ile.inverse_transform(np.array([item_j])).item()\n",
    "                    if item_i not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, :feature_lengh] = features_map[fe][item_i].toarray()\n",
    "                    if item_j not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, feature_lengh:] = features_map[fe][item_j].toarray()\n",
    "                # dense = np.concatenate((dense, tmp), axis=1)\n",
    "                dense = np.hstack((dense, tmp))\n",
    "\n",
    "        # create user feature\n",
    "        u_feature = users_feature[1].keys()\n",
    "        for fe in u_feature:\n",
    "            # sparse\n",
    "            if fe.split(\"_\")[1] != 'age':\n",
    "                f_map = features_map[fe]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                for i, user in enumerate(data[b:b+batch_size, 0]):\n",
    "                    # i = 0\n",
    "                    user = le.inverse_transform(np.array([user])).item()\n",
    "                    if user not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # user_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp[i] = features_map[fe][user].toarray()\n",
    "                # sparse_ = np.concatenate((sparse_, tmp), axis=1)\n",
    "                sparse_ = np.hstack((sparse_, tmp))\n",
    "                \n",
    "            # dense\n",
    "            else:\n",
    "                f_map = features_map[fe]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                for i, user in enumerate(data[b:b+batch_size, 0]):\n",
    "                    # i = 0\n",
    "                    user = le.inverse_transform(np.array([user])).item()\n",
    "                    if user not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # user_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp[i] = features_map[fe][user].toarray()\n",
    "                # dense = np.concatenate((dense, tmp), axis=1)\n",
    "                dense = np.hstack((dense, tmp))\n",
    "        if b==0:\n",
    "            sparse_matrix = csr_matrix(sparse_)\n",
    "            dense_matrix = dense\n",
    "        else:\n",
    "            sparse_matrix = sparse.vstack((sparse_matrix, csr_matrix(sparse_)))\n",
    "            dense_matrix = np.vstack((dense_matrix, dense))\n",
    "        print(\"[{}/{}] sparse_matrix shape is {}\".format(b, data.shape[0], sparse_matrix.shape))\n",
    "    \n",
    "    return sparse_matrix, dense_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682edb3-cccb-4711-b0da-fa7754ae3070",
   "metadata": {},
   "source": [
    "### MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de72e1bc-5f21-45ac-aeb9-272d3b19aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_movie:[['196' '242' '3']\n",
      " ['186' '302' '3']\n",
      " ['22' '377' '1']]\n",
      "movie_genre:[['1' '3']\n",
      " ['1' '4']\n",
      " ['1' '5']]\n",
      "user_age:[['1' '3']\n",
      " ['2' '6']\n",
      " ['3' '3']]\n",
      "user_occupation:[['1' '1']\n",
      " ['2' '2']\n",
      " ['3' '3']]\n",
      "使用者評分大於三次的共有：(100000, 3)\n",
      "users:  943\n",
      "items:  1682\n",
      "(100000, 3)\n",
      "[0/194300] sparse_matrix shape is (100000, 2666)\n",
      "[100000/194300] sparse_matrix shape is (194300, 2666)\n",
      "(155440, 2676) (38860, 2676)\n",
      "(194300, 2676)\n"
     ]
    }
   ],
   "source": [
    "data = get_movielens()\n",
    "# str to int\n",
    "user_movie = np.array([list(map(int, data)) for data in data['user_movie']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_movie, 0)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 是否加上假資料\n",
    "fake=True\n",
    "if fake:\n",
    "    # 取得加上使用者未評分的sample假資料\n",
    "    filter_data = get_norating_data(filter_data)\n",
    "    \n",
    "# 取得電影個數及電影個數\n",
    "len_users, movies = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "training_data,  testing_data = training_testing(filter_data)\n",
    "\n",
    "users_dict, items_dict, features = get_feature_map(data, 'user_movie')\n",
    "movielens_training_df = generate_with_feature(training_data, users_dict, items_dict, init_col=[\"user\", \"movie\", \"rating\"])\n",
    "movielens_testing_df = generate_with_feature(testing_data, users_dict, items_dict, init_col=[\"user\", \"movie\", \"rating\"])\n",
    "\n",
    "\n",
    "\n",
    "# normalize rating value\n",
    "# training_data[:, 2:3] = normalize(training_data[:, 2:3], axis=0)\n",
    "# testing_data[:, 2:3] = normalize(testing_data[:, 2:3], axis=0)\n",
    "# train_min = training_data[:, 2:3].min()\n",
    "# train_max = training_data[:, 2:3].max()\n",
    "# training_rating = (training_data[:, 2] - train_min)/(train_max-train_min)\n",
    "# test_min = testing_data[:, 2:3].min()\n",
    "# test_max = testing_data[:, 2:3].max()\n",
    "# testing_rating = (testing_data[:, 2:3] - test_min)/(test_max-test_min)\n",
    "print(\"users: \", len(len_users))\n",
    "print(\"items: \", len(movies))\n",
    "\n",
    "# generarte one hot encoding\n",
    "bpr = False\n",
    "if bpr:\n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(training_data, len_users, movies)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    train_uij = np.vstack((uij_pos, uij_neg))\n",
    "    test_uij_pos, test_uij_neg = get_uij(testing_data, len_users, movies)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "    one_hot_x, y, add_fake_data = get_uij_one_hot_feature(data,  'user_movie', train_uij, batch_size=100000)\n",
    "else:\n",
    "    one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_movie', batch_size=100000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index, test_index, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(one_hot_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140b967-0b12-4efb-8752-75b341b6018f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9291be6-6e04-47d6-af61-f92487ace79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = get_yelp()\n",
    "# str to int\n",
    "user_business = np.array([list(map(int, data)) for data in data['user_business']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_business, 0)\n",
    "# user label encoder\n",
    "le = LabelEncoder()\n",
    "filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "filter_data[:, 0] += 1\n",
    "# item label encoder\n",
    "ile = LabelEncoder()\n",
    "filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "filter_data[:, 1] += 1\n",
    "# if want to inverse label \n",
    "# le.inverse_transform(yelp_training_encoder)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 是否加上假資料\n",
    "fake=True\n",
    "if fake:\n",
    "    # 取得加上使用者未評分的sample假資料\n",
    "    filter_data = get_norating_data(filter_data)\n",
    "\n",
    "# 取得business個數及users個數\n",
    "yelp_users, business = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "yelp_training_data,  yelp_testing_data = training_testing(filter_data)\n",
    "users_dict, items_dict, features = get_feature_map(data, 'user_business')\n",
    "yelp_training_df = generate_with_feature(yelp_training_data, users_dict, items_dict, init_col=[\"user\", \"business\", \"rating\"])\n",
    "yelp_testing_df = generate_with_feature(yelp_testing_data, users_dict, items_dict, init_col=[\"user\", \"business\", \"rating\"])\n",
    "\n",
    "print(\"users: \", len(yelp_users))\n",
    "print(\"items: \", len(business))\n",
    "# generarte one hot encoding\n",
    "bpr = False\n",
    "if bpr:\n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(training_data, yelp_users, business)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    train_uij = np.vstack((uij_pos, uij_neg))\n",
    "    test_uij_pos, test_uij_neg = get_uij(testing_data, yelp_users, business)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "    one_hot_x, y, add_fake_data = get_uij_one_hot_feature(data,  'user_business', train_uij)\n",
    "else:\n",
    "    one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_business')\n",
    "\n",
    "# generarte one hot encoding\n",
    "X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index_yelp, test_index_yelp, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n",
    "print(X_train_yelp.shape, X_test_yelp.shape)\n",
    "print(one_hot_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ba6de-2893-4a33-b9c2-e5a77953b129",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01661c74-3b39-49c2-a121-7002680d2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = get_douban()\n",
    "# str to int\n",
    "user_book = np.array([list(map(int, data)) for data in data['user_book']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_book, 0)\n",
    "# user label encoder\n",
    "le = LabelEncoder()\n",
    "filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "filter_data[:, 0] += 1\n",
    "# item label encoder\n",
    "ile = LabelEncoder()\n",
    "filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "filter_data[:, 1] += 1\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 是否加上假資料\n",
    "fake=True\n",
    "if fake:\n",
    "    # 取得加上使用者未評分的sample假資料\n",
    "    filter_data = get_norating_data(filter_data)\n",
    "\n",
    "# 取得business個數及users個數\n",
    "douban_users, books = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "douban_training_data,  douban_testing_data = training_testing(filter_data)\n",
    "users_dict, items_dict, features = get_feature_map(data, 'user_book')\n",
    "douban_training_df = generate_with_feature(douban_training_data, users_dict, items_dict, init_col=[\"user\", \"book\", \"rating\"])\n",
    "douban_testing_df = generate_with_feature(douban_testing_data, users_dict, items_dict, init_col=[\"user\", \"book\", \"rating\"])\n",
    "\n",
    "print(\"users: \", len(douban_users))\n",
    "print(\"items: \", len(books))\n",
    "# generarte one hot encoding\n",
    "bpr = False\n",
    "if bpr:\n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(training_data, douban_users, books)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    train_uij = np.vstack((uij_pos, uij_neg))\n",
    "    test_uij_pos, test_uij_neg = get_uij(testing_data, douban_users, books)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "    one_hot_x, y, add_fake_data = get_uij_one_hot_feature(data,  'user_book', train_uij)\n",
    "else:\n",
    "    one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_book')\n",
    "\n",
    "# generarte one hot encoding\n",
    "X_train_douban, X_test_douban, y_train_douban, y_test_douban = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index_douban, test_index_douban, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100878b3-d40a-4d90-a916-6cdde0b47339",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. User-based Collaborative Filtering (U-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550a514-2c1a-4a63-933f-ec3210319308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7207d08-17cf-46cc-836c-2a2928438f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def user_sim_score(users, items, train_data, test_data, k=int(config['CF']['user_K'])):\n",
    "    # make matrix\n",
    "    user_matrix = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "    # 計算bias\n",
    "    bias_matrix = util.get_bias(user_matrix, users, items)\n",
    "    # 計算相似度\n",
    "#     cos, pcc = util.get_sim_array(user_matrix)\n",
    "#     cosine_dis = cos -  np.identity(len(users))\n",
    "#     pcc_dis = pcc -  np.identity(len(users))\n",
    "    \n",
    "#     sim = {\"cos\":cosine_dis, \"pcc\":pcc_dis}\n",
    "    sim = [\"cos\", \"pcc\"]\n",
    "    evaluation = dict()\n",
    "    for s in sim:\n",
    "        delta_list = list()\n",
    "        predict_array = np.zeros((test_matrix.shape))\n",
    "        # sim_dis = sim[s]\n",
    "        sim_array = util.get_sim_array(user_matrix, sim=s)\n",
    "        sim_dis = sim_array -  np.identity(len(users))\n",
    "        for i in tqdm(range(len(users)), desc=f\"UCF predicting {s} score with {k}\"):\n",
    "            # Suv: 取出前K個最相似的使用者相似度 ex:K=3, output=[0.378, 0.353, 0.336]\n",
    "            Suv = heapq.nlargest(k ,sim_dis[i])\n",
    "            # 若i不存在，則跳過\n",
    "            if np.isnan(sim_dis[i]).all():\n",
    "                continue\n",
    "            # top_sim_index: 取出與使用者i最為相似的前K個使用者 ex:K=3, output=[915, 406, 214]\n",
    "            sim_dis_idx = sim_dis[i].tolist()\n",
    "            top_sim_index = list(map(sim_dis_idx.index, heapq.nlargest(k,sim_dis[i])))\n",
    "            # recall\n",
    "            prediction = list()\n",
    "            # 計算相似使用者與使用者i的評分誤差\n",
    "            for item_idx in range(len(items)):\n",
    "                # 取得使用者i的評分(ground truth)\n",
    "                rth = test_matrix[i, item_idx]\n",
    "                # 如果使用者i有進行評分，則才納入計算RMSE\n",
    "                if rth != 0:\n",
    "                    # 之後需剔除對電影m未評分的相似使用者，因此先進行複製，才不會影響下一部電影的計算\n",
    "                    copy_Suv = copy.deepcopy(Suv)\n",
    "                    # R: 若相似使用者對電影 m 有評分則進行調整\n",
    "                    R = list()\n",
    "                    # 判斷相似使用者是否對電影ｍ有評分，若有評分則將原始評分減去該使用者對電影m的bias\n",
    "                    for c, j in enumerate(top_sim_index):\n",
    "                        if  test_matrix[j, item_idx] == 0:\n",
    "                            R.append(0)\n",
    "                            copy_Suv[c] = 0\n",
    "                        else:\n",
    "                            R.append(test_matrix[j, item_idx] - bias_matrix[j, item_idx])\n",
    "                    # 如果所有相似使用者都沒評分則跳過此次計算\n",
    "                    if sum(R) != 0:\n",
    "                        # 預測使用者i對於第m部電影的評分 + 使用者i對電影m的偏差\n",
    "                        Rui = predict(copy_Suv, R) + bias_matrix[i, item_idx]\n",
    "                        # 計算square error\n",
    "                        delta_list.append(util.se(rth, Rui))\n",
    "                        # 儲存預測結果, 並取四捨五入\n",
    "                        predict_array[i, item_idx] = Rui\n",
    "        # 各評估指標\n",
    "        evaluation[f'{s}_rmse']= util.rmse(delta_list)\n",
    "        evaluation[f'{s}_recall@10'] = recall_k(test_matrix, predict_array) \n",
    "        evaluation[f'{s}_NDCG@10']=ndcg_score(test_matrix, predict_array, k=10)\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = user_sim_score(len_users, movies, training_data, testing_data)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "print(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = user_sim_score(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = user_sim_score(douban_users, books, douban_training_data, douban_testing_data)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76ff5f-9a67-4ed0-9a0f-f4f94add8de9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Item-based Collaborative Filtering (I-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b2f05-9860-4be9-b6be-88fb7e345438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "def item_sim_score(users, items, train_data, test_data, k=int(config['CF']['user_K'])):\n",
    "    # make matrix\n",
    "    user_matrix = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "    item_matrix = user_matrix.T \n",
    "    item_test = test_matrix.T\n",
    "    #item_test = sparse.csr_matrix(item_test)\n",
    "    del test_matrix\n",
    "    \n",
    "    # 計算bias\n",
    "    bias_matrix = util.get_bias(user_matrix, users, items)\n",
    "    item_bias = bias_matrix.T\n",
    "    del bias_matrix\n",
    "    del user_matrix\n",
    "    \n",
    "    # 計算相似度\n",
    "    #cos, pcc = util.get_sim_array(item_matrix)\n",
    "    #cosine_dis = cos -  np.identity(len(items))\n",
    "    #cosine_dis = sparse.csr_matrix(cosine_dis)\n",
    "    #pcc_dis = pcc -  np.identity(len(items))\n",
    "    #pcc_dis = sparse.csr_matrix(pcc_dis)\n",
    "    #sim = {\"cos\":cosine_dis, \"pcc\":pcc_dis}\n",
    "    sim = [\"cos\", \"pcc\"]\n",
    "    evaluation = dict()\n",
    "    for s in sim:\n",
    "        delta_list = list()\n",
    "        predict_array = np.zeros((item_test.shape))\n",
    "        # predict array to spase\n",
    "        predict_array = sparse.csr_matrix(predict_array)\n",
    "        sim_array = util.get_sim_array(item_matrix, sim=s)\n",
    "        sim_dis = sim_array -  np.identity(len(items))\n",
    "        # sim_dis = sim[s]\n",
    "        for i in tqdm(range(len(items)), desc=f\"ICF predicting {s} score with {k}\"):\n",
    "            # Siv: 取出前K個最相似的使用者相似度 ex:K=3, output=[0.378, 0.353, 0.336]\n",
    "            Siv = heapq.nlargest(k ,sim_dis[i])\n",
    "            # 若i不存在，則跳過\n",
    "            if np.isnan(sim_dis[i]).all():\n",
    "                continue\n",
    "            sim_dis[i][np.isnan(sim_dis[i])] = 0\n",
    "            # top_sim_index: 取出與使用者i最為相似的前K個使用者 ex:K=3, output=[915, 406, 214]\n",
    "            sim_dis_idx = sim_dis[i].tolist()\n",
    "            top_sim_index = list(map(sim_dis_idx.index, heapq.nlargest(k,sim_dis[i])))\n",
    "            # recall\n",
    "            prediction = list()\n",
    "            # 計算相似電影與電影i的評分誤差\n",
    "            for user_idx in range(len(users)):\n",
    "                # 取得項目i的評分(ground truth)\n",
    "                rth = item_test[i, user_idx]\n",
    "                # 如果使用者i有進行評分，則才納入計算RMSE\n",
    "                if rth != 0:\n",
    "                    # 之後需剔除對電影m未評分的相似使用者，因此先進行複製，才不會影響下一部電影的計算\n",
    "                    copy_Siv = copy.deepcopy(Siv)\n",
    "                    # R: 若相似使用者對電影 m 有評分則進行調整\n",
    "                    R = list()\n",
    "                    # 判斷相似使用者是否對電影ｍ有評分，若有評分則將原始評分減去該使用者對電影m的bias\n",
    "                    for c, j in enumerate(top_sim_index):\n",
    "                        if  item_test[j, user_idx] == 0:\n",
    "                            R.append(0)\n",
    "                            copy_Siv[c] = 0\n",
    "                        else:\n",
    "                            R.append(item_test[j, user_idx] - item_bias[j, user_idx])\n",
    "                    # 如果所有相似使用者都沒評分則跳過此次計算\n",
    "                    if sum(R) != 0:\n",
    "                        # 預測使用者i對於第m部電影的評分 + 使用者i對電影m的偏差\n",
    "                        Rui = predict(copy_Siv, R) + item_bias[i, user_idx]\n",
    "                        # 計算square error\n",
    "                        delta_list.append(util.se(rth, Rui))\n",
    "                        # 儲存預測結果, 並取四捨五入\n",
    "                        if np.isnan(Rui):\n",
    "                            Rui=0\n",
    "                        predict_array[i, user_idx] = Rui\n",
    "        \n",
    "        \n",
    "        # 各評估指標\n",
    "        delta_list = pd.Series(delta_list, dtype=object).fillna(0).tolist()\n",
    "        evaluation[f'{s}_rmse']= util.rmse(delta_list)\n",
    "        evaluation[f'{s}_recall@10'] = recall_k(item_test, predict_array) \n",
    "        evaluation[f'{s}_NDCG@10']=ndcg_score(item_test, predict_array.toarray(), k=10)\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"ICF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = item_sim_score(len_users, movies, training_data, testing_data)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"ICF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = item_sim_score(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"ICF\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = item_sim_score(douban_users, books, douban_training_data, douban_testing_data)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555705a2-5424-4878-8f2d-d0c87a5de4d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e38937-3e80-412a-9a71-4e68c459cf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "# 進行測試資料驗證評估\n",
    "def test(test_data, p, q, gu=False, bu=False, bi=False):\n",
    "    rmse_test = list()\n",
    "\n",
    "    for test in test_data:\n",
    "        user = test[0] - 1\n",
    "        movie = test[1] - 1\n",
    "        # 判斷是否有bias\n",
    "        if gu and bu.any() and bi.any():\n",
    "            rmse_test.append(util.se(test[2], (np.dot(p[user], q[movie]) + gu + bu[user] + bi[movie])))\n",
    "        else:\n",
    "            rmse_test.append(util.se(test[2], (np.dot(p[user], q[movie]))))\n",
    "    return util.rmse(rmse_test)\n",
    "\n",
    "def execute_matrix_factorization(users, items, train_data, test_data):\n",
    "    # 存放測試資料集的rmse結果\n",
    "    MF_bias_testing = list()\n",
    "    # init evaluation\n",
    "    evaluation = dict()\n",
    "    user_item = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "\n",
    "    # init setting global mean\n",
    "    gu= util.get_u(user_item)\n",
    "    # init setting user mean as bias\n",
    "    bu = np.array([util.get_ubias(user_item, i) - gu for i in range(len(users))])\n",
    "    # init setting items mean as bias\n",
    "    bi = np.array([util.get_ibias(user_item, m) - gu for m in range(len(items))])\n",
    "\n",
    "    # init lentent vector\n",
    "    K = int(config[\"MF\"][\"latent_vector_number\"])\n",
    "    # init user lentent matrix\n",
    "    P = np.random.uniform(low=0, high=3, size=(users.max(), K))\n",
    "    # init items lentent matrix\n",
    "    Q = np.random.uniform(low=0, high=3, size=(items.max(), K))\n",
    "\n",
    "    # parameter\n",
    "    epochs = int(config[\"MF\"][\"epochs\"])\n",
    "    alpha = float(config[\"MF\"][\"alpha\"])\n",
    "    l = float(config[\"MF\"][\"learning_rate\"])\n",
    "\n",
    "    # 更新次數, init=100\n",
    "    for epoch in range(epochs):\n",
    "        # 存放 spuare error 結果\n",
    "        se_list = list()\n",
    "        # 針對user有評分過的rating位置進行更新(User Latent Matrix)\n",
    "        for j in range(len(users)):\n",
    "            # 找出被使用者j評分過的電影\n",
    "            # movie_index = [i for i, e in enumerate(user_item[j]) if e != 0]\n",
    "            movie_index = np.nonzero(user_item[j])[0]\n",
    "            for m in movie_index:\n",
    "                # 對u 做偏微分進行ＳＧＤ更新\n",
    "                tmp_gu = gu - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(gu))\n",
    "                # 對bu 做偏微分進行ＳＧＤ更新\n",
    "                tmp_bu = bu[j] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(bu[j]))\n",
    "                # 對bi 做偏微分進行ＳＧＤ更新\n",
    "                tmp_bi = bi[m] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(bi[m]))\n",
    "                # 若user item 有值則對Q的相對欄位進行SGD更新, 將更新後user latent matrix先暫存\n",
    "                tmp = Q[m] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) * P[j] + l*(Q[m]))\n",
    "                # 更新 movie latent matrix\n",
    "                P[j] -= alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) * Q[m] + l*(P[j]))\n",
    "                # 更新 user latent matrix\n",
    "                Q[m] = tmp\n",
    "                # 更新bias\n",
    "                gu = tmp_gu\n",
    "                bu[j] = tmp_bu\n",
    "                bi[m] = tmp_bi\n",
    "                # 計算ＳＥ\n",
    "                se_list.append(util.se(user_item[j, m], (np.dot(P[j], Q[m]) + gu + bu[j] + bi[m])))\n",
    "                \n",
    "        # 進行驗證資料測試\n",
    "        MF_bias_testing.append(test(test_data, P, Q, gu, bu, bi))\n",
    "        if epoch % 9 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] gu={gu}, bu={np.mean(bu)}, bi={np.mean(bi)}, testing error={MF_bias_testing[-1]}\")\n",
    "\n",
    "    # 各評估指標\n",
    "    evaluation['rmse']= MF_bias_testing[-1]\n",
    "    evaluation['recall@10'] = recall_k(test_matrix, np.dot(P, Q.T))\n",
    "    evaluation['NDCG@10'] = ndcg_score(test_matrix, np.dot(P, Q.T))\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_matrix_factorization(len_users, movies, training_data, testing_data)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"MF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = execute_matrix_factorization(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"MF\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = execute_matrix_factorization(douban_users, books, douban_training_data, douban_testing_data)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345f5f0-cb98-436a-bc18-3d33e399f6cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. BPR-MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed929932-c906-4d20-b84c-3c8329c6d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Movielens:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3rn2yi43) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ruby-leaf-160</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/3rn2yi43\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/3rn2yi43</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_231524-3rn2yi43/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3rn2yi43). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220429_232645-1pj7lz2y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/1pj7lz2y\" target=\"_blank\">glorious-snowball-161</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_movielens\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:00<00:00, 2353.96it/s]\n",
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:00<00:00, 8973.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/943] uij_pos: (239, 4), uij_neg: (238, 4)\n",
      "[300/943] uij_pos: (28185, 4), uij_neg: (27998, 4)\n",
      "[600/943] uij_pos: (59223, 4), uij_neg: (58840, 4)\n",
      "[900/943] uij_pos: (85840, 4), uij_neg: (85286, 4)\n",
      "uij_positive: (89267, 4), uij_negative: (88690, 4)\n",
      "[0/943] uij_pos: (44, 4), uij_neg: (44, 4)\n",
      "[300/943] uij_pos: (7471, 4), uij_neg: (7316, 4)\n",
      "[600/943] uij_pos: (15455, 4), uij_neg: (15150, 4)\n",
      "[900/943] uij_pos: (22434, 4), uij_neg: (21972, 4)\n",
      "testing uij_positive: (23270, 4), testing uij_negative: (22786, 4)\n",
      "[0/100] testing error=0.5530065562885556\n",
      "[9/100] testing error=0.5530065399186914\n",
      "[18/100] testing error=0.5530065492137272\n",
      "[27/100] testing error=0.5530065526123277\n",
      "[36/100] testing error=0.5530065536114757\n",
      "[45/100] testing error=0.5530065539055203\n",
      "[54/100] testing error=0.5530065539935322\n",
      "[63/100] testing error=0.5530065540197368\n",
      "[72/100] testing error=0.553006554027163\n",
      "[81/100] testing error=0.5530065540289968\n",
      "[90/100] testing error=0.5530065540292651\n",
      "[99/100] testing error=0.5530065540291795\n",
      "943 user like item816 more than item768, score is 1.6077519788196017e-11: \n",
      "{'rmse': 0.5530065540291795, 'recall@10': 0.0004220651434499046, 'NDCG@10': 0.24915501459443967}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.24916</td></tr><tr><td>recall@10</td><td>0.00042</td></tr><tr><td>rmse</td><td>0.55301</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glorious-snowball-161</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/1pj7lz2y\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/1pj7lz2y</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_232645-1pj7lz2y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "# 進行測試資料驗證評估\n",
    "def test(test_uij, users, items, p, q, gu=False, bu=False, bi=False):\n",
    "    rmse_test = list()\n",
    "    \n",
    "    \n",
    "    for u, i, j, rank in test_uij:\n",
    "        u_idx = u - 1\n",
    "        i_idx = i - 1\n",
    "        j_idx = j - 1\n",
    "        rui = np.dot(p[u_idx], q[i_idx])\n",
    "        ruj = np.dot(p[u_idx], q[j_idx])\n",
    "        x_uij =  rui - ruj\n",
    "        # sigmoid\n",
    "        exp_x = np.exp(-x_uij)\n",
    "        y_hat = 1/(1 + np.exp(exp_x))\n",
    "        \n",
    "        rmse_test.append(util.se(y_hat, rank))\n",
    "        \n",
    "    return util.rmse(rmse_test)\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "def execute_bpr_matrix_factorization(users, items, train_data, test_data):\n",
    "    # 存放測試資料集的rmse結果\n",
    "    MF_bias_testing = list()\n",
    "    # init evaluation\n",
    "    evaluation = dict()\n",
    "    user_item = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "\n",
    "    # init setting global mean\n",
    "    gu= util.get_u(user_item)\n",
    "    # init setting user mean as bias\n",
    "    bu = np.array([util.get_ubias(user_item, i) - gu for i in range(len(users))])\n",
    "    # init setting items mean as bias\n",
    "    bi = np.array([util.get_ibias(user_item, m) - gu for m in range(len(items))])\n",
    "\n",
    "    # init lentent vector\n",
    "    K = int(config[\"MF\"][\"latent_vector_number\"])\n",
    "    # init user lentent matrix\n",
    "    # P = np.random.uniform(low=0, high=3, size=(users.max(), K))\n",
    "    P = np.random.randn(users.max(), K)/10\n",
    "    # init items lentent matrix\n",
    "    # Q = np.random.uniform(low=0, high=3, size=(items.max(), K))\n",
    "    Q = np.random.randn(items.max(), K)/10\n",
    "    \n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(train_data, users, items)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    # uij = np.vstack(uij_pos, uij_neg)\n",
    "    test_uij_pos, test_uij_neg = get_uij(test_data, users, items)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "\n",
    "    # parameter\n",
    "    epochs = int(config[\"MF\"][\"epochs\"])\n",
    "    alpha = float(config[\"MF\"][\"alpha\"])\n",
    "    l = float(config[\"MF\"][\"learning_rate\"])\n",
    "\n",
    "    # 更新次數, init=100\n",
    "    for epoch in range(epochs):\n",
    "        # 針對user有評分過的rating位置進行更新(User Latent Matrix)\n",
    "        for u, i, j, rank in uij_pos:\n",
    "            # 計算x_uij\n",
    "            # rui = np.dot(P[u], Q[i]) + gu + bu[u] + bi[i]\n",
    "            # ruj = np.dot(P[u], Q[j]) + gu + bu[u] + bi[j]\n",
    "            i_idx = i-1\n",
    "            j_idx = j-1\n",
    "            u_idx = u-1\n",
    "            rui = np.dot(P[u_idx], Q[i_idx])\n",
    "            ruj = np.dot(P[u_idx], Q[j_idx])\n",
    "            x_uij =  rui - ruj\n",
    "            \n",
    "            # sigmoid\n",
    "            exp_x = np.exp(-x_uij)\n",
    "            partial_BPR = 1/(1 + np.exp(exp_x))\n",
    "            \n",
    "            # 更新 user latent matrix\n",
    "            New_P= alpha * (partial_BPR*(Q[i_idx]-Q[j_idx]) + l*(P[u_idx]))\n",
    "            # 若user item 有值則對Q的相對欄位進行SGD更新, 將更新後user latent matrix先暫存\n",
    "            Q[i_idx] -=  alpha * (partial_BPR*P[u_idx] + l*(Q[i_idx]))\n",
    "            Q[j_idx] -=  alpha * (partial_BPR*-P[u_idx] + l*(Q[j_idx]))\n",
    "            # 更新 user latent matrix\n",
    "            P[u_idx] = New_P\n",
    "            # # 更新bias\n",
    "            # # 對u 做偏微分進行ＳＧＤ更新\n",
    "            # gu = gu - alpha * ((rui - user_item[j,m]) + l*(gu))\n",
    "            # # 對bu 做偏微分進行ＳＧＤ更新\n",
    "            # bu[j] = bu[j] - alpha * ((rui - user_item[j,m]) + l*(bu[j]))\n",
    "            # # 對bi 做偏微分進行ＳＧＤ更新\n",
    "            # bi[m] = bi[m] - alpha * ((rui - user_item[j,m]) + l*(bi[m]))\n",
    "            \n",
    "\n",
    "                \n",
    "        # 進行驗證資料測試\n",
    "        MF_bias_testing.append(test(test_uij, users, items, P, Q))\n",
    "        if epoch % 9 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] testing error={MF_bias_testing[-1]}\")\n",
    "    \n",
    "    rui = np.dot(P[u_idx], Q[i_idx])\n",
    "    ruj = np.dot(P[u_idx], Q[j_idx])\n",
    "    x_uij =  rui - ruj\n",
    "    print(\"{} user like item{} more than item{}, score is {}: \".format(u, i, j, x_uij))\n",
    "    # 各評估指標\n",
    "    evaluation['rmse']= MF_bias_testing[-1]\n",
    "    evaluation['recall@10'] = recall_k(test_matrix, np.dot(P, Q.T))\n",
    "    evaluation['NDCG@10'] = ndcg_score(test_matrix, np.dot(P, Q.T))\n",
    "    \n",
    "    return evaluation, P, Q\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"BPR-MF\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt, P, Q = execute_bpr_matrix_factorization(len_users, movies, training_data, testing_data)\n",
    "print(movie_reuslt)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_matrix_factorization(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_matrix_factorization(douban_users, books, douban_training_data, douban_testing_data)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282af117-a92e-423b-9046-37d7649708f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac852f2d-998e-4559-a13e-6918187bde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywFM\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_factorization_machine(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} FM Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "\n",
    "        # reshape y\n",
    "        y_train = y_train.reshape(1, -1)[0]\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        fm = pywFM.FM(task='regression')\n",
    "\n",
    "        model = fm.run(X_train, y_train, X_val, y_val)\n",
    "        predict_values = model.predictions\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(predict_values - y_val))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_factorization_machine(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"FM\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa178ba6-8d9b-49d5-8ffc-c6df7c4190b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6. BPR-FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d92308f-b439-47b8-8d2f-47936b751813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.7123524561238528, 'recall@10': 0.04213040476104326, 'NDCG@10': 0.29914077644948034}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.29914</td></tr><tr><td>recall@10</td><td>0.04213</td></tr><tr><td>rmse</td><td>0.71235</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glowing-dream-162</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/bfvwamad\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/bfvwamad</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_233543-bfvwamad/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pywFM\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_bpr_factorization_machine(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} FM Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.reshape(1, -1)[0]\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        fm = pywFM.FM(task='classification')\n",
    "\n",
    "        model = fm.run(X_train, y_train, X_val, y_val)\n",
    "        predict_values = model.predictions\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        print(predict_values - y_val)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"BPR-FM\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = execute_bpr_factorization_machine(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "print(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460fc07-b61b-4a62-b5bc-df381606ea0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7.GBDT+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "159f1913-085a-46f4-8cae-607b071f536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.3435173759332357, 'recall@10': 0.017182727960031178, 'NDCG@10': 0.2678576171164622}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.26786</td></tr><tr><td>recall@10</td><td>0.01718</td></tr><tr><td>rmse</td><td>1.34352</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">blooming-donkey-164</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/2w2x9hlz\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/2w2x9hlz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220430_021835-2w2x9hlz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sktools import GradientBoostingFeatureGenerator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_gbdt_lr(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} GBDT+LR Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.ravel()\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        gbf = GradientBoostingFeatureGenerator(regression=True)\n",
    "        lr = LogisticRegression()\n",
    "        pipe = Pipeline([(\"gb_features\", gbf), (\"logistic\", lr)])\n",
    "        \n",
    "        pipe.fit(X_train.toarray(), y_train)\n",
    "\n",
    "        predict_values = pipe.predict(X_val.toarray())\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"GBDT_LR\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = execute_gbdt_lr(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "print(movie_reuslt)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee77f90-24a7-4de4-bdb5-e863d29b7a2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 8. XGB-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3d45a75-fe62-49a0-9e90-0b917edb4158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.3514107576601617, 'recall@10': 0.017182727960031178, 'NDCG@10': 0.2678576171164622}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.26786</td></tr><tr><td>recall@10</td><td>0.01718</td></tr><tr><td>rmse</td><td>1.35141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">silvery-glitter-165</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/2qzhlcen\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/2qzhlcen</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220430_040000-2qzhlcen/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_xgb_lr(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} XGB+LR Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.ravel()\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        gbf = SelectFromModel(estimator=XGBRegressor(), max_features=100, threshold=-np.inf)\n",
    "        lr = LogisticRegression()\n",
    "        pipe = Pipeline([(\"xgb_features\", gbf), (\"logistic\", lr)])\n",
    "        \n",
    "        pipe.fit(X_train.toarray(), y_train)\n",
    "\n",
    "        predict_values = pipe.predict(X_val.toarray())\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"XGB_LR\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = execute_xgb_lr(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "print(movie_reuslt)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fd429-f996-4335-a26f-338efe74ee54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. NN-based RecSys Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978674b1-f0e1-44c7-b8ac-7e12b7989bb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08cd156a-7d29-43ae-94fb-423513aacc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_based_models import DeepCTRModel\n",
    "\n",
    "\n",
    "def deepfm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"DeepFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.DeepFM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"DeepFM={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def nfm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"NFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.NFM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"NFM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def dcn(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"DCN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.DCN(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"DCN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def wd(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"W&D\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.WD(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"W&D={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def ccpm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"CCPM\",\n",
    "                        reinit=True)\n",
    "    # no suppot dense\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation', 'user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.CCPM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"CCPM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def fnn(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"FNN\",\n",
    "                        reinit=True)\n",
    "    deer= DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result, _ = deer.FNN(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"FNN={result}\")\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "def ipnn(dataframe, testing_data, test_index, users, movies, inner=True, outter=False):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"IPNN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result, _ = deer.PNN(dataframe, testing_data, test_index, users, movies, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"IPNN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def opnn(dataframe, testing_data, test_index, users, movies, inner=False, outter=True):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"OPNN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result, _ = deer.PNN(dataframe, testing_data, test_index, users, movies, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"OPNN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def pin(dataframe, testing_data, test_index, users, movies, inner=True, outter=True):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"PIN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.PNN(dataframe, testing_data, test_index, users, movies, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"PIN={result}\")\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efb099-3ab2-443d-a617-f8f967e03c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7981ea-1c09-4332-9a81-bd52e6f3fbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFM={'rmse': 1.4254262683002223, 'recall@10': 0.49286467198741046, 'ndcg@10': 0.9012225480734772}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ndcg@10</td><td>0.90122</td></tr><tr><td>recall@10</td><td>0.49286</td></tr><tr><td>rmse</td><td>1.42543</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">devoted-dawn-197</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/3bt8b78p\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/3bt8b78p</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220430_232647-3bt8b78p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "# 1. FM-supported Neural Networks\n",
    "fnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 2. Product-based Neural Networks\n",
    "ipnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "opnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "pin(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 3. Convolutional Click Prediction Model \n",
    "ccpm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 4. neumf\n",
    "# 5. Wide&Deep\n",
    "wd(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 6. Deep Drossing\n",
    "dcn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 7. Neural Factorization Machine\n",
    "nfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 8. Deep Factorization Machine\n",
    "deepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# # 1. FM-supported Neural Networks\n",
    "# fnn(yelp_training_df, yelp_testing_df, test_index, yelp_users, business)\n",
    "# # 2. Product-based Neural Networks\n",
    "# ipnn(yelp_training_df, yelp_testing_df, test_index, yelp_users, business)\n",
    "# opnn(yelp_training_df, yelp_testing_df, test_index, yelp_users, business)\n",
    "# # 3. Convolutional Click Prediction Model \n",
    "# ccpm(yelp_training_df, yelp_testing_df, test_index, yelp_users, business)\n",
    "# # 4. neumf\n",
    "# # 5. Wide&Deep\n",
    "# wd(yelp_training_df, yelp_testing_df, test_index, yelp_users, business)\n",
    "# # 6. Deep Drossing\n",
    "# dcn(yelp_training_df, yelp_testing_df, test_index, yelp_users, business)\n",
    "# # 7. Neural Factorization Machine\n",
    "# nfm(yelp_training_df, yelp_testing_df, test_index, yelp_users, business)\n",
    "# # 8. Deep Factorization Machine\n",
    "# deepfm(yelp_training_df, yelp_testing_df, test_index, yelp_users, business)\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# # 1. FM-supported Neural Networks\n",
    "# fnn(douban_training_df, douban_testing_df, test_index, douban_users, books)\n",
    "# # 2. Product-based Neural Networks\n",
    "# ipnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# opnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 3. Convolutional Click Prediction Model \n",
    "# ccpm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 4. neumf\n",
    "# # 5. Wide&Deep\n",
    "# wd(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 6. Deep Drossing\n",
    "# dcn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 7. Neural Factorization Machine\n",
    "# nfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 8. Deep Factorization Machine\n",
    "# deepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f83b5-a90d-4570-b030-70937d006dfe",
   "metadata": {},
   "source": [
    "## 10. Recent NN-based RecSys Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5eaab9-115a-4f2b-bd71-7bb692e6c27d",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9cdc365-b79b-4928-9119-e1eb0723d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_based_models import DeepCTRModel\n",
    "\n",
    "def din(train_df, test_df, test_index, users, movies, watch_history = ['movie', 'movie_genre'], target=\"rating\"):\n",
    "    # run = wandb.init(project=config['general']['movielens'],\n",
    "    #                     entity=config['general']['entity'],\n",
    "    #                     group=\"DIN\",\n",
    "    #                     reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.DIN(train_df, test_df, test_index, users, movies, watch_history, target)\n",
    "    clear_output()\n",
    "    print(f\"DIN={result}\")\n",
    "    # run.finish()\n",
    "\n",
    "def xdeepfm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"xDeepFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.xDeepFM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"xDeepFM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def afm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"AFM\",\n",
    "                        reinit=True)\n",
    "    # no dense\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation', 'user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.AFM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"AFM={result}\")\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765641d-6037-4366-9e63-a8fb1cbb78c4",
   "metadata": {},
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa6504-66b8-49ab-829b-f4cb09ddd022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trasfer history items: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38860/38860 [00:02<00:00, 17112.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5 Cross Validation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trasfer history items: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 155440/155440 [00:09<00:00, 16969.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "486/486 - 336s - loss: 2.5677 - mse: 2.5677 - val_loss: 2.3579 - val_mse: 2.3579\n",
      "Epoch 2/10\n",
      "486/486 - 319s - loss: 2.1894 - mse: 2.1894 - val_loss: 2.1716 - val_mse: 2.1716\n",
      "Epoch 3/10\n",
      "486/486 - 320s - loss: 2.0409 - mse: 2.0408 - val_loss: 2.0580 - val_mse: 2.0580\n",
      "Epoch 4/10\n",
      "486/486 - 320s - loss: 1.9524 - mse: 1.9524 - val_loss: 1.9982 - val_mse: 1.9982\n",
      "Epoch 5/10\n",
      "486/486 - 320s - loss: 1.8888 - mse: 1.8887 - val_loss: 1.9650 - val_mse: 1.9650\n",
      "Epoch 6/10\n",
      "486/486 - 320s - loss: 1.8421 - mse: 1.8420 - val_loss: 1.9931 - val_mse: 1.9930\n",
      "Epoch 7/10\n"
     ]
    }
   ],
   "source": [
    "# 1. Attentional Factorization Machines\n",
    "afm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 3. xDeepFM\n",
    "xdeepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 4. Deep Interest Network\n",
    "din(movielens_training_df, movielens_testing_df, test_index, len_users, movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e206e4c-16f6-4591-a7ab-14bec64fe102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
