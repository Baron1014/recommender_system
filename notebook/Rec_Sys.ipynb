{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d29f8d-13fb-4094-ac45-cf49ef5e22b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from imp import reload\n",
    "from dataaccessframeworks.read_data import get_movielens, user_filter, training_testing, get_yelp, get_douban, training_testing_XY\n",
    "from dataaccessframeworks.data_preprocessing import get_one_hot_feature, generate_eval_array\n",
    "from models.collaborative_filtering import get_user_item_matrix, predict\n",
    "from models.evaluation import recall_k\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "import configparser\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from util.mywandb import WandbLog\n",
    "import util.utility as util\n",
    "import itertools\n",
    "from random import sample\n",
    "from IPython.display import clear_output\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.path.dirname(os.getcwd()), 'config.ini'))\n",
    "LIBFM_PATH = '/home/baron/libfm/bin/'\n",
    "os.environ['LIBFM_PATH'] = LIBFM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bd60d8-f070-4b2b-b73a-34d25167da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data input:\n",
    "[[user, item, rank], .....]\n",
    "'''\n",
    "def get_uij(data, users, items, sample_rate=1000):\n",
    "    for ii, user in enumerate(users):\n",
    "        items_data = data[data[:, 0]==user]\n",
    "        item_compare = list()\n",
    "        item_neg = list()\n",
    "        neg_count = 0\n",
    "        items_iter =[i for i in itertools.combinations(items, 2)]\n",
    "        items_iter = sample(items_iter, sample_rate)\n",
    "        for i, j in items_iter:\n",
    "            # if i exist items, but j not exsit items, i>j\n",
    "            if i in items_data[:, 1] and j not in items_data[:, 1]:\n",
    "                item_compare.append([user, i, j, 1])\n",
    "            # if j exist items, but i not exsit items, j>i\n",
    "            elif i not in items_data[:, 1] and j in items_data[:, 1]:\n",
    "                item_compare.append([user, j, i, 1])\n",
    "            # if i exist items, and also j exsit items, compare i and j\n",
    "            elif i in items_data[:, 1] and j in items_data[:, 1]:\n",
    "                ri = items_data[(items_data[:, 0]==user) & (items_data[:, 1]==i)][0, 2]\n",
    "                rj = items_data[(items_data[:, 0]==user) & (items_data[:, 1]==j)][0, 2]\n",
    "                if ri > rj:\n",
    "                    item_compare.append([user, i, j, 1])\n",
    "                elif ri < rj:\n",
    "                    item_compare.append([user, j, i, 1])\n",
    "                else:\n",
    "                    if neg_count < len(item_compare)//2:\n",
    "                        item_neg.append([user, j, i, 0])\n",
    "                        item_neg.append([user, i, j, 0])\n",
    "                        neg_count+=1\n",
    "            else:\n",
    "                if neg_count < len(item_compare)//2:\n",
    "                    item_neg.append([user, j, i, 0])\n",
    "                    item_neg.append([user, i, j, 0])\n",
    "                    neg_count+=1\n",
    "        if ii==0:\n",
    "            uij = np.array(item_compare)\n",
    "            uij_neg = np.array(item_neg)\n",
    "        else:\n",
    "            if len(item_compare)!= 0:\n",
    "                uij = np.vstack((uij, np.array(item_compare)))\n",
    "            if len(item_neg)!= 0:\n",
    "                uij_neg = np.vstack((uij_neg, np.array(item_neg)))\n",
    "        \n",
    "        if ii%300==0:\n",
    "            print(\"[{}/{}] uij_pos: {}, uij_neg: {}\".format(ii, len(users), uij.shape, uij_neg.shape))\n",
    "    \n",
    "    return uij, uij_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4011be-f3e9-46b9-a80e-ae726fc3e542",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc7fb39-e567-42b6-b8a5-b5fdb61739b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataaccessframeworks.data_preprocessing import get_feature_map, generate_with_feature, get_norating_data\n",
    "\n",
    "def get_uij_one_hot_feature(data, user_item_col, uij_data, y_col=3, time_col=3, batch_size=10000):\n",
    "    # 取得user及items feature map \n",
    "    users_dict, items_dict, features = get_feature_map(data, user_item_col)\n",
    "\n",
    "    # 將user item 數值轉為integer\n",
    "    # user_items = np.array([list(map(int, data))for data in data[user_item_col]])\n",
    "    # 使用者評分次數小於三筆則剔除\n",
    "    filter_data = user_filter(uij_data, 0)\n",
    "    print(filter_data.shape)\n",
    "    print(filter_data[:5])\n",
    "    # user label encoder\n",
    "    le = LabelEncoder()\n",
    "    filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "    # item label encoder\n",
    "    ile = LabelEncoder()\n",
    "    filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "    filter_data[:, 2] = ile.fit_transform(filter_data[:, 2])\n",
    "    \n",
    "    # 做特徵的onehot encoding \n",
    "    one_hot_encoder_data, y, concat_data = get_uij_onehot_encoding(filter_data, users_dict, items_dict, features, le, ile, batch_size, y_col)\n",
    "\n",
    "    return one_hot_encoder_data, y, concat_data\n",
    "\n",
    "# 取得user及items的one hot encoding map\n",
    "def get_uij_onehot_encoding(data, users_dict, items_dict, features, le, ile, batch_size, y_col):\n",
    "    #users_onehot = get_users_onehot(data)\n",
    "    sparse_, dense = get_uij_feature_onehot(data, users_dict, items_dict, features, le, ile, batch_size)\n",
    "    \n",
    "    # 取得y\n",
    "    y = data[:,y_col].reshape(-1,1)\n",
    "    \n",
    "    # return np.concatenate((sparse_, dense), axis=1), y, concat_data\n",
    "    return sparse.hstack((sparse_, dense), format='csr'), y, data\n",
    "\n",
    "# 取得feature one hot\n",
    "def get_uij_feature_onehot(data, users_feature, items_feature, features_map, le, ile, batch_size):\n",
    "    # 取得user & item個數\n",
    "    user_number = np.max(data[:,0]) + 1\n",
    "    item_number = np.max(data[:,1]) + 1\n",
    "    i_feature = items_feature[1].keys()\n",
    "    # one hot encoding\n",
    "    for b in range(0, data.shape[0], batch_size):\n",
    "        user_one_hot = np.eye(user_number)[data[b:b+batch_size,0]]\n",
    "        itemi_one_hot = np.eye(item_number)[data[b:b+batch_size,1]]\n",
    "        itemj_one_hot = np.eye(item_number)[data[b:b+batch_size,2]]\n",
    "        sparse_ = np.concatenate((user_one_hot, itemi_one_hot, itemj_one_hot), axis=1)\n",
    "        dense = np.empty((user_one_hot.shape[0], 1), int)\n",
    "\n",
    "        # create items feature \n",
    "        i_feature = items_feature[1].keys()\n",
    "        for fe in i_feature:\n",
    "            # sparse\n",
    "            if fe.split(\"_\")[1] != 'year':\n",
    "                f_map = features_map[fe]\n",
    "                feature_lengh = f_map[list(f_map.keys())[0]].shape[1]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), feature_lengh*2))\n",
    "                for i, item_ij in enumerate(data[b:b+batch_size, 1:3]):\n",
    "                    item_i, item_j = item_ij\n",
    "                    item_i = ile.inverse_transform(np.array([item_i])).item()\n",
    "                    item_j = ile.inverse_transform(np.array([item_j])).item()\n",
    "                    if item_i not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, :feature_lengh] = features_map[fe][item_i].toarray()\n",
    "                    if item_j not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, feature_lengh:] = features_map[fe][item_j].toarray()\n",
    "                # sparse_ = np.concatenate((sparse_, tmp), axis=1)\n",
    "                sparse_ = np.hstack((sparse_, tmp))\n",
    "            # dense\n",
    "            else:\n",
    "                # i = 0\n",
    "                f_map = features_map[fe]\n",
    "                feature_lengh = f_map[list(f_map.keys())[0]].shape[1]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), feature_lengh*2))\n",
    "                for i, item_ij in enumerate(data[b:b+batch_size, 1:3]):\n",
    "                    item_i, item_j = item_ij\n",
    "                    item_i = ile.inverse_transform(np.array([item_i])).item()\n",
    "                    item_j = ile.inverse_transform(np.array([item_j])).item()\n",
    "                    if item_i not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, :feature_lengh] = features_map[fe][item_i].toarray()\n",
    "                    if item_j not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, feature_lengh:] = features_map[fe][item_j].toarray()\n",
    "                # dense = np.concatenate((dense, tmp), axis=1)\n",
    "                dense = np.hstack((dense, tmp))\n",
    "\n",
    "        # create user feature\n",
    "        u_feature = users_feature[1].keys()\n",
    "        for fe in u_feature:\n",
    "            # sparse\n",
    "            if fe.split(\"_\")[1] != 'age':\n",
    "                f_map = features_map[fe]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                for i, user in enumerate(data[b:b+batch_size, 0]):\n",
    "                    # i = 0\n",
    "                    user = le.inverse_transform(np.array([user])).item()\n",
    "                    if user not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # user_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp[i] = features_map[fe][user].toarray()\n",
    "                # sparse_ = np.concatenate((sparse_, tmp), axis=1)\n",
    "                sparse_ = np.hstack((sparse_, tmp))\n",
    "                \n",
    "            # dense\n",
    "            else:\n",
    "                f_map = features_map[fe]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                for i, user in enumerate(data[b:b+batch_size, 0]):\n",
    "                    # i = 0\n",
    "                    user = le.inverse_transform(np.array([user])).item()\n",
    "                    if user not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # user_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp[i] = features_map[fe][user].toarray()\n",
    "                # dense = np.concatenate((dense, tmp), axis=1)\n",
    "                dense = np.hstack((dense, tmp))\n",
    "        if b==0:\n",
    "            sparse_matrix = csr_matrix(sparse_)\n",
    "            dense_matrix = dense\n",
    "        else:\n",
    "            sparse_matrix = sparse.vstack((sparse_matrix, csr_matrix(sparse_)))\n",
    "            dense_matrix = np.vstack((dense_matrix, dense))\n",
    "        print(\"[{}/{}] sparse_matrix shape is {}\".format(b, data.shape[0], sparse_matrix.shape))\n",
    "    \n",
    "    return sparse_matrix, dense_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682edb3-cccb-4711-b0da-fa7754ae3070",
   "metadata": {},
   "source": [
    "### MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de72e1bc-5f21-45ac-aeb9-272d3b19aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_movie:[['196' '242' '3']\n",
      " ['186' '302' '3']\n",
      " ['22' '377' '1']]\n",
      "movie_genre:[['1' '3']\n",
      " ['1' '4']\n",
      " ['1' '5']]\n",
      "user_age:[['1' '3']\n",
      " ['2' '6']\n",
      " ['3' '3']]\n",
      "user_occupation:[['1' '1']\n",
      " ['2' '2']\n",
      " ['3' '3']]\n",
      "使用者評分大於三次的共有：(100000, 3)\n",
      "users:  943\n",
      "items:  1682\n",
      "(100000, 3)\n",
      "[0/194300] sparse_matrix shape is (100000, 2666)\n",
      "[100000/194300] sparse_matrix shape is (194300, 2666)\n",
      "(155440, 2676) (38860, 2676)\n",
      "(194300, 2676)\n"
     ]
    }
   ],
   "source": [
    "data = get_movielens()\n",
    "# str to int\n",
    "user_movie = np.array([list(map(int, data)) for data in data['user_movie']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_movie, 0)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 是否加上假資料\n",
    "fake=True\n",
    "if fake:\n",
    "    # 取得加上使用者未評分的sample假資料\n",
    "    filter_data = get_norating_data(filter_data)\n",
    "    \n",
    "# 取得電影個數及電影個數\n",
    "len_users, movies = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "training_data,  testing_data = training_testing(filter_data)\n",
    "\n",
    "users_dict, items_dict, features = get_feature_map(data, 'user_movie')\n",
    "movielens_training_df = generate_with_feature(training_data, users_dict, items_dict, init_col=[\"user\", \"movie\", \"rating\"])\n",
    "movielens_testing_df = generate_with_feature(testing_data, users_dict, items_dict, init_col=[\"user\", \"movie\", \"rating\"])\n",
    "\n",
    "\n",
    "\n",
    "# normalize rating value\n",
    "# training_data[:, 2:3] = normalize(training_data[:, 2:3], axis=0)\n",
    "# testing_data[:, 2:3] = normalize(testing_data[:, 2:3], axis=0)\n",
    "# train_min = training_data[:, 2:3].min()\n",
    "# train_max = training_data[:, 2:3].max()\n",
    "# training_rating = (training_data[:, 2] - train_min)/(train_max-train_min)\n",
    "# test_min = testing_data[:, 2:3].min()\n",
    "# test_max = testing_data[:, 2:3].max()\n",
    "# testing_rating = (testing_data[:, 2:3] - test_min)/(test_max-test_min)\n",
    "print(\"users: \", len(len_users))\n",
    "print(\"items: \", len(movies))\n",
    "\n",
    "# generarte one hot encoding\n",
    "bpr = False\n",
    "if bpr:\n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(training_data, len_users, movies)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    train_uij = np.vstack((uij_pos, uij_neg))\n",
    "    test_uij_pos, test_uij_neg = get_uij(testing_data, len_users, movies)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "    one_hot_x, y, add_fake_data = get_uij_one_hot_feature(data,  'user_movie', train_uij, batch_size=100000)\n",
    "else:\n",
    "    one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_movie', batch_size=100000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index, test_index, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(one_hot_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140b967-0b12-4efb-8752-75b341b6018f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9291be6-6e04-47d6-af61-f92487ace79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = get_yelp()\n",
    "# str to int\n",
    "user_business = np.array([list(map(int, data)) for data in data['user_business']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_business, 0)\n",
    "# user label encoder\n",
    "le = LabelEncoder()\n",
    "filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "filter_data[:, 0] += 1\n",
    "# item label encoder\n",
    "ile = LabelEncoder()\n",
    "filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "filter_data[:, 1] += 1\n",
    "# if want to inverse label \n",
    "# le.inverse_transform(yelp_training_encoder)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 取得business個數及users個數\n",
    "yelp_users, business = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "# yelp_training_data,  yelp_testing_data = training_testing(filter_data)\n",
    "print(\"users: \", len(yelp_users))\n",
    "print(\"items: \", len(business))\n",
    "# generarte one hot encoding\n",
    "one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_business')\n",
    "X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index_yelp, test_index_yelp, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ba6de-2893-4a33-b9c2-e5a77953b129",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ad654-29d3-4430-9469-ba209cc7b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uij index for bpr\n",
    "uij_pos, uij_neg = get_uij(np.hstack((X_train, y_train)), users, items)\n",
    "print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "uij = np.vstack(uij_pos, uij_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01661c74-3b39-49c2-a121-7002680d2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = get_douban()\n",
    "# str to int\n",
    "user_book = np.array([list(map(int, data)) for data in data['user_book']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_book, 0)\n",
    "# user label encoder\n",
    "le = LabelEncoder()\n",
    "filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "filter_data[:, 0] += 1\n",
    "# item label encoder\n",
    "ile = LabelEncoder()\n",
    "filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "filter_data[:, 1] += 1\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 取得business個數及users個數\n",
    "douban_users, books = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "# douban_training_data,  douban_testing_data = training_testing(filter_data)\n",
    "print(\"users: \", len(douban_users))\n",
    "print(\"items: \", len(books))\n",
    "# generarte one hot encoding\n",
    "one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_book')\n",
    "X_train_douban, X_test_douban, y_train_douban, y_test_douban = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index_douban, test_index_douban, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100878b3-d40a-4d90-a916-6cdde0b47339",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. User-based Collaborative Filtering (U-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550a514-2c1a-4a63-933f-ec3210319308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7207d08-17cf-46cc-836c-2a2928438f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def user_sim_score(users, items, train_data, test_data, k=int(config['CF']['user_K'])):\n",
    "    # make matrix\n",
    "    user_matrix = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "    # 計算bias\n",
    "    bias_matrix = util.get_bias(user_matrix, users, items)\n",
    "    # 計算相似度\n",
    "#     cos, pcc = util.get_sim_array(user_matrix)\n",
    "#     cosine_dis = cos -  np.identity(len(users))\n",
    "#     pcc_dis = pcc -  np.identity(len(users))\n",
    "    \n",
    "#     sim = {\"cos\":cosine_dis, \"pcc\":pcc_dis}\n",
    "    sim = [\"cos\", \"pcc\"]\n",
    "    evaluation = dict()\n",
    "    for s in sim:\n",
    "        delta_list = list()\n",
    "        predict_array = np.zeros((test_matrix.shape))\n",
    "        # sim_dis = sim[s]\n",
    "        sim_array = util.get_sim_array(user_matrix, sim=s)\n",
    "        sim_dis = sim_array -  np.identity(len(users))\n",
    "        for i in tqdm(range(len(users)), desc=f\"UCF predicting {s} score with {k}\"):\n",
    "            # Suv: 取出前K個最相似的使用者相似度 ex:K=3, output=[0.378, 0.353, 0.336]\n",
    "            Suv = heapq.nlargest(k ,sim_dis[i])\n",
    "            # 若i不存在，則跳過\n",
    "            if np.isnan(sim_dis[i]).all():\n",
    "                continue\n",
    "            # top_sim_index: 取出與使用者i最為相似的前K個使用者 ex:K=3, output=[915, 406, 214]\n",
    "            sim_dis_idx = sim_dis[i].tolist()\n",
    "            top_sim_index = list(map(sim_dis_idx.index, heapq.nlargest(k,sim_dis[i])))\n",
    "            # recall\n",
    "            prediction = list()\n",
    "            # 計算相似使用者與使用者i的評分誤差\n",
    "            for item_idx in range(len(items)):\n",
    "                # 取得使用者i的評分(ground truth)\n",
    "                rth = test_matrix[i, item_idx]\n",
    "                # 如果使用者i有進行評分，則才納入計算RMSE\n",
    "                if rth != 0:\n",
    "                    # 之後需剔除對電影m未評分的相似使用者，因此先進行複製，才不會影響下一部電影的計算\n",
    "                    copy_Suv = copy.deepcopy(Suv)\n",
    "                    # R: 若相似使用者對電影 m 有評分則進行調整\n",
    "                    R = list()\n",
    "                    # 判斷相似使用者是否對電影ｍ有評分，若有評分則將原始評分減去該使用者對電影m的bias\n",
    "                    for c, j in enumerate(top_sim_index):\n",
    "                        if  test_matrix[j, item_idx] == 0:\n",
    "                            R.append(0)\n",
    "                            copy_Suv[c] = 0\n",
    "                        else:\n",
    "                            R.append(test_matrix[j, item_idx] - bias_matrix[j, item_idx])\n",
    "                    # 如果所有相似使用者都沒評分則跳過此次計算\n",
    "                    if sum(R) != 0:\n",
    "                        # 預測使用者i對於第m部電影的評分 + 使用者i對電影m的偏差\n",
    "                        Rui = predict(copy_Suv, R) + bias_matrix[i, item_idx]\n",
    "                        # 計算square error\n",
    "                        delta_list.append(util.se(rth, Rui))\n",
    "                        # 儲存預測結果, 並取四捨五入\n",
    "                        predict_array[i, item_idx] = Rui\n",
    "        # 各評估指標\n",
    "        evaluation[f'{s}_rmse']= util.rmse(delta_list)\n",
    "        evaluation[f'{s}_recall@10'] = recall_k(test_matrix, predict_array) \n",
    "        evaluation[f'{s}_NDCG@10']=ndcg_score(test_matrix, predict_array, k=10)\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = user_sim_score(len_users, movies, training_data, testing_data)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "print(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = user_sim_score(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = user_sim_score(douban_users, books, douban_training_data, douban_testing_data)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76ff5f-9a67-4ed0-9a0f-f4f94add8de9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Item-based Collaborative Filtering (I-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b2f05-9860-4be9-b6be-88fb7e345438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "def item_sim_score(users, items, train_data, test_data, k=int(config['CF']['user_K'])):\n",
    "    # make matrix\n",
    "    user_matrix = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "    item_matrix = user_matrix.T \n",
    "    item_test = test_matrix.T\n",
    "    #item_test = sparse.csr_matrix(item_test)\n",
    "    del test_matrix\n",
    "    \n",
    "    # 計算bias\n",
    "    bias_matrix = util.get_bias(user_matrix, users, items)\n",
    "    item_bias = bias_matrix.T\n",
    "    del bias_matrix\n",
    "    del user_matrix\n",
    "    \n",
    "    # 計算相似度\n",
    "    #cos, pcc = util.get_sim_array(item_matrix)\n",
    "    #cosine_dis = cos -  np.identity(len(items))\n",
    "    #cosine_dis = sparse.csr_matrix(cosine_dis)\n",
    "    #pcc_dis = pcc -  np.identity(len(items))\n",
    "    #pcc_dis = sparse.csr_matrix(pcc_dis)\n",
    "    #sim = {\"cos\":cosine_dis, \"pcc\":pcc_dis}\n",
    "    sim = [\"cos\", \"pcc\"]\n",
    "    evaluation = dict()\n",
    "    for s in sim:\n",
    "        delta_list = list()\n",
    "        predict_array = np.zeros((item_test.shape))\n",
    "        # predict array to spase\n",
    "        predict_array = sparse.csr_matrix(predict_array)\n",
    "        sim_array = util.get_sim_array(item_matrix, sim=s)\n",
    "        sim_dis = sim_array -  np.identity(len(items))\n",
    "        # sim_dis = sim[s]\n",
    "        for i in tqdm(range(len(items)), desc=f\"ICF predicting {s} score with {k}\"):\n",
    "            # Siv: 取出前K個最相似的使用者相似度 ex:K=3, output=[0.378, 0.353, 0.336]\n",
    "            Siv = heapq.nlargest(k ,sim_dis[i])\n",
    "            # 若i不存在，則跳過\n",
    "            if np.isnan(sim_dis[i]).all():\n",
    "                continue\n",
    "            sim_dis[i][np.isnan(sim_dis[i])] = 0\n",
    "            # top_sim_index: 取出與使用者i最為相似的前K個使用者 ex:K=3, output=[915, 406, 214]\n",
    "            sim_dis_idx = sim_dis[i].tolist()\n",
    "            top_sim_index = list(map(sim_dis_idx.index, heapq.nlargest(k,sim_dis[i])))\n",
    "            # recall\n",
    "            prediction = list()\n",
    "            # 計算相似電影與電影i的評分誤差\n",
    "            for user_idx in range(len(users)):\n",
    "                # 取得項目i的評分(ground truth)\n",
    "                rth = item_test[i, user_idx]\n",
    "                # 如果使用者i有進行評分，則才納入計算RMSE\n",
    "                if rth != 0:\n",
    "                    # 之後需剔除對電影m未評分的相似使用者，因此先進行複製，才不會影響下一部電影的計算\n",
    "                    copy_Siv = copy.deepcopy(Siv)\n",
    "                    # R: 若相似使用者對電影 m 有評分則進行調整\n",
    "                    R = list()\n",
    "                    # 判斷相似使用者是否對電影ｍ有評分，若有評分則將原始評分減去該使用者對電影m的bias\n",
    "                    for c, j in enumerate(top_sim_index):\n",
    "                        if  item_test[j, user_idx] == 0:\n",
    "                            R.append(0)\n",
    "                            copy_Siv[c] = 0\n",
    "                        else:\n",
    "                            R.append(item_test[j, user_idx] - item_bias[j, user_idx])\n",
    "                    # 如果所有相似使用者都沒評分則跳過此次計算\n",
    "                    if sum(R) != 0:\n",
    "                        # 預測使用者i對於第m部電影的評分 + 使用者i對電影m的偏差\n",
    "                        Rui = predict(copy_Siv, R) + item_bias[i, user_idx]\n",
    "                        # 計算square error\n",
    "                        delta_list.append(util.se(rth, Rui))\n",
    "                        # 儲存預測結果, 並取四捨五入\n",
    "                        if np.isnan(Rui):\n",
    "                            Rui=0\n",
    "                        predict_array[i, user_idx] = Rui\n",
    "        \n",
    "        \n",
    "        # 各評估指標\n",
    "        delta_list = pd.Series(delta_list, dtype=object).fillna(0).tolist()\n",
    "        evaluation[f'{s}_rmse']= util.rmse(delta_list)\n",
    "        evaluation[f'{s}_recall@10'] = recall_k(item_test, predict_array) \n",
    "        evaluation[f'{s}_NDCG@10']=ndcg_score(item_test, predict_array.toarray(), k=10)\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"ICF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = item_sim_score(len_users, movies, training_data, testing_data)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"ICF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = item_sim_score(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"ICF\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = item_sim_score(douban_users, books, douban_training_data, douban_testing_data)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555705a2-5424-4878-8f2d-d0c87a5de4d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e38937-3e80-412a-9a71-4e68c459cf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "# 進行測試資料驗證評估\n",
    "def test(test_data, p, q, gu=False, bu=False, bi=False):\n",
    "    rmse_test = list()\n",
    "\n",
    "    for test in test_data:\n",
    "        user = test[0] - 1\n",
    "        movie = test[1] - 1\n",
    "        # 判斷是否有bias\n",
    "        if gu and bu.any() and bi.any():\n",
    "            rmse_test.append(util.se(test[2], (np.dot(p[user], q[movie]) + gu + bu[user] + bi[movie])))\n",
    "        else:\n",
    "            rmse_test.append(util.se(test[2], (np.dot(p[user], q[movie]))))\n",
    "    return util.rmse(rmse_test)\n",
    "\n",
    "def execute_matrix_factorization(users, items, train_data, test_data):\n",
    "    # 存放測試資料集的rmse結果\n",
    "    MF_bias_testing = list()\n",
    "    # init evaluation\n",
    "    evaluation = dict()\n",
    "    user_item = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "\n",
    "    # init setting global mean\n",
    "    gu= util.get_u(user_item)\n",
    "    # init setting user mean as bias\n",
    "    bu = np.array([util.get_ubias(user_item, i) - gu for i in range(len(users))])\n",
    "    # init setting items mean as bias\n",
    "    bi = np.array([util.get_ibias(user_item, m) - gu for m in range(len(items))])\n",
    "\n",
    "    # init lentent vector\n",
    "    K = int(config[\"MF\"][\"latent_vector_number\"])\n",
    "    # init user lentent matrix\n",
    "    P = np.random.uniform(low=0, high=3, size=(users.max(), K))\n",
    "    # init items lentent matrix\n",
    "    Q = np.random.uniform(low=0, high=3, size=(items.max(), K))\n",
    "\n",
    "    # parameter\n",
    "    epochs = int(config[\"MF\"][\"epochs\"])\n",
    "    alpha = float(config[\"MF\"][\"alpha\"])\n",
    "    l = float(config[\"MF\"][\"learning_rate\"])\n",
    "\n",
    "    # 更新次數, init=100\n",
    "    for epoch in range(epochs):\n",
    "        # 存放 spuare error 結果\n",
    "        se_list = list()\n",
    "        # 針對user有評分過的rating位置進行更新(User Latent Matrix)\n",
    "        for j in range(len(users)):\n",
    "            # 找出被使用者j評分過的電影\n",
    "            # movie_index = [i for i, e in enumerate(user_item[j]) if e != 0]\n",
    "            movie_index = np.nonzero(user_item[j])[0]\n",
    "            for m in movie_index:\n",
    "                # 對u 做偏微分進行ＳＧＤ更新\n",
    "                tmp_gu = gu - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(gu))\n",
    "                # 對bu 做偏微分進行ＳＧＤ更新\n",
    "                tmp_bu = bu[j] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(bu[j]))\n",
    "                # 對bi 做偏微分進行ＳＧＤ更新\n",
    "                tmp_bi = bi[m] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(bi[m]))\n",
    "                # 若user item 有值則對Q的相對欄位進行SGD更新, 將更新後user latent matrix先暫存\n",
    "                tmp = Q[m] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) * P[j] + l*(Q[m]))\n",
    "                # 更新 movie latent matrix\n",
    "                P[j] -= alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) * Q[m] + l*(P[j]))\n",
    "                # 更新 user latent matrix\n",
    "                Q[m] = tmp\n",
    "                # 更新bias\n",
    "                gu = tmp_gu\n",
    "                bu[j] = tmp_bu\n",
    "                bi[m] = tmp_bi\n",
    "                # 計算ＳＥ\n",
    "                se_list.append(util.se(user_item[j, m], (np.dot(P[j], Q[m]) + gu + bu[j] + bi[m])))\n",
    "                \n",
    "        # 進行驗證資料測試\n",
    "        MF_bias_testing.append(test(test_data, P, Q, gu, bu, bi))\n",
    "        if epoch % 9 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] gu={gu}, bu={np.mean(bu)}, bi={np.mean(bi)}, testing error={MF_bias_testing[-1]}\")\n",
    "\n",
    "    # 各評估指標\n",
    "    evaluation['rmse']= MF_bias_testing[-1]\n",
    "    evaluation['recall@10'] = recall_k(test_matrix, np.dot(P, Q.T))\n",
    "    evaluation['NDCG@10'] = ndcg_score(test_matrix, np.dot(P, Q.T))\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_matrix_factorization(len_users, movies, training_data, testing_data)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"MF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = execute_matrix_factorization(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"MF\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = execute_matrix_factorization(douban_users, books, douban_training_data, douban_testing_data)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345f5f0-cb98-436a-bc18-3d33e399f6cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. BPR-MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed929932-c906-4d20-b84c-3c8329c6d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Movielens:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3rn2yi43) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ruby-leaf-160</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/3rn2yi43\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/3rn2yi43</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_231524-3rn2yi43/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3rn2yi43). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220429_232645-1pj7lz2y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/1pj7lz2y\" target=\"_blank\">glorious-snowball-161</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_movielens\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:00<00:00, 2353.96it/s]\n",
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:00<00:00, 8973.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/943] uij_pos: (239, 4), uij_neg: (238, 4)\n",
      "[300/943] uij_pos: (28185, 4), uij_neg: (27998, 4)\n",
      "[600/943] uij_pos: (59223, 4), uij_neg: (58840, 4)\n",
      "[900/943] uij_pos: (85840, 4), uij_neg: (85286, 4)\n",
      "uij_positive: (89267, 4), uij_negative: (88690, 4)\n",
      "[0/943] uij_pos: (44, 4), uij_neg: (44, 4)\n",
      "[300/943] uij_pos: (7471, 4), uij_neg: (7316, 4)\n",
      "[600/943] uij_pos: (15455, 4), uij_neg: (15150, 4)\n",
      "[900/943] uij_pos: (22434, 4), uij_neg: (21972, 4)\n",
      "testing uij_positive: (23270, 4), testing uij_negative: (22786, 4)\n",
      "[0/100] testing error=0.5530065562885556\n",
      "[9/100] testing error=0.5530065399186914\n",
      "[18/100] testing error=0.5530065492137272\n",
      "[27/100] testing error=0.5530065526123277\n",
      "[36/100] testing error=0.5530065536114757\n",
      "[45/100] testing error=0.5530065539055203\n",
      "[54/100] testing error=0.5530065539935322\n",
      "[63/100] testing error=0.5530065540197368\n",
      "[72/100] testing error=0.553006554027163\n",
      "[81/100] testing error=0.5530065540289968\n",
      "[90/100] testing error=0.5530065540292651\n",
      "[99/100] testing error=0.5530065540291795\n",
      "943 user like item816 more than item768, score is 1.6077519788196017e-11: \n",
      "{'rmse': 0.5530065540291795, 'recall@10': 0.0004220651434499046, 'NDCG@10': 0.24915501459443967}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.24916</td></tr><tr><td>recall@10</td><td>0.00042</td></tr><tr><td>rmse</td><td>0.55301</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glorious-snowball-161</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/1pj7lz2y\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/1pj7lz2y</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_232645-1pj7lz2y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "# 進行測試資料驗證評估\n",
    "def test(test_uij, users, items, p, q, gu=False, bu=False, bi=False):\n",
    "    rmse_test = list()\n",
    "    \n",
    "    \n",
    "    for u, i, j, rank in test_uij:\n",
    "        u_idx = u - 1\n",
    "        i_idx = i - 1\n",
    "        j_idx = j - 1\n",
    "        rui = np.dot(p[u_idx], q[i_idx])\n",
    "        ruj = np.dot(p[u_idx], q[j_idx])\n",
    "        x_uij =  rui - ruj\n",
    "        # sigmoid\n",
    "        exp_x = np.exp(-x_uij)\n",
    "        y_hat = 1/(1 + np.exp(exp_x))\n",
    "        \n",
    "        rmse_test.append(util.se(y_hat, rank))\n",
    "        \n",
    "    return util.rmse(rmse_test)\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "def execute_bpr_matrix_factorization(users, items, train_data, test_data):\n",
    "    # 存放測試資料集的rmse結果\n",
    "    MF_bias_testing = list()\n",
    "    # init evaluation\n",
    "    evaluation = dict()\n",
    "    user_item = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "\n",
    "    # init setting global mean\n",
    "    gu= util.get_u(user_item)\n",
    "    # init setting user mean as bias\n",
    "    bu = np.array([util.get_ubias(user_item, i) - gu for i in range(len(users))])\n",
    "    # init setting items mean as bias\n",
    "    bi = np.array([util.get_ibias(user_item, m) - gu for m in range(len(items))])\n",
    "\n",
    "    # init lentent vector\n",
    "    K = int(config[\"MF\"][\"latent_vector_number\"])\n",
    "    # init user lentent matrix\n",
    "    # P = np.random.uniform(low=0, high=3, size=(users.max(), K))\n",
    "    P = np.random.randn(users.max(), K)/10\n",
    "    # init items lentent matrix\n",
    "    # Q = np.random.uniform(low=0, high=3, size=(items.max(), K))\n",
    "    Q = np.random.randn(items.max(), K)/10\n",
    "    \n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(train_data, users, items)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    # uij = np.vstack(uij_pos, uij_neg)\n",
    "    test_uij_pos, test_uij_neg = get_uij(test_data, users, items)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "\n",
    "    # parameter\n",
    "    epochs = int(config[\"MF\"][\"epochs\"])\n",
    "    alpha = float(config[\"MF\"][\"alpha\"])\n",
    "    l = float(config[\"MF\"][\"learning_rate\"])\n",
    "\n",
    "    # 更新次數, init=100\n",
    "    for epoch in range(epochs):\n",
    "        # 針對user有評分過的rating位置進行更新(User Latent Matrix)\n",
    "        for u, i, j, rank in uij_pos:\n",
    "            # 計算x_uij\n",
    "            # rui = np.dot(P[u], Q[i]) + gu + bu[u] + bi[i]\n",
    "            # ruj = np.dot(P[u], Q[j]) + gu + bu[u] + bi[j]\n",
    "            i_idx = i-1\n",
    "            j_idx = j-1\n",
    "            u_idx = u-1\n",
    "            rui = np.dot(P[u_idx], Q[i_idx])\n",
    "            ruj = np.dot(P[u_idx], Q[j_idx])\n",
    "            x_uij =  rui - ruj\n",
    "            \n",
    "            # sigmoid\n",
    "            exp_x = np.exp(-x_uij)\n",
    "            partial_BPR = 1/(1 + np.exp(exp_x))\n",
    "            \n",
    "            # 更新 user latent matrix\n",
    "            New_P= alpha * (partial_BPR*(Q[i_idx]-Q[j_idx]) + l*(P[u_idx]))\n",
    "            # 若user item 有值則對Q的相對欄位進行SGD更新, 將更新後user latent matrix先暫存\n",
    "            Q[i_idx] -=  alpha * (partial_BPR*P[u_idx] + l*(Q[i_idx]))\n",
    "            Q[j_idx] -=  alpha * (partial_BPR*-P[u_idx] + l*(Q[j_idx]))\n",
    "            # 更新 user latent matrix\n",
    "            P[u_idx] = New_P\n",
    "            # # 更新bias\n",
    "            # # 對u 做偏微分進行ＳＧＤ更新\n",
    "            # gu = gu - alpha * ((rui - user_item[j,m]) + l*(gu))\n",
    "            # # 對bu 做偏微分進行ＳＧＤ更新\n",
    "            # bu[j] = bu[j] - alpha * ((rui - user_item[j,m]) + l*(bu[j]))\n",
    "            # # 對bi 做偏微分進行ＳＧＤ更新\n",
    "            # bi[m] = bi[m] - alpha * ((rui - user_item[j,m]) + l*(bi[m]))\n",
    "            \n",
    "\n",
    "                \n",
    "        # 進行驗證資料測試\n",
    "        MF_bias_testing.append(test(test_uij, users, items, P, Q))\n",
    "        if epoch % 9 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] testing error={MF_bias_testing[-1]}\")\n",
    "    \n",
    "    rui = np.dot(P[u_idx], Q[i_idx])\n",
    "    ruj = np.dot(P[u_idx], Q[j_idx])\n",
    "    x_uij =  rui - ruj\n",
    "    print(\"{} user like item{} more than item{}, score is {}: \".format(u, i, j, x_uij))\n",
    "    # 各評估指標\n",
    "    evaluation['rmse']= MF_bias_testing[-1]\n",
    "    evaluation['recall@10'] = recall_k(test_matrix, np.dot(P, Q.T))\n",
    "    evaluation['NDCG@10'] = ndcg_score(test_matrix, np.dot(P, Q.T))\n",
    "    \n",
    "    return evaluation, P, Q\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"BPR-MF\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt, P, Q = execute_bpr_matrix_factorization(len_users, movies, training_data, testing_data)\n",
    "print(movie_reuslt)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_matrix_factorization(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_matrix_factorization(douban_users, books, douban_training_data, douban_testing_data)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282af117-a92e-423b-9046-37d7649708f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac852f2d-998e-4559-a13e-6918187bde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywFM\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_factorization_machine(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} FM Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "\n",
    "        # reshape y\n",
    "        y_train = y_train.reshape(1, -1)[0]\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        fm = pywFM.FM(task='regression')\n",
    "\n",
    "        model = fm.run(X_train, y_train, X_val, y_val)\n",
    "        predict_values = model.predictions\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(predict_values - y_val))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_factorization_machine(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"FM\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa178ba6-8d9b-49d5-8ffc-c6df7c4190b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6. BPR-FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d92308f-b439-47b8-8d2f-47936b751813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.7123524561238528, 'recall@10': 0.04213040476104326, 'NDCG@10': 0.29914077644948034}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.29914</td></tr><tr><td>recall@10</td><td>0.04213</td></tr><tr><td>rmse</td><td>0.71235</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glowing-dream-162</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/bfvwamad\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/bfvwamad</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_233543-bfvwamad/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pywFM\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_bpr_factorization_machine(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} FM Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.reshape(1, -1)[0]\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        fm = pywFM.FM(task='classification')\n",
    "\n",
    "        model = fm.run(X_train, y_train, X_val, y_val)\n",
    "        predict_values = model.predictions\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        print(predict_values - y_val)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"BPR-FM\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = execute_bpr_factorization_machine(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "print(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460fc07-b61b-4a62-b5bc-df381606ea0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7.GBDT+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "159f1913-085a-46f4-8cae-607b071f536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.3435173759332357, 'recall@10': 0.017182727960031178, 'NDCG@10': 0.2678576171164622}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.26786</td></tr><tr><td>recall@10</td><td>0.01718</td></tr><tr><td>rmse</td><td>1.34352</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">blooming-donkey-164</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/2w2x9hlz\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/2w2x9hlz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220430_021835-2w2x9hlz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sktools import GradientBoostingFeatureGenerator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_gbdt_lr(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} GBDT+LR Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.ravel()\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        gbf = GradientBoostingFeatureGenerator(regression=True)\n",
    "        lr = LogisticRegression()\n",
    "        pipe = Pipeline([(\"gb_features\", gbf), (\"logistic\", lr)])\n",
    "        \n",
    "        pipe.fit(X_train.toarray(), y_train)\n",
    "\n",
    "        predict_values = pipe.predict(X_val.toarray())\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"GBDT_LR\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = execute_gbdt_lr(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "print(movie_reuslt)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee77f90-24a7-4de4-bdb5-e863d29b7a2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 8. XGB-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3d45a75-fe62-49a0-9e90-0b917edb4158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.3514107576601617, 'recall@10': 0.017182727960031178, 'NDCG@10': 0.2678576171164622}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.26786</td></tr><tr><td>recall@10</td><td>0.01718</td></tr><tr><td>rmse</td><td>1.35141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">silvery-glitter-165</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/2qzhlcen\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/2qzhlcen</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220430_040000-2qzhlcen/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_xgb_lr(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} XGB+LR Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.ravel()\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        gbf = SelectFromModel(estimator=XGBRegressor(), max_features=100, threshold=-np.inf)\n",
    "        lr = LogisticRegression()\n",
    "        pipe = Pipeline([(\"xgb_features\", gbf), (\"logistic\", lr)])\n",
    "        \n",
    "        pipe.fit(X_train.toarray(), y_train)\n",
    "\n",
    "        predict_values = pipe.predict(X_val.toarray())\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"XGB_LR\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = execute_xgb_lr(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "print(movie_reuslt)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fd429-f996-4335-a26f-338efe74ee54",
   "metadata": {},
   "source": [
    "## 9. NN-based RecSys Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978674b1-f0e1-44c7-b8ac-7e12b7989bb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08cd156a-7d29-43ae-94fb-423513aacc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_based_models import DeepCTRModel\n",
    "\n",
    "\n",
    "def deepfm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"DeepFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.DeepFM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"DeepFM={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def nfm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"NFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.NFM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"NFM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def dcn(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"DCN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.DCN(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"DCN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def wd(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"W&D\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.WD(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"W&D={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def ccpm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"CCPM\",\n",
    "                        reinit=True)\n",
    "    # no suppot dense\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation', 'user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.CCPM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"CCPM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def fnn(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"FNN\",\n",
    "                        reinit=True)\n",
    "    deer= DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result, _ = deer.FNN(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"FNN={result}\")\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "def ipnn(dataframe, testing_data, test_index, users, movies, inner=True, outter=False):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"IPNN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result, _ = deer.PNN(dataframe, testing_data, test_index, users, movies, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"IPNN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def opnn(dataframe, testing_data, test_index, users, movies, inner=False, outter=True):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"OPNN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result, _ = deer.PNN(dataframe, testing_data, test_index, users, movies, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"OPNN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def pin(dataframe, testing_data, test_index, users, movies, inner=True, outter=True):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"PIN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.PNN(dataframe, testing_data, test_index, users, movies, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"PIN={result}\")\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efb099-3ab2-443d-a617-f8f967e03c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7981ea-1c09-4332-9a81-bd52e6f3fbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN={'rmse': 1.5967797514547128, 'recall@10': 0.4741153207507846, 'ndcg@10': 0.8667856875568678}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ndcg@10</td><td>0.86679</td></tr><tr><td>recall@10</td><td>0.47412</td></tr><tr><td>rmse</td><td>1.59678</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">visionary-tree-177</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/1ru4dgcm\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/1ru4dgcm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220430_132131-1ru4dgcm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 13:32:41.874617: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-04-30 13:32:41.874657: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220430_133240-26gb6ymk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/26gb6ymk\" target=\"_blank\">glad-sound-178</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_movielens\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "438/438 - 2s - loss: 2.6576 - mse: 2.6575 - val_loss: 2.2494 - val_mse: 2.2493\n",
      "Epoch 2/100\n",
      "438/438 - 1s - loss: 2.2388 - mse: 2.2386 - val_loss: 2.2637 - val_mse: 2.2635\n",
      "Epoch 3/100\n",
      "438/438 - 1s - loss: 2.2141 - mse: 2.2139 - val_loss: 2.2173 - val_mse: 2.2170\n",
      "Epoch 4/100\n",
      "438/438 - 1s - loss: 2.1960 - mse: 2.1957 - val_loss: 2.2159 - val_mse: 2.2156\n",
      "Epoch 5/100\n",
      "438/438 - 1s - loss: 2.1582 - mse: 2.1579 - val_loss: 2.1609 - val_mse: 2.1605\n",
      "Epoch 6/100\n",
      "438/438 - 1s - loss: 2.0837 - mse: 2.0833 - val_loss: 2.1011 - val_mse: 2.1006\n",
      "Epoch 7/100\n",
      "438/438 - 1s - loss: 2.0261 - mse: 2.0256 - val_loss: 2.0745 - val_mse: 2.0740\n",
      "Epoch 8/100\n",
      "438/438 - 1s - loss: 1.9791 - mse: 1.9785 - val_loss: 2.0367 - val_mse: 2.0361\n",
      "Epoch 9/100\n",
      "438/438 - 1s - loss: 1.9364 - mse: 1.9357 - val_loss: 2.0261 - val_mse: 2.0254\n",
      "Epoch 10/100\n",
      "438/438 - 1s - loss: 1.9036 - mse: 1.9029 - val_loss: 2.0265 - val_mse: 2.0257\n",
      "Epoch 11/100\n",
      "438/438 - 1s - loss: 1.8644 - mse: 1.8636 - val_loss: 1.9988 - val_mse: 1.9980\n",
      "Epoch 12/100\n",
      "438/438 - 1s - loss: 1.8215 - mse: 1.8206 - val_loss: 2.0035 - val_mse: 2.0026\n",
      "Epoch 13/100\n",
      "438/438 - 1s - loss: 1.7846 - mse: 1.7837 - val_loss: 1.9975 - val_mse: 1.9965\n",
      "Epoch 14/100\n",
      "438/438 - 1s - loss: 1.7542 - mse: 1.7532 - val_loss: 1.9919 - val_mse: 1.9909\n",
      "Epoch 15/100\n",
      "438/438 - 1s - loss: 1.7232 - mse: 1.7221 - val_loss: 1.9916 - val_mse: 1.9905\n",
      "Epoch 16/100\n",
      "438/438 - 1s - loss: 1.6998 - mse: 1.6986 - val_loss: 1.9887 - val_mse: 1.9876\n",
      "Epoch 17/100\n",
      "438/438 - 1s - loss: 1.6772 - mse: 1.6760 - val_loss: 1.9961 - val_mse: 1.9948\n",
      "Epoch 18/100\n",
      "438/438 - 1s - loss: 1.6599 - mse: 1.6586 - val_loss: 2.0258 - val_mse: 2.0244\n",
      "Epoch 19/100\n",
      "438/438 - 1s - loss: 1.6414 - mse: 1.6400 - val_loss: 2.0144 - val_mse: 2.0131\n",
      "Epoch 20/100\n",
      "438/438 - 1s - loss: 1.6274 - mse: 1.6259 - val_loss: 2.0220 - val_mse: 2.0206\n",
      "Epoch 21/100\n",
      "438/438 - 1s - loss: 1.6115 - mse: 1.6101 - val_loss: 2.0316 - val_mse: 2.0301\n",
      "Epoch 22/100\n",
      "438/438 - 1s - loss: 1.5999 - mse: 1.5984 - val_loss: 2.0273 - val_mse: 2.0257\n",
      "Epoch 23/100\n",
      "438/438 - 1s - loss: 1.5856 - mse: 1.5840 - val_loss: 2.0582 - val_mse: 2.0565\n",
      "Epoch 24/100\n",
      "438/438 - 1s - loss: 1.5750 - mse: 1.5733 - val_loss: 2.1044 - val_mse: 2.1027\n",
      "Epoch 25/100\n",
      "438/438 - 1s - loss: 1.5633 - mse: 1.5616 - val_loss: 2.0791 - val_mse: 2.0773\n",
      "Epoch 26/100\n",
      "438/438 - 1s - loss: 1.5521 - mse: 1.5503 - val_loss: 2.0721 - val_mse: 2.0703\n",
      "Epoch 27/100\n",
      "438/438 - 1s - loss: 1.5457 - mse: 1.5438 - val_loss: 2.1349 - val_mse: 2.1330\n",
      "Epoch 28/100\n",
      "438/438 - 1s - loss: 1.5308 - mse: 1.5288 - val_loss: 2.1520 - val_mse: 2.1501\n",
      "Epoch 29/100\n",
      "438/438 - 1s - loss: 1.5252 - mse: 1.5232 - val_loss: 2.0954 - val_mse: 2.0934\n",
      "Epoch 30/100\n",
      "438/438 - 1s - loss: 1.5149 - mse: 1.5129 - val_loss: 2.1122 - val_mse: 2.1102\n",
      "Epoch 31/100\n",
      "438/438 - 1s - loss: 1.5044 - mse: 1.5023 - val_loss: 2.1171 - val_mse: 2.1150\n",
      "Epoch 32/100\n",
      "438/438 - 1s - loss: 1.4936 - mse: 1.4914 - val_loss: 2.1589 - val_mse: 2.1568\n",
      "Epoch 33/100\n",
      "438/438 - 1s - loss: 1.4866 - mse: 1.4844 - val_loss: 2.1288 - val_mse: 2.1266\n",
      "Epoch 34/100\n",
      "438/438 - 1s - loss: 1.4751 - mse: 1.4729 - val_loss: 2.1399 - val_mse: 2.1376\n",
      "Epoch 35/100\n",
      "438/438 - 1s - loss: 1.4698 - mse: 1.4675 - val_loss: 2.2029 - val_mse: 2.2006\n",
      "Epoch 36/100\n",
      "438/438 - 1s - loss: 1.4595 - mse: 1.4571 - val_loss: 2.1579 - val_mse: 2.1555\n",
      "Epoch 37/100\n",
      "438/438 - 1s - loss: 1.4504 - mse: 1.4479 - val_loss: 2.1783 - val_mse: 2.1759\n",
      "Epoch 38/100\n",
      "438/438 - 1s - loss: 1.4386 - mse: 1.4361 - val_loss: 2.2028 - val_mse: 2.2003\n",
      "Epoch 39/100\n",
      "438/438 - 1s - loss: 1.4340 - mse: 1.4315 - val_loss: 2.1995 - val_mse: 2.1969\n",
      "Epoch 40/100\n",
      "438/438 - 1s - loss: 1.4242 - mse: 1.4216 - val_loss: 2.2052 - val_mse: 2.2026\n",
      "Epoch 41/100\n",
      "438/438 - 1s - loss: 1.4153 - mse: 1.4126 - val_loss: 2.1785 - val_mse: 2.1758\n",
      "Epoch 42/100\n",
      "438/438 - 1s - loss: 1.4070 - mse: 1.4043 - val_loss: 2.2773 - val_mse: 2.2745\n",
      "Epoch 43/100\n",
      "438/438 - 1s - loss: 1.3978 - mse: 1.3950 - val_loss: 2.2398 - val_mse: 2.2370\n",
      "Epoch 44/100\n",
      "438/438 - 1s - loss: 1.3873 - mse: 1.3845 - val_loss: 2.2592 - val_mse: 2.2563\n",
      "Epoch 45/100\n",
      "438/438 - 1s - loss: 1.3815 - mse: 1.3785 - val_loss: 2.2500 - val_mse: 2.2471\n",
      "Epoch 46/100\n",
      "438/438 - 1s - loss: 1.3750 - mse: 1.3721 - val_loss: 2.2267 - val_mse: 2.2237\n",
      "Epoch 47/100\n",
      "438/438 - 1s - loss: 1.3645 - mse: 1.3615 - val_loss: 2.2375 - val_mse: 2.2344\n",
      "Epoch 48/100\n",
      "438/438 - 1s - loss: 1.3570 - mse: 1.3539 - val_loss: 2.2909 - val_mse: 2.2878\n",
      "Epoch 49/100\n",
      "438/438 - 1s - loss: 1.3520 - mse: 1.3489 - val_loss: 2.2985 - val_mse: 2.2953\n",
      "Epoch 50/100\n",
      "438/438 - 1s - loss: 1.3450 - mse: 1.3418 - val_loss: 2.2359 - val_mse: 2.2327\n",
      "Epoch 51/100\n",
      "438/438 - 1s - loss: 1.3361 - mse: 1.3328 - val_loss: 2.2921 - val_mse: 2.2888\n",
      "Epoch 52/100\n",
      "438/438 - 1s - loss: 1.3259 - mse: 1.3226 - val_loss: 2.2643 - val_mse: 2.2609\n",
      "Epoch 53/100\n",
      "438/438 - 1s - loss: 1.3184 - mse: 1.3151 - val_loss: 2.3140 - val_mse: 2.3106\n",
      "Epoch 54/100\n",
      "438/438 - 1s - loss: 1.3112 - mse: 1.3078 - val_loss: 2.2917 - val_mse: 2.2883\n",
      "Epoch 55/100\n",
      "438/438 - 1s - loss: 1.3038 - mse: 1.3003 - val_loss: 2.2818 - val_mse: 2.2783\n",
      "Epoch 56/100\n",
      "438/438 - 1s - loss: 1.2953 - mse: 1.2918 - val_loss: 2.3101 - val_mse: 2.3066\n",
      "Epoch 57/100\n",
      "438/438 - 1s - loss: 1.2876 - mse: 1.2840 - val_loss: 2.3572 - val_mse: 2.3536\n",
      "Epoch 58/100\n",
      "438/438 - 1s - loss: 1.2827 - mse: 1.2790 - val_loss: 2.3037 - val_mse: 2.3001\n",
      "Epoch 59/100\n",
      "438/438 - 1s - loss: 1.2707 - mse: 1.2670 - val_loss: 2.3663 - val_mse: 2.3626\n",
      "Epoch 60/100\n",
      "438/438 - 1s - loss: 1.2653 - mse: 1.2615 - val_loss: 2.3357 - val_mse: 2.3319\n",
      "Epoch 61/100\n",
      "438/438 - 1s - loss: 1.2589 - mse: 1.2550 - val_loss: 2.3888 - val_mse: 2.3850\n",
      "Epoch 62/100\n",
      "438/438 - 1s - loss: 1.2551 - mse: 1.2512 - val_loss: 2.3318 - val_mse: 2.3279\n",
      "Epoch 63/100\n",
      "438/438 - 1s - loss: 1.2434 - mse: 1.2395 - val_loss: 2.3617 - val_mse: 2.3578\n",
      "Epoch 64/100\n",
      "438/438 - 1s - loss: 1.2386 - mse: 1.2346 - val_loss: 2.3889 - val_mse: 2.3849\n",
      "Epoch 65/100\n",
      "438/438 - 1s - loss: 1.2321 - mse: 1.2280 - val_loss: 2.3608 - val_mse: 2.3567\n",
      "Epoch 66/100\n",
      "438/438 - 1s - loss: 1.2242 - mse: 1.2201 - val_loss: 2.3865 - val_mse: 2.3823\n",
      "Epoch 67/100\n",
      "438/438 - 1s - loss: 1.2181 - mse: 1.2139 - val_loss: 2.4203 - val_mse: 2.4162\n",
      "Epoch 68/100\n",
      "438/438 - 1s - loss: 1.2130 - mse: 1.2088 - val_loss: 2.3994 - val_mse: 2.3951\n",
      "Epoch 69/100\n",
      "438/438 - 1s - loss: 1.2058 - mse: 1.2016 - val_loss: 2.4404 - val_mse: 2.4361\n",
      "Epoch 70/100\n",
      "438/438 - 1s - loss: 1.1961 - mse: 1.1918 - val_loss: 2.4298 - val_mse: 2.4255\n",
      "Epoch 71/100\n",
      "438/438 - 1s - loss: 1.1902 - mse: 1.1859 - val_loss: 2.3871 - val_mse: 2.3827\n",
      "Epoch 72/100\n",
      "438/438 - 1s - loss: 1.1883 - mse: 1.1839 - val_loss: 2.4492 - val_mse: 2.4448\n",
      "Epoch 73/100\n",
      "438/438 - 1s - loss: 1.1758 - mse: 1.1713 - val_loss: 2.4939 - val_mse: 2.4894\n",
      "Epoch 74/100\n",
      "438/438 - 1s - loss: 1.1703 - mse: 1.1658 - val_loss: 2.4241 - val_mse: 2.4195\n",
      "Epoch 75/100\n",
      "438/438 - 1s - loss: 1.1641 - mse: 1.1595 - val_loss: 2.4348 - val_mse: 2.4301\n",
      "Epoch 76/100\n",
      "438/438 - 1s - loss: 1.1594 - mse: 1.1547 - val_loss: 2.4752 - val_mse: 2.4706\n",
      "Epoch 77/100\n",
      "438/438 - 1s - loss: 1.1502 - mse: 1.1455 - val_loss: 2.4333 - val_mse: 2.4286\n",
      "Epoch 78/100\n",
      "438/438 - 1s - loss: 1.1458 - mse: 1.1411 - val_loss: 2.4954 - val_mse: 2.4906\n",
      "Epoch 79/100\n",
      "438/438 - 1s - loss: 1.1367 - mse: 1.1319 - val_loss: 2.4605 - val_mse: 2.4557\n",
      "Epoch 80/100\n",
      "438/438 - 1s - loss: 1.1334 - mse: 1.1286 - val_loss: 2.4348 - val_mse: 2.4299\n",
      "Epoch 81/100\n",
      "438/438 - 1s - loss: 1.1249 - mse: 1.1200 - val_loss: 2.4768 - val_mse: 2.4719\n",
      "Epoch 82/100\n",
      "438/438 - 1s - loss: 1.1226 - mse: 1.1177 - val_loss: 2.4860 - val_mse: 2.4810\n",
      "Epoch 83/100\n",
      "438/438 - 1s - loss: 1.1136 - mse: 1.1086 - val_loss: 2.5124 - val_mse: 2.5073\n",
      "Epoch 84/100\n",
      "438/438 - 1s - loss: 1.1060 - mse: 1.1009 - val_loss: 2.5028 - val_mse: 2.4977\n",
      "Epoch 85/100\n",
      "438/438 - 1s - loss: 1.1088 - mse: 1.1037 - val_loss: 2.4834 - val_mse: 2.4783\n",
      "Epoch 86/100\n",
      "438/438 - 1s - loss: 1.0981 - mse: 1.0929 - val_loss: 2.5581 - val_mse: 2.5529\n",
      "Epoch 87/100\n",
      "438/438 - 1s - loss: 1.0846 - mse: 1.0794 - val_loss: 2.4965 - val_mse: 2.4913\n",
      "Epoch 88/100\n",
      "438/438 - 1s - loss: 1.0836 - mse: 1.0783 - val_loss: 2.5098 - val_mse: 2.5045\n",
      "Epoch 89/100\n",
      "438/438 - 1s - loss: 1.0810 - mse: 1.0757 - val_loss: 2.5329 - val_mse: 2.5276\n",
      "Epoch 90/100\n",
      "438/438 - 1s - loss: 1.0730 - mse: 1.0677 - val_loss: 2.5138 - val_mse: 2.5085\n",
      "Epoch 91/100\n",
      "438/438 - 1s - loss: 1.0668 - mse: 1.0614 - val_loss: 2.5402 - val_mse: 2.5347\n",
      "Epoch 92/100\n",
      "438/438 - 1s - loss: 1.0654 - mse: 1.0600 - val_loss: 2.5775 - val_mse: 2.5720\n",
      "Epoch 93/100\n",
      "438/438 - 1s - loss: 1.0617 - mse: 1.0562 - val_loss: 2.5278 - val_mse: 2.5223\n",
      "Epoch 94/100\n",
      "438/438 - 1s - loss: 1.0508 - mse: 1.0453 - val_loss: 2.4959 - val_mse: 2.4903\n",
      "Epoch 95/100\n",
      "438/438 - 1s - loss: 1.0462 - mse: 1.0406 - val_loss: 2.5654 - val_mse: 2.5598\n",
      "Epoch 96/100\n",
      "438/438 - 1s - loss: 1.0417 - mse: 1.0361 - val_loss: 2.5507 - val_mse: 2.5450\n",
      "Epoch 97/100\n",
      "438/438 - 1s - loss: 1.0361 - mse: 1.0304 - val_loss: 2.5275 - val_mse: 2.5218\n",
      "Epoch 98/100\n",
      "438/438 - 1s - loss: 1.0340 - mse: 1.0283 - val_loss: 2.5909 - val_mse: 2.5851\n",
      "Epoch 99/100\n",
      "438/438 - 1s - loss: 1.0304 - mse: 1.0246 - val_loss: 2.5756 - val_mse: 2.5698\n",
      "Epoch 100/100\n",
      "438/438 - 1s - loss: 1.0261 - mse: 1.0203 - val_loss: 2.5702 - val_mse: 2.5643\n",
      "Epoch 1/100\n",
      "438/438 - 3s - loss: 2.6404 - mse: 2.6403 - val_loss: 2.2383 - val_mse: 2.2381\n",
      "Epoch 2/100\n",
      "438/438 - 1s - loss: 2.2075 - mse: 2.2073 - val_loss: 2.1650 - val_mse: 2.1648\n",
      "Epoch 3/100\n",
      "438/438 - 1s - loss: 2.1106 - mse: 2.1104 - val_loss: 2.1208 - val_mse: 2.1205\n",
      "Epoch 4/100\n",
      "438/438 - 1s - loss: 2.0431 - mse: 2.0427 - val_loss: 2.0803 - val_mse: 2.0799\n",
      "Epoch 5/100\n",
      "438/438 - 1s - loss: 1.9843 - mse: 1.9839 - val_loss: 2.0232 - val_mse: 2.0228\n",
      "Epoch 6/100\n",
      "438/438 - 1s - loss: 1.9090 - mse: 1.9086 - val_loss: 2.0067 - val_mse: 2.0062\n",
      "Epoch 7/100\n",
      "438/438 - 1s - loss: 1.8418 - mse: 1.8412 - val_loss: 1.9826 - val_mse: 1.9820\n",
      "Epoch 8/100\n",
      "438/438 - 1s - loss: 1.7838 - mse: 1.7831 - val_loss: 1.9682 - val_mse: 1.9675\n",
      "Epoch 9/100\n",
      "438/438 - 1s - loss: 1.7356 - mse: 1.7349 - val_loss: 1.9747 - val_mse: 1.9740\n",
      "Epoch 10/100\n",
      "438/438 - 1s - loss: 1.7058 - mse: 1.7051 - val_loss: 1.9764 - val_mse: 1.9756\n",
      "Epoch 11/100\n",
      "438/438 - 1s - loss: 1.6797 - mse: 1.6788 - val_loss: 1.9831 - val_mse: 1.9822\n",
      "Epoch 12/100\n",
      "438/438 - 1s - loss: 1.6602 - mse: 1.6593 - val_loss: 1.9934 - val_mse: 1.9925\n",
      "Epoch 13/100\n",
      "438/438 - 1s - loss: 1.6400 - mse: 1.6391 - val_loss: 1.9961 - val_mse: 1.9951\n",
      "Epoch 14/100\n",
      "438/438 - 1s - loss: 1.6253 - mse: 1.6243 - val_loss: 2.0127 - val_mse: 2.0117\n",
      "Epoch 15/100\n",
      "438/438 - 1s - loss: 1.6116 - mse: 1.6105 - val_loss: 2.0133 - val_mse: 2.0122\n",
      "Epoch 16/100\n",
      "438/438 - 1s - loss: 1.5990 - mse: 1.5979 - val_loss: 2.0332 - val_mse: 2.0320\n",
      "Epoch 17/100\n",
      "438/438 - 1s - loss: 1.5877 - mse: 1.5865 - val_loss: 2.0242 - val_mse: 2.0230\n",
      "Epoch 18/100\n",
      "438/438 - 1s - loss: 1.5757 - mse: 1.5745 - val_loss: 2.0442 - val_mse: 2.0429\n",
      "Epoch 19/100\n",
      "438/438 - 1s - loss: 1.5632 - mse: 1.5619 - val_loss: 2.0405 - val_mse: 2.0391\n",
      "Epoch 20/100\n",
      "438/438 - 1s - loss: 1.5570 - mse: 1.5556 - val_loss: 2.0599 - val_mse: 2.0585\n",
      "Epoch 21/100\n",
      "438/438 - 1s - loss: 1.5489 - mse: 1.5474 - val_loss: 2.0838 - val_mse: 2.0823\n",
      "Epoch 22/100\n",
      "438/438 - 1s - loss: 1.5395 - mse: 1.5380 - val_loss: 2.0521 - val_mse: 2.0506\n",
      "Epoch 23/100\n",
      "438/438 - 1s - loss: 1.5281 - mse: 1.5265 - val_loss: 2.0706 - val_mse: 2.0690\n",
      "Epoch 24/100\n",
      "438/438 - 1s - loss: 1.5196 - mse: 1.5180 - val_loss: 2.0874 - val_mse: 2.0858\n",
      "Epoch 25/100\n",
      "438/438 - 1s - loss: 1.5111 - mse: 1.5095 - val_loss: 2.1032 - val_mse: 2.1015\n",
      "Epoch 26/100\n",
      "438/438 - 1s - loss: 1.5012 - mse: 1.4995 - val_loss: 2.1238 - val_mse: 2.1221\n",
      "Epoch 27/100\n",
      "438/438 - 1s - loss: 1.4935 - mse: 1.4917 - val_loss: 2.1404 - val_mse: 2.1386\n",
      "Epoch 28/100\n",
      "438/438 - 1s - loss: 1.4864 - mse: 1.4845 - val_loss: 2.1414 - val_mse: 2.1395\n",
      "Epoch 29/100\n",
      "438/438 - 1s - loss: 1.4741 - mse: 1.4722 - val_loss: 2.1946 - val_mse: 2.1927\n",
      "Epoch 30/100\n",
      "438/438 - 1s - loss: 1.4680 - mse: 1.4661 - val_loss: 2.1563 - val_mse: 2.1543\n",
      "Epoch 31/100\n",
      "438/438 - 1s - loss: 1.4566 - mse: 1.4546 - val_loss: 2.1786 - val_mse: 2.1765\n",
      "Epoch 32/100\n",
      "438/438 - 1s - loss: 1.4484 - mse: 1.4464 - val_loss: 2.1645 - val_mse: 2.1624\n",
      "Epoch 33/100\n",
      "438/438 - 1s - loss: 1.4395 - mse: 1.4373 - val_loss: 2.1679 - val_mse: 2.1657\n",
      "Epoch 34/100\n",
      "438/438 - 1s - loss: 1.4314 - mse: 1.4292 - val_loss: 2.1800 - val_mse: 2.1778\n",
      "Epoch 35/100\n",
      "438/438 - 1s - loss: 1.4243 - mse: 1.4221 - val_loss: 2.1810 - val_mse: 2.1788\n",
      "Epoch 36/100\n",
      "438/438 - 1s - loss: 1.4134 - mse: 1.4111 - val_loss: 2.1983 - val_mse: 2.1960\n",
      "Epoch 37/100\n",
      "438/438 - 1s - loss: 1.4070 - mse: 1.4046 - val_loss: 2.1775 - val_mse: 2.1751\n",
      "Epoch 38/100\n",
      "438/438 - 1s - loss: 1.3999 - mse: 1.3975 - val_loss: 2.2157 - val_mse: 2.2132\n",
      "Epoch 39/100\n",
      "438/438 - 1s - loss: 1.3835 - mse: 1.3810 - val_loss: 2.1981 - val_mse: 2.1956\n",
      "Epoch 40/100\n",
      "438/438 - 1s - loss: 1.3769 - mse: 1.3744 - val_loss: 2.2438 - val_mse: 2.2413\n",
      "Epoch 41/100\n",
      "438/438 - 1s - loss: 1.3673 - mse: 1.3647 - val_loss: 2.2245 - val_mse: 2.2219\n",
      "Epoch 42/100\n",
      "438/438 - 1s - loss: 1.3624 - mse: 1.3597 - val_loss: 2.2152 - val_mse: 2.2125\n",
      "Epoch 43/100\n",
      "438/438 - 1s - loss: 1.3510 - mse: 1.3483 - val_loss: 2.2673 - val_mse: 2.2646\n",
      "Epoch 44/100\n",
      "438/438 - 1s - loss: 1.3445 - mse: 1.3418 - val_loss: 2.2470 - val_mse: 2.2442\n",
      "Epoch 45/100\n",
      "438/438 - 1s - loss: 1.3278 - mse: 1.3250 - val_loss: 2.2336 - val_mse: 2.2308\n",
      "Epoch 46/100\n",
      "438/438 - 1s - loss: 1.3242 - mse: 1.3214 - val_loss: 2.3000 - val_mse: 2.2971\n",
      "Epoch 47/100\n",
      "438/438 - 1s - loss: 1.3104 - mse: 1.3074 - val_loss: 2.3036 - val_mse: 2.3007\n",
      "Epoch 48/100\n",
      "438/438 - 1s - loss: 1.3056 - mse: 1.3026 - val_loss: 2.2797 - val_mse: 2.2767\n",
      "Epoch 49/100\n",
      "438/438 - 1s - loss: 1.2891 - mse: 1.2861 - val_loss: 2.3578 - val_mse: 2.3547\n",
      "Epoch 50/100\n",
      "438/438 - 1s - loss: 1.2861 - mse: 1.2831 - val_loss: 2.3790 - val_mse: 2.3758\n",
      "Epoch 51/100\n",
      "438/438 - 1s - loss: 1.2759 - mse: 1.2727 - val_loss: 2.2815 - val_mse: 2.2783\n",
      "Epoch 52/100\n",
      "438/438 - 1s - loss: 1.2666 - mse: 1.2634 - val_loss: 2.3403 - val_mse: 2.3371\n",
      "Epoch 53/100\n",
      "438/438 - 1s - loss: 1.2562 - mse: 1.2529 - val_loss: 2.2875 - val_mse: 2.2842\n",
      "Epoch 54/100\n",
      "438/438 - 1s - loss: 1.2486 - mse: 1.2453 - val_loss: 2.2915 - val_mse: 2.2882\n",
      "Epoch 55/100\n",
      "438/438 - 1s - loss: 1.2360 - mse: 1.2326 - val_loss: 2.3494 - val_mse: 2.3460\n",
      "Epoch 56/100\n",
      "438/438 - 1s - loss: 1.2289 - mse: 1.2255 - val_loss: 2.3419 - val_mse: 2.3384\n",
      "Epoch 57/100\n",
      "438/438 - 1s - loss: 1.2188 - mse: 1.2154 - val_loss: 2.3414 - val_mse: 2.3379\n",
      "Epoch 58/100\n"
     ]
    }
   ],
   "source": [
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "# 1. FM-supported Neural Networks\n",
    "fnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 2. Product-based Neural Networks\n",
    "ipnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "opnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 3. Convolutional Click Prediction Model \n",
    "ccpm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 4. neumf\n",
    "# 5. Wide&Deep\n",
    "wd(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 6. Deep Drossing\n",
    "dcn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 7. Neural Factorization Machine\n",
    "nfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 8. Deep Factorization Machine\n",
    "deepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "# 1. FM-supported Neural Networks\n",
    "fnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 2. Product-based Neural Networks\n",
    "ipnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "opnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 3. Convolutional Click Prediction Model \n",
    "ccpm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 4. neumf\n",
    "# 5. Wide&Deep\n",
    "wd(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 6. Deep Drossing\n",
    "dcn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 7. Neural Factorization Machine\n",
    "nfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 8. Deep Factorization Machine\n",
    "deepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "# 1. FM-supported Neural Networks\n",
    "fnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 2. Product-based Neural Networks\n",
    "ipnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "opnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 3. Convolutional Click Prediction Model \n",
    "ccpm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 4. neumf\n",
    "# 5. Wide&Deep\n",
    "wd(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 6. Deep Drossing\n",
    "dcn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 7. Neural Factorization Machine\n",
    "nfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 8. Deep Factorization Machine\n",
    "deepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f83b5-a90d-4570-b030-70937d006dfe",
   "metadata": {},
   "source": [
    "## 10. Recent NN-based RecSys Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5eaab9-115a-4f2b-bd71-7bb692e6c27d",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdc365-b79b-4928-9119-e1eb0723d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_based_models import DeepCTRModel\n",
    "\n",
    "def din(train_df, test_df, test_index, users, movies, watch_history = ['movie', 'movie_genre'], target=\"rating\"):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"DIN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.DIN(train_df, test_df, test_index, users, movies, watch_history, target)\n",
    "    clear_output()\n",
    "    print(f\"DIN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def xdeepfm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"xDeepFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "                        dense=['user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.xDeepFM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"xDeepFM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def afm(dataframe, testing_data, test_index, users, movies):\n",
    "    run = wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"AFM\",\n",
    "                        reinit=True)\n",
    "    # no dense\n",
    "    deer = DeepCTRModel(sparse=['user', 'movie', 'movie_genre', 'user_occupation', 'user_age'],\n",
    "                        y=['rating'])\n",
    "    result = deer.AFM(dataframe, testing_data, test_index, users, movies)\n",
    "    clear_output()\n",
    "    print(f\"AFM={result}\")\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765641d-6037-4366-9e63-a8fb1cbb78c4",
   "metadata": {},
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa6504-66b8-49ab-829b-f4cb09ddd022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Attentional Factorization Machines\n",
    "afm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 3. xDeepFM\n",
    "xdeepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# 4. Deep Interest Network\n",
    "din(movielens_training_df, movielens_testing_df, test_index, len_users, movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df43df0-cdcf-498f-bf51-a83351eb0546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
