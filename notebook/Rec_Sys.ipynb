{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d29f8d-13fb-4094-ac45-cf49ef5e22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from imp import reload\n",
    "from dataaccessframeworks.read_data import get_movielens, user_filter, training_testing, get_yelp, get_douban, training_testing_XY\n",
    "from dataaccessframeworks.data_preprocessing import get_one_hot_feature, generate_eval_array\n",
    "from models.collaborative_filtering import get_user_item_matrix, predict\n",
    "from models.evaluation import recall_k\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "import configparser\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from util.mywandb import WandbLog\n",
    "import util.utility as util\n",
    "import itertools\n",
    "from random import sample\n",
    "from IPython.display import clear_output\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.path.dirname(os.getcwd()), 'config.ini'))\n",
    "LIBFM_PATH = '/home/baron/libfm/bin/'\n",
    "os.environ['LIBFM_PATH'] = LIBFM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bd60d8-f070-4b2b-b73a-34d25167da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data input:\n",
    "[[user, item, rank], .....]\n",
    "'''\n",
    "def get_uij(data, users, items, sample_rate=1000):\n",
    "    for ii, user in enumerate(users):\n",
    "        items_data = data[data[:, 0]==user]\n",
    "        item_compare = list()\n",
    "        item_neg = list()\n",
    "        neg_count = 0\n",
    "        items_iter =[i for i in itertools.combinations(items, 2)]\n",
    "        items_iter = sample(items_iter, sample_rate)\n",
    "        for i, j in items_iter:\n",
    "            # if i exist items, but j not exsit items, i>j\n",
    "            if i in items_data[:, 1] and j not in items_data[:, 1]:\n",
    "                item_compare.append([user, i, j, 1])\n",
    "            # if j exist items, but i not exsit items, j>i\n",
    "            elif i not in items_data[:, 1] and j in items_data[:, 1]:\n",
    "                item_compare.append([user, j, i, 1])\n",
    "            # if i exist items, and also j exsit items, compare i and j\n",
    "            elif i in items_data[:, 1] and j in items_data[:, 1]:\n",
    "                ri = items_data[(items_data[:, 0]==user) & (items_data[:, 1]==i)][0, 2]\n",
    "                rj = items_data[(items_data[:, 0]==user) & (items_data[:, 1]==j)][0, 2]\n",
    "                if ri > rj:\n",
    "                    item_compare.append([user, i, j, 1])\n",
    "                elif ri < rj:\n",
    "                    item_compare.append([user, j, i, 1])\n",
    "                else:\n",
    "                    if neg_count < len(item_compare)//2:\n",
    "                        item_neg.append([user, j, i, 0])\n",
    "                        item_neg.append([user, i, j, 0])\n",
    "                        neg_count+=1\n",
    "            else:\n",
    "                if neg_count < len(item_compare)//2:\n",
    "                    item_neg.append([user, j, i, 0])\n",
    "                    item_neg.append([user, i, j, 0])\n",
    "                    neg_count+=1\n",
    "        if ii==0:\n",
    "            uij = np.array(item_compare)\n",
    "            uij_neg = np.array(item_neg)\n",
    "        else:\n",
    "            if len(item_compare)!= 0:\n",
    "                uij = np.vstack((uij, np.array(item_compare)))\n",
    "            if len(item_neg)!= 0 and uij_neg.shape[0]!=0:\n",
    "                uij_neg = np.vstack((uij_neg, np.array(item_neg)))\n",
    "        \n",
    "        if ii%300==0:\n",
    "            print(\"[{}/{}] uij_pos: {}, uij_neg: {}\".format(ii, len(users), uij.shape, uij_neg.shape))\n",
    "    \n",
    "    return uij, uij_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4011be-f3e9-46b9-a80e-ae726fc3e542",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc7fb39-e567-42b6-b8a5-b5fdb61739b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataaccessframeworks.data_preprocessing import get_feature_map, generate_with_feature, get_norating_data\n",
    "\n",
    "def get_uij_one_hot_feature(data, user_item_col, uij_data, y_col=3, time_col=3, batch_size=10000):\n",
    "    # 取得user及items feature map \n",
    "    users_dict, items_dict, features = get_feature_map(data, user_item_col)\n",
    "\n",
    "    # 將user item 數值轉為integer\n",
    "    # user_items = np.array([list(map(int, data))for data in data[user_item_col]])\n",
    "    # 使用者評分次數小於三筆則剔除\n",
    "    filter_data = user_filter(uij_data, 0)\n",
    "    print(filter_data.shape)\n",
    "    print(filter_data[:5])\n",
    "    # user label encoder\n",
    "    le = LabelEncoder()\n",
    "    filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "    # item label encoder\n",
    "    ile = LabelEncoder()\n",
    "    filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "    filter_data[:, 2] = ile.fit_transform(filter_data[:, 2])\n",
    "    \n",
    "    # 做特徵的onehot encoding \n",
    "    one_hot_encoder_data, y, concat_data = get_uij_onehot_encoding(filter_data, users_dict, items_dict, features, le, ile, batch_size, y_col)\n",
    "\n",
    "    return one_hot_encoder_data, y, concat_data\n",
    "\n",
    "# 取得user及items的one hot encoding map\n",
    "def get_uij_onehot_encoding(data, users_dict, items_dict, features, le, ile, batch_size, y_col):\n",
    "    #users_onehot = get_users_onehot(data)\n",
    "    sparse_, dense = get_uij_feature_onehot(data, users_dict, items_dict, features, le, ile, batch_size)\n",
    "    \n",
    "    # 取得y\n",
    "    y = data[:,y_col].reshape(-1,1)\n",
    "    \n",
    "    # return np.concatenate((sparse_, dense), axis=1), y, concat_data\n",
    "    return sparse.hstack((sparse_, dense), format='csr'), y, data\n",
    "\n",
    "# 取得feature one hot\n",
    "def get_uij_feature_onehot(data, users_feature, items_feature, features_map, le, ile, batch_size):\n",
    "    # 取得user & item個數\n",
    "    user_number = np.max(data[:,0]) + 1\n",
    "    item_number = np.max(data[:,1]) + 1\n",
    "    i_feature = items_feature[1].keys()\n",
    "    # one hot encoding\n",
    "    for b in range(0, data.shape[0], batch_size):\n",
    "        user_one_hot = np.eye(user_number)[data[b:b+batch_size,0]]\n",
    "        itemi_one_hot = np.eye(item_number)[data[b:b+batch_size,1]]\n",
    "        itemj_one_hot = np.eye(item_number)[data[b:b+batch_size,2]]\n",
    "        sparse_ = np.concatenate((user_one_hot, itemi_one_hot, itemj_one_hot), axis=1)\n",
    "        dense = np.empty((user_one_hot.shape[0], 1), int)\n",
    "\n",
    "        # create items feature \n",
    "        i_feature = items_feature[1].keys()\n",
    "        for fe in i_feature:\n",
    "            # sparse\n",
    "            if fe.split(\"_\")[1] != 'year':\n",
    "                f_map = features_map[fe]\n",
    "                feature_lengh = f_map[list(f_map.keys())[0]].shape[1]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), feature_lengh*2))\n",
    "                for i, item_ij in enumerate(data[b:b+batch_size, 1:3]):\n",
    "                    item_i, item_j = item_ij\n",
    "                    item_i = ile.inverse_transform(np.array([item_i])).item()\n",
    "                    item_j = ile.inverse_transform(np.array([item_j])).item()\n",
    "                    if item_i not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, :feature_lengh] = features_map[fe][item_i].toarray()\n",
    "                    if item_j not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, feature_lengh:] = features_map[fe][item_j].toarray()\n",
    "                # sparse_ = np.concatenate((sparse_, tmp), axis=1)\n",
    "                sparse_ = np.hstack((sparse_, tmp))\n",
    "            # dense\n",
    "            else:\n",
    "                # i = 0\n",
    "                f_map = features_map[fe]\n",
    "                feature_lengh = f_map[list(f_map.keys())[0]].shape[1]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), feature_lengh*2))\n",
    "                for i, item_ij in enumerate(data[b:b+batch_size, 1:3]):\n",
    "                    item_i, item_j = item_ij\n",
    "                    item_i = ile.inverse_transform(np.array([item_i])).item()\n",
    "                    item_j = ile.inverse_transform(np.array([item_j])).item()\n",
    "                    if item_i not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, :feature_lengh] = features_map[fe][item_i].toarray()\n",
    "                    if item_j not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, feature_lengh:] = features_map[fe][item_j].toarray()\n",
    "                # dense = np.concatenate((dense, tmp), axis=1)\n",
    "                dense = np.hstack((dense, tmp))\n",
    "\n",
    "        # create user feature\n",
    "        u_feature = users_feature[1].keys()\n",
    "        for fe in u_feature:\n",
    "            # sparse\n",
    "            if fe.split(\"_\")[1] != 'age':\n",
    "                f_map = features_map[fe]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                for i, user in enumerate(data[b:b+batch_size, 0]):\n",
    "                    # i = 0\n",
    "                    user = le.inverse_transform(np.array([user])).item()\n",
    "                    if user not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # user_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp[i] = features_map[fe][user].toarray()\n",
    "                # sparse_ = np.concatenate((sparse_, tmp), axis=1)\n",
    "                sparse_ = np.hstack((sparse_, tmp))\n",
    "                \n",
    "            # dense\n",
    "            else:\n",
    "                f_map = features_map[fe]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                for i, user in enumerate(data[b:b+batch_size, 0]):\n",
    "                    # i = 0\n",
    "                    user = le.inverse_transform(np.array([user])).item()\n",
    "                    if user not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # user_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp[i] = features_map[fe][user].toarray()\n",
    "                # dense = np.concatenate((dense, tmp), axis=1)\n",
    "                dense = np.hstack((dense, tmp))\n",
    "        if b==0:\n",
    "            sparse_matrix = csr_matrix(sparse_)\n",
    "            dense_matrix = dense\n",
    "        else:\n",
    "            sparse_matrix = sparse.vstack((sparse_matrix, csr_matrix(sparse_)))\n",
    "            dense_matrix = np.vstack((dense_matrix, dense))\n",
    "        print(\"[{}/{}] sparse_matrix shape is {}\".format(b, data.shape[0], sparse_matrix.shape))\n",
    "    \n",
    "    return sparse_matrix, dense_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682edb3-cccb-4711-b0da-fa7754ae3070",
   "metadata": {},
   "source": [
    "### MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de72e1bc-5f21-45ac-aeb9-272d3b19aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_movie:[['196' '242' '3']\n",
      " ['186' '302' '3']\n",
      " ['22' '377' '1']]\n",
      "movie_genre:[['1' '3']\n",
      " ['1' '4']\n",
      " ['1' '5']]\n",
      "user_age:[['1' '3']\n",
      " ['2' '6']\n",
      " ['3' '3']]\n",
      "user_occupation:[['1' '1']\n",
      " ['2' '2']\n",
      " ['3' '3']]\n",
      "使用者評分大於三次的共有：(100000, 3)\n",
      "users:  943\n",
      "items:  1682\n",
      "(100000, 3)\n",
      "[0/194300] sparse_matrix shape is (100000, 2666)\n",
      "[100000/194300] sparse_matrix shape is (194300, 2666)\n",
      "(155440, 2676) (38860, 2676)\n",
      "(194300, 2676)\n"
     ]
    }
   ],
   "source": [
    "data = get_movielens()\n",
    "# str to int\n",
    "user_movie = np.array([list(map(int, data)) for data in data['user_movie']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_movie, 0)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 是否加上假資料\n",
    "fake=True\n",
    "if fake:\n",
    "    # 取得加上使用者未評分的sample假資料\n",
    "    filter_data = get_norating_data(filter_data)\n",
    "    \n",
    "# 取得電影個數及電影個數\n",
    "len_users, movies = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "training_data,  testing_data = training_testing(filter_data)\n",
    "\n",
    "users_dict, items_dict, features = get_feature_map(data, 'user_movie')\n",
    "movielens_training_df = generate_with_feature(training_data, users_dict, items_dict, init_col=[\"user\", \"movie\", \"rating\"])\n",
    "movielens_testing_df = generate_with_feature(testing_data, users_dict, items_dict, init_col=[\"user\", \"movie\", \"rating\"])\n",
    "\n",
    "\n",
    "# normalize rating value\n",
    "# training_data[:, 2:3] = normalize(training_data[:, 2:3], axis=0)\n",
    "# testing_data[:, 2:3] = normalize(testing_data[:, 2:3], axis=0)\n",
    "# train_min = training_data[:, 2:3].min()\n",
    "# train_max = training_data[:, 2:3].max()\n",
    "# training_rating = (training_data[:, 2] - train_min)/(train_max-train_min)\n",
    "# test_min = testing_data[:, 2:3].min()\n",
    "# test_max = testing_data[:, 2:3].max()\n",
    "# testing_rating = (testing_data[:, 2:3] - test_min)/(test_max-test_min)\n",
    "print(\"users: \", len(len_users))\n",
    "print(\"items: \", len(movies))\n",
    "\n",
    "# generarte one hot encoding\n",
    "bpr = False\n",
    "if bpr:\n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(training_data, len_users, movies)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    train_uij = np.vstack((uij_pos, uij_neg))\n",
    "    test_uij_pos, test_uij_neg = get_uij(testing_data, len_users, movies)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "    one_hot_x, y, add_fake_data = get_uij_one_hot_feature(data,  'user_movie', train_uij, batch_size=100000)\n",
    "else:\n",
    "    one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_movie', batch_size=100000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index, test_index, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(one_hot_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140b967-0b12-4efb-8752-75b341b6018f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9291be6-6e04-47d6-af61-f92487ace79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_category:[['1', '334', '1'], ['1', '426', '1'], ['2', '211', '1']]\n",
      "business_city:[['1', '31', '1'], ['2', '35', '1'], ['3', '35', '1']]\n",
      "user_business:[['1', '8391', '5'], ['1', '8971', '5'], ['2', '186', '5']]\n",
      "user_compliment:[['2', '1', '1'], ['2', '2', '1'], ['2', '3', '1']]\n",
      "使用者評分大於三次的共有：(184835, 3)\n",
      "users:  7326\n",
      "items:  14127\n",
      "(184835, 3)\n",
      "[0/917435] sparse_matrix shape is (10000, 22025)\n",
      "[10000/917435] sparse_matrix shape is (20000, 22025)\n",
      "[20000/917435] sparse_matrix shape is (30000, 22025)\n",
      "[30000/917435] sparse_matrix shape is (40000, 22025)\n",
      "[40000/917435] sparse_matrix shape is (50000, 22025)\n",
      "[50000/917435] sparse_matrix shape is (60000, 22025)\n",
      "[60000/917435] sparse_matrix shape is (70000, 22025)\n",
      "[70000/917435] sparse_matrix shape is (80000, 22025)\n",
      "[80000/917435] sparse_matrix shape is (90000, 22025)\n",
      "[90000/917435] sparse_matrix shape is (100000, 22025)\n",
      "[100000/917435] sparse_matrix shape is (110000, 22025)\n",
      "[110000/917435] sparse_matrix shape is (120000, 22025)\n",
      "[120000/917435] sparse_matrix shape is (130000, 22025)\n",
      "[130000/917435] sparse_matrix shape is (140000, 22025)\n",
      "[140000/917435] sparse_matrix shape is (150000, 22025)\n",
      "[150000/917435] sparse_matrix shape is (160000, 22025)\n",
      "[160000/917435] sparse_matrix shape is (170000, 22025)\n",
      "[170000/917435] sparse_matrix shape is (180000, 22025)\n",
      "[180000/917435] sparse_matrix shape is (190000, 22025)\n",
      "[190000/917435] sparse_matrix shape is (200000, 22025)\n",
      "[200000/917435] sparse_matrix shape is (210000, 22025)\n",
      "[210000/917435] sparse_matrix shape is (220000, 22025)\n",
      "[220000/917435] sparse_matrix shape is (230000, 22025)\n",
      "[230000/917435] sparse_matrix shape is (240000, 22025)\n",
      "[240000/917435] sparse_matrix shape is (250000, 22025)\n",
      "[250000/917435] sparse_matrix shape is (260000, 22025)\n",
      "[260000/917435] sparse_matrix shape is (270000, 22025)\n",
      "[270000/917435] sparse_matrix shape is (280000, 22025)\n",
      "[280000/917435] sparse_matrix shape is (290000, 22025)\n",
      "[290000/917435] sparse_matrix shape is (300000, 22025)\n",
      "[300000/917435] sparse_matrix shape is (310000, 22025)\n",
      "[310000/917435] sparse_matrix shape is (320000, 22025)\n",
      "[320000/917435] sparse_matrix shape is (330000, 22025)\n",
      "[330000/917435] sparse_matrix shape is (340000, 22025)\n",
      "[340000/917435] sparse_matrix shape is (350000, 22025)\n",
      "[350000/917435] sparse_matrix shape is (360000, 22025)\n",
      "[360000/917435] sparse_matrix shape is (370000, 22025)\n",
      "[370000/917435] sparse_matrix shape is (380000, 22025)\n",
      "[380000/917435] sparse_matrix shape is (390000, 22025)\n",
      "[390000/917435] sparse_matrix shape is (400000, 22025)\n",
      "[400000/917435] sparse_matrix shape is (410000, 22025)\n",
      "[410000/917435] sparse_matrix shape is (420000, 22025)\n",
      "[420000/917435] sparse_matrix shape is (430000, 22025)\n",
      "[430000/917435] sparse_matrix shape is (440000, 22025)\n",
      "[440000/917435] sparse_matrix shape is (450000, 22025)\n",
      "[450000/917435] sparse_matrix shape is (460000, 22025)\n",
      "[460000/917435] sparse_matrix shape is (470000, 22025)\n",
      "[470000/917435] sparse_matrix shape is (480000, 22025)\n",
      "[480000/917435] sparse_matrix shape is (490000, 22025)\n",
      "[490000/917435] sparse_matrix shape is (500000, 22025)\n",
      "[500000/917435] sparse_matrix shape is (510000, 22025)\n",
      "[510000/917435] sparse_matrix shape is (520000, 22025)\n",
      "[520000/917435] sparse_matrix shape is (530000, 22025)\n",
      "[530000/917435] sparse_matrix shape is (540000, 22025)\n",
      "[540000/917435] sparse_matrix shape is (550000, 22025)\n",
      "[550000/917435] sparse_matrix shape is (560000, 22025)\n",
      "[560000/917435] sparse_matrix shape is (570000, 22025)\n",
      "[570000/917435] sparse_matrix shape is (580000, 22025)\n",
      "[580000/917435] sparse_matrix shape is (590000, 22025)\n",
      "[590000/917435] sparse_matrix shape is (600000, 22025)\n",
      "[600000/917435] sparse_matrix shape is (610000, 22025)\n",
      "[610000/917435] sparse_matrix shape is (620000, 22025)\n",
      "[620000/917435] sparse_matrix shape is (630000, 22025)\n",
      "[630000/917435] sparse_matrix shape is (640000, 22025)\n",
      "[640000/917435] sparse_matrix shape is (650000, 22025)\n",
      "[650000/917435] sparse_matrix shape is (660000, 22025)\n",
      "[660000/917435] sparse_matrix shape is (670000, 22025)\n",
      "[670000/917435] sparse_matrix shape is (680000, 22025)\n",
      "[680000/917435] sparse_matrix shape is (690000, 22025)\n",
      "[690000/917435] sparse_matrix shape is (700000, 22025)\n",
      "[700000/917435] sparse_matrix shape is (710000, 22025)\n",
      "[710000/917435] sparse_matrix shape is (720000, 22025)\n",
      "[720000/917435] sparse_matrix shape is (730000, 22025)\n",
      "[730000/917435] sparse_matrix shape is (740000, 22025)\n",
      "[740000/917435] sparse_matrix shape is (750000, 22025)\n",
      "[750000/917435] sparse_matrix shape is (760000, 22025)\n",
      "[760000/917435] sparse_matrix shape is (770000, 22025)\n",
      "[770000/917435] sparse_matrix shape is (780000, 22025)\n",
      "[780000/917435] sparse_matrix shape is (790000, 22025)\n",
      "[790000/917435] sparse_matrix shape is (800000, 22025)\n",
      "[800000/917435] sparse_matrix shape is (810000, 22025)\n",
      "[810000/917435] sparse_matrix shape is (820000, 22025)\n",
      "[820000/917435] sparse_matrix shape is (830000, 22025)\n",
      "[830000/917435] sparse_matrix shape is (840000, 22025)\n",
      "[840000/917435] sparse_matrix shape is (850000, 22025)\n",
      "[850000/917435] sparse_matrix shape is (860000, 22025)\n",
      "[860000/917435] sparse_matrix shape is (870000, 22025)\n",
      "[870000/917435] sparse_matrix shape is (880000, 22025)\n",
      "[880000/917435] sparse_matrix shape is (890000, 22025)\n",
      "[890000/917435] sparse_matrix shape is (900000, 22025)\n",
      "[900000/917435] sparse_matrix shape is (910000, 22025)\n",
      "[910000/917435] sparse_matrix shape is (917435, 22025)\n",
      "(733948, 22026) (183487, 22026)\n",
      "(917435, 22026)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = get_yelp()\n",
    "# str to int\n",
    "user_business = np.array([list(map(int, data)) for data in data['user_business']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_business, 0)\n",
    "# user label encoder\n",
    "le = LabelEncoder()\n",
    "filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "filter_data[:, 0] += 1\n",
    "# item label encoder\n",
    "ile = LabelEncoder()\n",
    "filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "filter_data[:, 1] += 1\n",
    "# if want to inverse label \n",
    "# le.inverse_transform(yelp_training_encoder)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 是否加上假資料\n",
    "fake=True\n",
    "if fake:\n",
    "    # 取得加上使用者未評分的sample假資料\n",
    "    filter_data = get_norating_data(filter_data)\n",
    "\n",
    "# 取得business個數及users個數\n",
    "yelp_users, business = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "yelp_training_data,  yelp_testing_data = training_testing(filter_data)\n",
    "users_dict, items_dict, features = get_feature_map(data, 'user_business')\n",
    "yelp_training_df = generate_with_feature(yelp_training_data, users_dict, items_dict, init_col=[\"user\", \"business\", \"rating\"])\n",
    "yelp_testing_df = generate_with_feature(yelp_testing_data, users_dict, items_dict, init_col=[\"user\", \"business\", \"rating\"])\n",
    "\n",
    "print(\"users: \", len(yelp_users))\n",
    "print(\"items: \", len(business))\n",
    "# generarte one hot encoding\n",
    "bpr = False\n",
    "if bpr:\n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(yelp_training_data, yelp_users, business)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    train_uij = np.vstack((uij_pos, uij_neg))\n",
    "    test_uij_pos, test_uij_neg = get_uij(yelp_testing_data, yelp_users, business)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "    one_hot_x, y, add_fake_data = get_uij_one_hot_feature(data,  'user_business', train_uij)\n",
    "else:\n",
    "    one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_business')\n",
    "\n",
    "# generarte one hot encoding\n",
    "X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index_yelp, test_index_yelp, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n",
    "print(X_train_yelp.shape, X_test_yelp.shape)\n",
    "print(one_hot_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ba6de-2893-4a33-b9c2-e5a77953b129",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01661c74-3b39-49c2-a121-7002680d2ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_author:[['12131', '3871'], ['20995', '10690'], ['9905', '3845']]\n",
      "book_publisher:[['12131', '108'], ['20995', '1470'], ['9905', '1696']]\n",
      "book_year:[['9905', '16'], ['21153', '15'], ['12823', '15']]\n",
      "user_book:[['10855', '938', '4'], ['10027', '3', '3'], ['741', '2426', '5']]\n",
      "user_group:[['3587', '232'], ['3587', '666'], ['3587', '226']]\n",
      "user_location:[['3587', '33'], ['3210', '179'], ['7993', '394']]\n",
      "使用者評分大於三次的共有：(788898, 3)\n",
      "users:  11266\n",
      "items:  22347\n",
      "(788898, 3)\n",
      "[0/1915498] sparse_matrix shape is (10000, 49626)\n",
      "[10000/1915498] sparse_matrix shape is (20000, 49626)\n",
      "[20000/1915498] sparse_matrix shape is (30000, 49626)\n",
      "[30000/1915498] sparse_matrix shape is (40000, 49626)\n",
      "[40000/1915498] sparse_matrix shape is (50000, 49626)\n",
      "[50000/1915498] sparse_matrix shape is (60000, 49626)\n",
      "[60000/1915498] sparse_matrix shape is (70000, 49626)\n",
      "[70000/1915498] sparse_matrix shape is (80000, 49626)\n",
      "[80000/1915498] sparse_matrix shape is (90000, 49626)\n",
      "[90000/1915498] sparse_matrix shape is (100000, 49626)\n",
      "[100000/1915498] sparse_matrix shape is (110000, 49626)\n",
      "[110000/1915498] sparse_matrix shape is (120000, 49626)\n",
      "[120000/1915498] sparse_matrix shape is (130000, 49626)\n",
      "[130000/1915498] sparse_matrix shape is (140000, 49626)\n",
      "[140000/1915498] sparse_matrix shape is (150000, 49626)\n",
      "[150000/1915498] sparse_matrix shape is (160000, 49626)\n",
      "[160000/1915498] sparse_matrix shape is (170000, 49626)\n",
      "[170000/1915498] sparse_matrix shape is (180000, 49626)\n",
      "[180000/1915498] sparse_matrix shape is (190000, 49626)\n",
      "[190000/1915498] sparse_matrix shape is (200000, 49626)\n",
      "[200000/1915498] sparse_matrix shape is (210000, 49626)\n",
      "[210000/1915498] sparse_matrix shape is (220000, 49626)\n",
      "[220000/1915498] sparse_matrix shape is (230000, 49626)\n",
      "[230000/1915498] sparse_matrix shape is (240000, 49626)\n",
      "[240000/1915498] sparse_matrix shape is (250000, 49626)\n",
      "[250000/1915498] sparse_matrix shape is (260000, 49626)\n",
      "[260000/1915498] sparse_matrix shape is (270000, 49626)\n",
      "[270000/1915498] sparse_matrix shape is (280000, 49626)\n",
      "[280000/1915498] sparse_matrix shape is (290000, 49626)\n",
      "[290000/1915498] sparse_matrix shape is (300000, 49626)\n",
      "[300000/1915498] sparse_matrix shape is (310000, 49626)\n",
      "[310000/1915498] sparse_matrix shape is (320000, 49626)\n",
      "[320000/1915498] sparse_matrix shape is (330000, 49626)\n",
      "[330000/1915498] sparse_matrix shape is (340000, 49626)\n",
      "[340000/1915498] sparse_matrix shape is (350000, 49626)\n",
      "[350000/1915498] sparse_matrix shape is (360000, 49626)\n",
      "[360000/1915498] sparse_matrix shape is (370000, 49626)\n",
      "[370000/1915498] sparse_matrix shape is (380000, 49626)\n",
      "[380000/1915498] sparse_matrix shape is (390000, 49626)\n",
      "[390000/1915498] sparse_matrix shape is (400000, 49626)\n",
      "[400000/1915498] sparse_matrix shape is (410000, 49626)\n",
      "[410000/1915498] sparse_matrix shape is (420000, 49626)\n",
      "[420000/1915498] sparse_matrix shape is (430000, 49626)\n",
      "[430000/1915498] sparse_matrix shape is (440000, 49626)\n",
      "[440000/1915498] sparse_matrix shape is (450000, 49626)\n",
      "[450000/1915498] sparse_matrix shape is (460000, 49626)\n",
      "[460000/1915498] sparse_matrix shape is (470000, 49626)\n",
      "[470000/1915498] sparse_matrix shape is (480000, 49626)\n",
      "[480000/1915498] sparse_matrix shape is (490000, 49626)\n",
      "[490000/1915498] sparse_matrix shape is (500000, 49626)\n",
      "[500000/1915498] sparse_matrix shape is (510000, 49626)\n",
      "[510000/1915498] sparse_matrix shape is (520000, 49626)\n",
      "[520000/1915498] sparse_matrix shape is (530000, 49626)\n",
      "[530000/1915498] sparse_matrix shape is (540000, 49626)\n",
      "[540000/1915498] sparse_matrix shape is (550000, 49626)\n",
      "[550000/1915498] sparse_matrix shape is (560000, 49626)\n",
      "[560000/1915498] sparse_matrix shape is (570000, 49626)\n",
      "[570000/1915498] sparse_matrix shape is (580000, 49626)\n",
      "[580000/1915498] sparse_matrix shape is (590000, 49626)\n",
      "[590000/1915498] sparse_matrix shape is (600000, 49626)\n",
      "[600000/1915498] sparse_matrix shape is (610000, 49626)\n",
      "[610000/1915498] sparse_matrix shape is (620000, 49626)\n",
      "[620000/1915498] sparse_matrix shape is (630000, 49626)\n",
      "[630000/1915498] sparse_matrix shape is (640000, 49626)\n",
      "[640000/1915498] sparse_matrix shape is (650000, 49626)\n",
      "[650000/1915498] sparse_matrix shape is (660000, 49626)\n",
      "[660000/1915498] sparse_matrix shape is (670000, 49626)\n",
      "[670000/1915498] sparse_matrix shape is (680000, 49626)\n",
      "[680000/1915498] sparse_matrix shape is (690000, 49626)\n",
      "[690000/1915498] sparse_matrix shape is (700000, 49626)\n",
      "[700000/1915498] sparse_matrix shape is (710000, 49626)\n",
      "[750000/1915498] sparse_matrix shape is (760000, 49626)\n",
      "[760000/1915498] sparse_matrix shape is (770000, 49626)\n",
      "[770000/1915498] sparse_matrix shape is (780000, 49626)\n",
      "[780000/1915498] sparse_matrix shape is (790000, 49626)\n",
      "[790000/1915498] sparse_matrix shape is (800000, 49626)\n",
      "[800000/1915498] sparse_matrix shape is (810000, 49626)\n",
      "[810000/1915498] sparse_matrix shape is (820000, 49626)\n",
      "[820000/1915498] sparse_matrix shape is (830000, 49626)\n",
      "[830000/1915498] sparse_matrix shape is (840000, 49626)\n",
      "[840000/1915498] sparse_matrix shape is (850000, 49626)\n",
      "[850000/1915498] sparse_matrix shape is (860000, 49626)\n",
      "[860000/1915498] sparse_matrix shape is (870000, 49626)\n",
      "[870000/1915498] sparse_matrix shape is (880000, 49626)\n",
      "[880000/1915498] sparse_matrix shape is (890000, 49626)\n",
      "[890000/1915498] sparse_matrix shape is (900000, 49626)\n",
      "[900000/1915498] sparse_matrix shape is (910000, 49626)\n",
      "[910000/1915498] sparse_matrix shape is (920000, 49626)\n",
      "[920000/1915498] sparse_matrix shape is (930000, 49626)\n",
      "[930000/1915498] sparse_matrix shape is (940000, 49626)\n",
      "[940000/1915498] sparse_matrix shape is (950000, 49626)\n",
      "[950000/1915498] sparse_matrix shape is (960000, 49626)\n",
      "[960000/1915498] sparse_matrix shape is (970000, 49626)\n",
      "[970000/1915498] sparse_matrix shape is (980000, 49626)\n",
      "[980000/1915498] sparse_matrix shape is (990000, 49626)\n",
      "[990000/1915498] sparse_matrix shape is (1000000, 49626)\n",
      "[1000000/1915498] sparse_matrix shape is (1010000, 49626)\n",
      "[1010000/1915498] sparse_matrix shape is (1020000, 49626)\n",
      "[1020000/1915498] sparse_matrix shape is (1030000, 49626)\n",
      "[1030000/1915498] sparse_matrix shape is (1040000, 49626)\n",
      "[1040000/1915498] sparse_matrix shape is (1050000, 49626)\n",
      "[1050000/1915498] sparse_matrix shape is (1060000, 49626)\n",
      "[1060000/1915498] sparse_matrix shape is (1070000, 49626)\n",
      "[1070000/1915498] sparse_matrix shape is (1080000, 49626)\n",
      "[1080000/1915498] sparse_matrix shape is (1090000, 49626)\n",
      "[1090000/1915498] sparse_matrix shape is (1100000, 49626)\n",
      "[1100000/1915498] sparse_matrix shape is (1110000, 49626)\n",
      "[1110000/1915498] sparse_matrix shape is (1120000, 49626)\n",
      "[1120000/1915498] sparse_matrix shape is (1130000, 49626)\n",
      "[1130000/1915498] sparse_matrix shape is (1140000, 49626)\n",
      "[1140000/1915498] sparse_matrix shape is (1150000, 49626)\n",
      "[1150000/1915498] sparse_matrix shape is (1160000, 49626)\n",
      "[1160000/1915498] sparse_matrix shape is (1170000, 49626)\n",
      "[1170000/1915498] sparse_matrix shape is (1180000, 49626)\n",
      "[1180000/1915498] sparse_matrix shape is (1190000, 49626)\n",
      "[1190000/1915498] sparse_matrix shape is (1200000, 49626)\n",
      "[1200000/1915498] sparse_matrix shape is (1210000, 49626)\n",
      "[1210000/1915498] sparse_matrix shape is (1220000, 49626)\n",
      "[1220000/1915498] sparse_matrix shape is (1230000, 49626)\n",
      "[1230000/1915498] sparse_matrix shape is (1240000, 49626)\n",
      "[1240000/1915498] sparse_matrix shape is (1250000, 49626)\n",
      "[1250000/1915498] sparse_matrix shape is (1260000, 49626)\n",
      "[1260000/1915498] sparse_matrix shape is (1270000, 49626)\n",
      "[1270000/1915498] sparse_matrix shape is (1280000, 49626)\n",
      "[1280000/1915498] sparse_matrix shape is (1290000, 49626)\n",
      "[1290000/1915498] sparse_matrix shape is (1300000, 49626)\n",
      "[1300000/1915498] sparse_matrix shape is (1310000, 49626)\n",
      "[1310000/1915498] sparse_matrix shape is (1320000, 49626)\n",
      "[1320000/1915498] sparse_matrix shape is (1330000, 49626)\n",
      "[1330000/1915498] sparse_matrix shape is (1340000, 49626)\n",
      "[1340000/1915498] sparse_matrix shape is (1350000, 49626)\n",
      "[1350000/1915498] sparse_matrix shape is (1360000, 49626)\n",
      "[1360000/1915498] sparse_matrix shape is (1370000, 49626)\n",
      "[1370000/1915498] sparse_matrix shape is (1380000, 49626)\n",
      "[1380000/1915498] sparse_matrix shape is (1390000, 49626)\n",
      "[1390000/1915498] sparse_matrix shape is (1400000, 49626)\n",
      "[1400000/1915498] sparse_matrix shape is (1410000, 49626)\n",
      "[1410000/1915498] sparse_matrix shape is (1420000, 49626)\n",
      "[1420000/1915498] sparse_matrix shape is (1430000, 49626)\n",
      "[1430000/1915498] sparse_matrix shape is (1440000, 49626)\n",
      "[1440000/1915498] sparse_matrix shape is (1450000, 49626)\n",
      "[1450000/1915498] sparse_matrix shape is (1460000, 49626)\n",
      "[1460000/1915498] sparse_matrix shape is (1470000, 49626)\n",
      "[1470000/1915498] sparse_matrix shape is (1480000, 49626)\n",
      "[1480000/1915498] sparse_matrix shape is (1490000, 49626)\n",
      "[1490000/1915498] sparse_matrix shape is (1500000, 49626)\n",
      "[1500000/1915498] sparse_matrix shape is (1510000, 49626)\n",
      "[1510000/1915498] sparse_matrix shape is (1520000, 49626)\n",
      "[1520000/1915498] sparse_matrix shape is (1530000, 49626)\n",
      "[1530000/1915498] sparse_matrix shape is (1540000, 49626)\n",
      "[1540000/1915498] sparse_matrix shape is (1550000, 49626)\n",
      "[1550000/1915498] sparse_matrix shape is (1560000, 49626)\n",
      "[1560000/1915498] sparse_matrix shape is (1570000, 49626)\n",
      "[1570000/1915498] sparse_matrix shape is (1580000, 49626)\n",
      "[1580000/1915498] sparse_matrix shape is (1590000, 49626)\n",
      "[1590000/1915498] sparse_matrix shape is (1600000, 49626)\n",
      "[1600000/1915498] sparse_matrix shape is (1610000, 49626)\n",
      "[1610000/1915498] sparse_matrix shape is (1620000, 49626)\n",
      "[1620000/1915498] sparse_matrix shape is (1630000, 49626)\n",
      "[1630000/1915498] sparse_matrix shape is (1640000, 49626)\n",
      "[1640000/1915498] sparse_matrix shape is (1650000, 49626)\n",
      "[1650000/1915498] sparse_matrix shape is (1660000, 49626)\n",
      "[1660000/1915498] sparse_matrix shape is (1670000, 49626)\n",
      "[1670000/1915498] sparse_matrix shape is (1680000, 49626)\n",
      "[1680000/1915498] sparse_matrix shape is (1690000, 49626)\n",
      "[1690000/1915498] sparse_matrix shape is (1700000, 49626)\n",
      "[1700000/1915498] sparse_matrix shape is (1710000, 49626)\n",
      "[1710000/1915498] sparse_matrix shape is (1720000, 49626)\n",
      "[1720000/1915498] sparse_matrix shape is (1730000, 49626)\n",
      "[1730000/1915498] sparse_matrix shape is (1740000, 49626)\n",
      "[1740000/1915498] sparse_matrix shape is (1750000, 49626)\n",
      "[1750000/1915498] sparse_matrix shape is (1760000, 49626)\n",
      "[1760000/1915498] sparse_matrix shape is (1770000, 49626)\n",
      "[1770000/1915498] sparse_matrix shape is (1780000, 49626)\n",
      "[1780000/1915498] sparse_matrix shape is (1790000, 49626)\n",
      "[1790000/1915498] sparse_matrix shape is (1800000, 49626)\n",
      "[1800000/1915498] sparse_matrix shape is (1810000, 49626)\n",
      "[1810000/1915498] sparse_matrix shape is (1820000, 49626)\n",
      "[1820000/1915498] sparse_matrix shape is (1830000, 49626)\n",
      "[1830000/1915498] sparse_matrix shape is (1840000, 49626)\n",
      "[1840000/1915498] sparse_matrix shape is (1850000, 49626)\n",
      "[1850000/1915498] sparse_matrix shape is (1860000, 49626)\n",
      "[1860000/1915498] sparse_matrix shape is (1870000, 49626)\n",
      "[1870000/1915498] sparse_matrix shape is (1880000, 49626)\n",
      "[1880000/1915498] sparse_matrix shape is (1890000, 49626)\n",
      "[1890000/1915498] sparse_matrix shape is (1900000, 49626)\n",
      "[1900000/1915498] sparse_matrix shape is (1910000, 49626)\n",
      "[1910000/1915498] sparse_matrix shape is (1915498, 49626)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = get_douban()\n",
    "# str to int\n",
    "user_book = np.array([list(map(int, data)) for data in data['user_book']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_book, 0)\n",
    "# user label encoder\n",
    "le = LabelEncoder()\n",
    "filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "filter_data[:, 0] += 1\n",
    "# item label encoder\n",
    "ile = LabelEncoder()\n",
    "filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "filter_data[:, 1] += 1\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 是否加上假資料\n",
    "fake=True\n",
    "if fake:\n",
    "    # 取得加上使用者未評分的sample假資料\n",
    "    filter_data = get_norating_data(filter_data)\n",
    "\n",
    "# 取得business個數及users個數\n",
    "douban_users, books = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "douban_training_data,  douban_testing_data = training_testing(filter_data)\n",
    "users_dict, items_dict, features = get_feature_map(data, 'user_book')\n",
    "douban_training_df = generate_with_feature(douban_training_data, users_dict, items_dict, init_col=[\"user\", \"book\", \"rating\"])\n",
    "douban_testing_df = generate_with_feature(douban_testing_data, users_dict, items_dict, init_col=[\"user\", \"book\", \"rating\"])\n",
    "\n",
    "print(\"users: \", len(douban_users))\n",
    "print(\"items: \", len(books))\n",
    "# generarte one hot encoding\n",
    "bpr = False\n",
    "if bpr:\n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(training_data, douban_users, books)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    train_uij = np.vstack((uij_pos, uij_neg))\n",
    "    test_uij_pos, test_uij_neg = get_uij(testing_data, douban_users, books)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "    one_hot_x, y, add_fake_data = get_uij_one_hot_feature(data,  'user_book', train_uij)\n",
    "else:\n",
    "    one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_book')\n",
    "\n",
    "# generarte one hot encoding\n",
    "X_train_douban, X_test_douban, y_train_douban, y_test_douban = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index_douban, test_index_douban, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100878b3-d40a-4d90-a916-6cdde0b47339",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. User-based Collaborative Filtering (U-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550a514-2c1a-4a63-933f-ec3210319308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7207d08-17cf-46cc-836c-2a2928438f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def user_sim_score(users, items, train_data, test_data, k=int(config['CF']['user_K'])):\n",
    "    # make matrix\n",
    "    user_matrix = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "    # 計算bias\n",
    "    bias_matrix = util.get_bias(user_matrix, users, items)\n",
    "    # 計算相似度\n",
    "#     cos, pcc = util.get_sim_array(user_matrix)\n",
    "#     cosine_dis = cos -  np.identity(len(users))\n",
    "#     pcc_dis = pcc -  np.identity(len(users))\n",
    "    \n",
    "#     sim = {\"cos\":cosine_dis, \"pcc\":pcc_dis}\n",
    "    sim = [\"cos\", \"pcc\"]\n",
    "    evaluation = dict()\n",
    "    for s in sim:\n",
    "        delta_list = list()\n",
    "        predict_array = np.zeros((test_matrix.shape))\n",
    "        # sim_dis = sim[s]\n",
    "        sim_array = util.get_sim_array(user_matrix, sim=s)\n",
    "        sim_dis = sim_array -  np.identity(len(users))\n",
    "        for i in tqdm(range(len(users)), desc=f\"UCF predicting {s} score with {k}\"):\n",
    "            # Suv: 取出前K個最相似的使用者相似度 ex:K=3, output=[0.378, 0.353, 0.336]\n",
    "            Suv = heapq.nlargest(k ,sim_dis[i])\n",
    "            # 若i不存在，則跳過\n",
    "            if np.isnan(sim_dis[i]).all():\n",
    "                continue\n",
    "            # top_sim_index: 取出與使用者i最為相似的前K個使用者 ex:K=3, output=[915, 406, 214]\n",
    "            sim_dis_idx = sim_dis[i].tolist()\n",
    "            top_sim_index = list(map(sim_dis_idx.index, heapq.nlargest(k,sim_dis[i])))\n",
    "            # recall\n",
    "            prediction = list()\n",
    "            # 計算相似使用者與使用者i的評分誤差\n",
    "            for item_idx in range(len(items)):\n",
    "                # 取得使用者i的評分(ground truth)\n",
    "                rth = test_matrix[i, item_idx]\n",
    "                # 如果使用者i有進行評分，則才納入計算RMSE\n",
    "                if rth != 0:\n",
    "                    # 之後需剔除對電影m未評分的相似使用者，因此先進行複製，才不會影響下一部電影的計算\n",
    "                    copy_Suv = copy.deepcopy(Suv)\n",
    "                    # R: 若相似使用者對電影 m 有評分則進行調整\n",
    "                    R = list()\n",
    "                    # 判斷相似使用者是否對電影ｍ有評分，若有評分則將原始評分減去該使用者對電影m的bias\n",
    "                    for c, j in enumerate(top_sim_index):\n",
    "                        if  test_matrix[j, item_idx] == 0:\n",
    "                            R.append(0)\n",
    "                            copy_Suv[c] = 0\n",
    "                        else:\n",
    "                            R.append(test_matrix[j, item_idx] - bias_matrix[j, item_idx])\n",
    "                    # 如果所有相似使用者都沒評分則跳過此次計算\n",
    "                    if sum(R) != 0:\n",
    "                        # 預測使用者i對於第m部電影的評分 + 使用者i對電影m的偏差\n",
    "                        Rui = predict(copy_Suv, R) + bias_matrix[i, item_idx]\n",
    "                        # 計算square error\n",
    "                        delta_list.append(util.se(rth, Rui))\n",
    "                        # 儲存預測結果, 並取四捨五入\n",
    "                        predict_array[i, item_idx] = Rui\n",
    "        # 各評估指標\n",
    "        evaluation[f'{s}_rmse']= util.rmse(delta_list)\n",
    "        evaluation[f'{s}_recall@10'] = recall_k(test_matrix, predict_array) \n",
    "        evaluation[f'{s}_NDCG@10']=ndcg_score(test_matrix, predict_array, k=10)\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = user_sim_score(len_users, movies, training_data, testing_data)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "print(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = user_sim_score(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = user_sim_score(douban_users, books, douban_training_data, douban_testing_data)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76ff5f-9a67-4ed0-9a0f-f4f94add8de9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Item-based Collaborative Filtering (I-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b2f05-9860-4be9-b6be-88fb7e345438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "def item_sim_score(users, items, train_data, test_data, k=int(config['CF']['user_K'])):\n",
    "    # make matrix\n",
    "    user_matrix = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "    item_matrix = user_matrix.T \n",
    "    item_test = test_matrix.T\n",
    "    #item_test = sparse.csr_matrix(item_test)\n",
    "    del test_matrix\n",
    "    \n",
    "    # 計算bias\n",
    "    bias_matrix = util.get_bias(user_matrix, users, items)\n",
    "    item_bias = bias_matrix.T\n",
    "    del bias_matrix\n",
    "    del user_matrix\n",
    "    \n",
    "    # 計算相似度\n",
    "    #cos, pcc = util.get_sim_array(item_matrix)\n",
    "    #cosine_dis = cos -  np.identity(len(items))\n",
    "    #cosine_dis = sparse.csr_matrix(cosine_dis)\n",
    "    #pcc_dis = pcc -  np.identity(len(items))\n",
    "    #pcc_dis = sparse.csr_matrix(pcc_dis)\n",
    "    #sim = {\"cos\":cosine_dis, \"pcc\":pcc_dis}\n",
    "    sim = [\"cos\", \"pcc\"]\n",
    "    evaluation = dict()\n",
    "    for s in sim:\n",
    "        delta_list = list()\n",
    "        predict_array = np.zeros((item_test.shape))\n",
    "        # predict array to spase\n",
    "        predict_array = sparse.csr_matrix(predict_array)\n",
    "        sim_array = util.get_sim_array(item_matrix, sim=s)\n",
    "        sim_dis = sim_array -  np.identity(len(items))\n",
    "        # sim_dis = sim[s]\n",
    "        for i in tqdm(range(len(items)), desc=f\"ICF predicting {s} score with {k}\"):\n",
    "            # Siv: 取出前K個最相似的使用者相似度 ex:K=3, output=[0.378, 0.353, 0.336]\n",
    "            Siv = heapq.nlargest(k ,sim_dis[i])\n",
    "            # 若i不存在，則跳過\n",
    "            if np.isnan(sim_dis[i]).all():\n",
    "                continue\n",
    "            sim_dis[i][np.isnan(sim_dis[i])] = 0\n",
    "            # top_sim_index: 取出與使用者i最為相似的前K個使用者 ex:K=3, output=[915, 406, 214]\n",
    "            sim_dis_idx = sim_dis[i].tolist()\n",
    "            top_sim_index = list(map(sim_dis_idx.index, heapq.nlargest(k,sim_dis[i])))\n",
    "            # recall\n",
    "            prediction = list()\n",
    "            # 計算相似電影與電影i的評分誤差\n",
    "            for user_idx in range(len(users)):\n",
    "                # 取得項目i的評分(ground truth)\n",
    "                rth = item_test[i, user_idx]\n",
    "                # 如果使用者i有進行評分，則才納入計算RMSE\n",
    "                if rth != 0:\n",
    "                    # 之後需剔除對電影m未評分的相似使用者，因此先進行複製，才不會影響下一部電影的計算\n",
    "                    copy_Siv = copy.deepcopy(Siv)\n",
    "                    # R: 若相似使用者對電影 m 有評分則進行調整\n",
    "                    R = list()\n",
    "                    # 判斷相似使用者是否對電影ｍ有評分，若有評分則將原始評分減去該使用者對電影m的bias\n",
    "                    for c, j in enumerate(top_sim_index):\n",
    "                        if  item_test[j, user_idx] == 0:\n",
    "                            R.append(0)\n",
    "                            copy_Siv[c] = 0\n",
    "                        else:\n",
    "                            R.append(item_test[j, user_idx] - item_bias[j, user_idx])\n",
    "                    # 如果所有相似使用者都沒評分則跳過此次計算\n",
    "                    if sum(R) != 0:\n",
    "                        # 預測使用者i對於第m部電影的評分 + 使用者i對電影m的偏差\n",
    "                        Rui = predict(copy_Siv, R) + item_bias[i, user_idx]\n",
    "                        # 計算square error\n",
    "                        delta_list.append(util.se(rth, Rui))\n",
    "                        # 儲存預測結果, 並取四捨五入\n",
    "                        if np.isnan(Rui):\n",
    "                            Rui=0\n",
    "                        predict_array[i, user_idx] = Rui\n",
    "        \n",
    "        \n",
    "        # 各評估指標\n",
    "        delta_list = pd.Series(delta_list, dtype=object).fillna(0).tolist()\n",
    "        evaluation[f'{s}_rmse']= util.rmse(delta_list)\n",
    "        evaluation[f'{s}_recall@10'] = recall_k(item_test, predict_array) \n",
    "        evaluation[f'{s}_NDCG@10']=ndcg_score(item_test, predict_array.toarray(), k=10)\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"ICF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = item_sim_score(len_users, movies, training_data, testing_data)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"ICF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = item_sim_score(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"ICF\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = item_sim_score(douban_users, books, douban_training_data, douban_testing_data)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555705a2-5424-4878-8f2d-d0c87a5de4d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e38937-3e80-412a-9a71-4e68c459cf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "# 進行測試資料驗證評估\n",
    "def test(test_data, p, q, gu=False, bu=False, bi=False):\n",
    "    rmse_test = list()\n",
    "\n",
    "    for test in test_data:\n",
    "        user = test[0] - 1\n",
    "        movie = test[1] - 1\n",
    "        # 判斷是否有bias\n",
    "        if gu and bu.any() and bi.any():\n",
    "            rmse_test.append(util.se(test[2], (np.dot(p[user], q[movie]) + gu + bu[user] + bi[movie])))\n",
    "        else:\n",
    "            rmse_test.append(util.se(test[2], (np.dot(p[user], q[movie]))))\n",
    "    return util.rmse(rmse_test)\n",
    "\n",
    "def execute_matrix_factorization(users, items, train_data, test_data):\n",
    "    # 存放測試資料集的rmse結果\n",
    "    MF_bias_testing = list()\n",
    "    # init evaluation\n",
    "    evaluation = dict()\n",
    "    user_item = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "\n",
    "    # init setting global mean\n",
    "    gu= util.get_u(user_item)\n",
    "    # init setting user mean as bias\n",
    "    bu = np.array([util.get_ubias(user_item, i) - gu for i in range(len(users))])\n",
    "    # init setting items mean as bias\n",
    "    bi = np.array([util.get_ibias(user_item, m) - gu for m in range(len(items))])\n",
    "\n",
    "    # init lentent vector\n",
    "    K = int(config[\"MF\"][\"latent_vector_number\"])\n",
    "    # init user lentent matrix\n",
    "    P = np.random.uniform(low=0, high=3, size=(users.max(), K))\n",
    "    # init items lentent matrix\n",
    "    Q = np.random.uniform(low=0, high=3, size=(items.max(), K))\n",
    "\n",
    "    # parameter\n",
    "    epochs = int(config[\"MF\"][\"epochs\"])\n",
    "    alpha = float(config[\"MF\"][\"alpha\"])\n",
    "    l = float(config[\"MF\"][\"learning_rate\"])\n",
    "\n",
    "    # 更新次數, init=100\n",
    "    for epoch in range(epochs):\n",
    "        # 存放 spuare error 結果\n",
    "        se_list = list()\n",
    "        # 針對user有評分過的rating位置進行更新(User Latent Matrix)\n",
    "        for j in range(len(users)):\n",
    "            # 找出被使用者j評分過的電影\n",
    "            # movie_index = [i for i, e in enumerate(user_item[j]) if e != 0]\n",
    "            movie_index = np.nonzero(user_item[j])[0]\n",
    "            for m in movie_index:\n",
    "                # 對u 做偏微分進行ＳＧＤ更新\n",
    "                tmp_gu = gu - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(gu))\n",
    "                # 對bu 做偏微分進行ＳＧＤ更新\n",
    "                tmp_bu = bu[j] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(bu[j]))\n",
    "                # 對bi 做偏微分進行ＳＧＤ更新\n",
    "                tmp_bi = bi[m] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(bi[m]))\n",
    "                # 若user item 有值則對Q的相對欄位進行SGD更新, 將更新後user latent matrix先暫存\n",
    "                tmp = Q[m] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) * P[j] + l*(Q[m]))\n",
    "                # 更新 movie latent matrix\n",
    "                P[j] -= alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) * Q[m] + l*(P[j]))\n",
    "                # 更新 user latent matrix\n",
    "                Q[m] = tmp\n",
    "                # 更新bias\n",
    "                gu = tmp_gu\n",
    "                bu[j] = tmp_bu\n",
    "                bi[m] = tmp_bi\n",
    "                # 計算ＳＥ\n",
    "                se_list.append(util.se(user_item[j, m], (np.dot(P[j], Q[m]) + gu + bu[j] + bi[m])))\n",
    "                \n",
    "        # 進行驗證資料測試\n",
    "        MF_bias_testing.append(test(test_data, P, Q, gu, bu, bi))\n",
    "        if epoch % 9 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] gu={gu}, bu={np.mean(bu)}, bi={np.mean(bi)}, testing error={MF_bias_testing[-1]}\")\n",
    "\n",
    "    # 各評估指標\n",
    "    evaluation['rmse']= MF_bias_testing[-1]\n",
    "    evaluation['recall@10'] = recall_k(test_matrix, np.dot(P, Q.T))\n",
    "    evaluation['NDCG@10'] = ndcg_score(test_matrix, np.dot(P, Q.T))\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_matrix_factorization(len_users, movies, training_data, testing_data)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"MF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = execute_matrix_factorization(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"MF\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = execute_matrix_factorization(douban_users, books, douban_training_data, douban_testing_data)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345f5f0-cb98-436a-bc18-3d33e399f6cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. BPR-MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed929932-c906-4d20-b84c-3c8329c6d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "# 進行測試資料驗證評估\n",
    "def test(test_uij, users, items, p, q, gu=False, bu=False, bi=False):\n",
    "    rmse_test = list()\n",
    "    \n",
    "    \n",
    "    for u, i, j, rank in test_uij:\n",
    "        u_idx = u - 1\n",
    "        i_idx = i - 1\n",
    "        j_idx = j - 1\n",
    "        rui = np.dot(p[u_idx], q[i_idx])\n",
    "        ruj = np.dot(p[u_idx], q[j_idx])\n",
    "        x_uij =  rui - ruj\n",
    "        # sigmoid\n",
    "        exp_x = np.exp(-x_uij)\n",
    "        y_hat = 1/(1 + np.exp(exp_x))\n",
    "        \n",
    "        rmse_test.append(util.se(y_hat, rank))\n",
    "        \n",
    "    return util.rmse(rmse_test)\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "def execute_bpr_matrix_factorization(users, items, train_data, test_data):\n",
    "    # 存放測試資料集的rmse結果\n",
    "    MF_bias_testing = list()\n",
    "    # init evaluation\n",
    "    evaluation = dict()\n",
    "    user_item = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "\n",
    "    # init setting global mean\n",
    "    gu= util.get_u(user_item)\n",
    "    # init setting user mean as bias\n",
    "    bu = np.array([util.get_ubias(user_item, i) - gu for i in range(len(users))])\n",
    "    # init setting items mean as bias\n",
    "    bi = np.array([util.get_ibias(user_item, m) - gu for m in range(len(items))])\n",
    "\n",
    "    # init lentent vector\n",
    "    K = int(config[\"MF\"][\"latent_vector_number\"])\n",
    "    # init user lentent matrix\n",
    "    # P = np.random.uniform(low=0, high=3, size=(users.max(), K))\n",
    "    P = np.random.randn(users.max(), K)/10\n",
    "    # init items lentent matrix\n",
    "    # Q = np.random.uniform(low=0, high=3, size=(items.max(), K))\n",
    "    Q = np.random.randn(items.max(), K)/10\n",
    "    \n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(train_data, users, items)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    # uij = np.vstack(uij_pos, uij_neg)\n",
    "    test_uij_pos, test_uij_neg = get_uij(test_data, users, items)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "\n",
    "    # parameter\n",
    "    epochs = int(config[\"MF\"][\"epochs\"])\n",
    "    alpha = float(config[\"MF\"][\"alpha\"])\n",
    "    l = float(config[\"MF\"][\"learning_rate\"])\n",
    "\n",
    "    # 更新次數, init=100\n",
    "    for epoch in range(epochs):\n",
    "        # 針對user有評分過的rating位置進行更新(User Latent Matrix)\n",
    "        for u, i, j, rank in uij_pos:\n",
    "            # 計算x_uij\n",
    "            # rui = np.dot(P[u], Q[i]) + gu + bu[u] + bi[i]\n",
    "            # ruj = np.dot(P[u], Q[j]) + gu + bu[u] + bi[j]\n",
    "            i_idx = i-1\n",
    "            j_idx = j-1\n",
    "            u_idx = u-1\n",
    "            rui = np.dot(P[u_idx], Q[i_idx])\n",
    "            ruj = np.dot(P[u_idx], Q[j_idx])\n",
    "            x_uij =  rui - ruj\n",
    "            \n",
    "            # sigmoid\n",
    "            exp_x = np.exp(-x_uij)\n",
    "            partial_BPR = 1/(1 + np.exp(exp_x))\n",
    "            \n",
    "            # 更新 user latent matrix\n",
    "            New_P= alpha * (partial_BPR*(Q[i_idx]-Q[j_idx]) + l*(P[u_idx]))\n",
    "            # 若user item 有值則對Q的相對欄位進行SGD更新, 將更新後user latent matrix先暫存\n",
    "            Q[i_idx] -=  alpha * (partial_BPR*P[u_idx] + l*(Q[i_idx]))\n",
    "            Q[j_idx] -=  alpha * (partial_BPR*-P[u_idx] + l*(Q[j_idx]))\n",
    "            # 更新 user latent matrix\n",
    "            P[u_idx] = New_P\n",
    "            # # 更新bias\n",
    "            # # 對u 做偏微分進行ＳＧＤ更新\n",
    "            # gu = gu - alpha * ((rui - user_item[j,m]) + l*(gu))\n",
    "            # # 對bu 做偏微分進行ＳＧＤ更新\n",
    "            # bu[j] = bu[j] - alpha * ((rui - user_item[j,m]) + l*(bu[j]))\n",
    "            # # 對bi 做偏微分進行ＳＧＤ更新\n",
    "            # bi[m] = bi[m] - alpha * ((rui - user_item[j,m]) + l*(bi[m]))\n",
    "            \n",
    "\n",
    "                \n",
    "        # 進行驗證資料測試\n",
    "        MF_bias_testing.append(test(test_uij, users, items, P, Q))\n",
    "        if epoch % 9 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] testing error={MF_bias_testing[-1]}\")\n",
    "    \n",
    "    rui = np.dot(P[u_idx], Q[i_idx])\n",
    "    ruj = np.dot(P[u_idx], Q[j_idx])\n",
    "    x_uij =  rui - ruj\n",
    "    print(\"{} user like item{} more than item{}, score is {}: \".format(u, i, j, x_uij))\n",
    "    # 各評估指標\n",
    "    evaluation['rmse']= MF_bias_testing[-1]\n",
    "    evaluation['recall@10'] = recall_k(test_matrix, np.dot(P, Q.T))\n",
    "    evaluation['NDCG@10'] = ndcg_score(test_matrix, np.dot(P, Q.T))\n",
    "    \n",
    "    return evaluation, P, Q\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt, P, Q = execute_bpr_matrix_factorization(len_users, movies, training_data, testing_data)\n",
    "# print(movie_reuslt)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"BPR-MF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = execute_bpr_matrix_factorization(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_bpr_matrix_factorization(douban_users, books, douban_training_data, douban_testing_data)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282af117-a92e-423b-9046-37d7649708f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac852f2d-998e-4559-a13e-6918187bde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywFM\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_factorization_machine(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} FM Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "\n",
    "        # reshape y\n",
    "        y_train = y_train.reshape(1, -1)[0]\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        fm = pywFM.FM(task='regression')\n",
    "\n",
    "        model = fm.run(X_train, y_train, X_val, y_val)\n",
    "        predict_values = model.predictions\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(predict_values - y_val))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_factorization_machine(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"FM\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa178ba6-8d9b-49d5-8ffc-c6df7c4190b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. BPR-FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92308f-b439-47b8-8d2f-47936b751813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywFM\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_bpr_factorization_machine(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} FM Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.reshape(1, -1)[0]\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        fm = pywFM.FM(task='classification')\n",
    "\n",
    "        model = fm.run(X_train, y_train, X_val, y_val)\n",
    "        predict_values = model.predictions\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_bpr_factorization_machine(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"BPR-FM\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = execute_bpr_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_bpr_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460fc07-b61b-4a62-b5bc-df381606ea0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7.GBDT+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "159f1913-085a-46f4-8cae-607b071f536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Yelp:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1c6s7y7d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">clone-astromech-16</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/1c6s7y7d\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_yelp/runs/1c6s7y7d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220504_135654-1c6s7y7d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1c6s7y7d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 14:10:19.612330: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-04 14:10:19.612369: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220504_141012-xca51zux</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/xca51zux\" target=\"_blank\">rogue-carrier-17</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_yelp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 0 GBDT+LR Cross-Validation\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 108. GiB for an array with shape (660553, 22026) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     60\u001b[0m                         entity\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     61\u001b[0m                         group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGBDT_LR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m wandb_log \u001b[38;5;241m=\u001b[39m WandbLog()\n\u001b[0;32m---> 63\u001b[0m yelp_reuslt \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_gbdt_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_index_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myelp_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m wandb_log\u001b[38;5;241m.\u001b[39mlog_evaluation(yelp_reuslt)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(yelp_reuslt)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mexecute_gbdt_lr\u001b[0;34m(X, y, X_test, y_test, training_index, test_index, users, items)\u001b[0m\n\u001b[1;32m     29\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m     30\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgb_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, gbf), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m\"\u001b[39m, lr)])\n\u001b[0;32m---> 32\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, y_train)\n\u001b[1;32m     34\u001b[0m predict_values \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_val\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m     35\u001b[0m predict \u001b[38;5;241m=\u001b[39m generate_eval_array(predict_values, val_index, users, items)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/compressed.py:1039\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1039\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/base.py:1202\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 108. GiB for an array with shape (660553, 22026) and data type float64"
     ]
    }
   ],
   "source": [
    "from sktools import GradientBoostingFeatureGenerator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_gbdt_lr(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} GBDT+LR Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.ravel()\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        gbf = GradientBoostingFeatureGenerator(regression=True)\n",
    "        lr = LogisticRegression()\n",
    "        pipe = Pipeline([(\"gb_features\", gbf), (\"logistic\", lr)])\n",
    "        \n",
    "        pipe.fit(X_train.toarray(), y_train)\n",
    "\n",
    "        predict_values = pipe.predict(X_val.toarray())\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"GBDT_LR\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_gbdt_lr(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "# print(movie_reuslt)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"GBDT_LR\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = execute_gbdt_lr(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"GBDT_LR\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_gbdt_lr(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee77f90-24a7-4de4-bdb5-e863d29b7a2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. XGB-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d45a75-fe62-49a0-9e90-0b917edb4158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Yelp:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xca51zux) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rogue-carrier-17</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/xca51zux\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_yelp/runs/xca51zux</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220504_141012-xca51zux/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xca51zux). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 14:10:34.296354: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-04 14:10:34.296394: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220504_141027-35ojzg2h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/35ojzg2h\" target=\"_blank\">stellar-master-18</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_yelp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 0 XGB+LR Cross-Validation\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 108. GiB for an array with shape (660553, 22026) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     61\u001b[0m                         entity\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     62\u001b[0m                         group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGB_LR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m wandb_log \u001b[38;5;241m=\u001b[39m WandbLog()\n\u001b[0;32m---> 64\u001b[0m yelp_reuslt \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_xgb_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_index_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index_yelp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myelp_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m wandb_log\u001b[38;5;241m.\u001b[39mlog_evaluation(yelp_reuslt)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(yelp_reuslt)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mexecute_xgb_lr\u001b[0;34m(X, y, X_test, y_test, training_index, test_index, users, items)\u001b[0m\n\u001b[1;32m     30\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m     31\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, gbf), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m\"\u001b[39m, lr)])\n\u001b[0;32m---> 33\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, y_train)\n\u001b[1;32m     35\u001b[0m predict_values \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_val\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m     36\u001b[0m predict \u001b[38;5;241m=\u001b[39m generate_eval_array(predict_values, val_index, users, items)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/compressed.py:1039\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1039\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/base.py:1202\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 108. GiB for an array with shape (660553, 22026) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_xgb_lr(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} XGB+LR Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.ravel()\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        gbf = SelectFromModel(estimator=XGBRegressor(), max_features=100, threshold=-np.inf)\n",
    "        lr = LogisticRegression()\n",
    "        pipe = Pipeline([(\"xgb_features\", gbf), (\"logistic\", lr)])\n",
    "        \n",
    "        pipe.fit(X_train.toarray(), y_train)\n",
    "\n",
    "        predict_values = pipe.predict(X_val.toarray())\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"XGB_LR\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_xgb_lr(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "# print(movie_reuslt)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"XGB_LR\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = execute_xgb_lr(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"XGB_LR\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_xgb_lr(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fd429-f996-4335-a26f-338efe74ee54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. NN-based RecSys Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978674b1-f0e1-44c7-b8ac-7e12b7989bb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08cd156a-7d29-43ae-94fb-423513aacc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 02:34:50.478459: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-05 02:34:50.478501: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from models.nn_based_models import DeepCTRModel\n",
    "\n",
    "\n",
    "def deepfm(dataframe, testing_data, test_index, users, items, dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"DeepFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result = deer.DeepFM(dataframe, testing_data, test_index, users, items)\n",
    "    clear_output()\n",
    "    print(f\"DeepFM={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def nfm(dataframe, testing_data, test_index, users, items, dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"NFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result = deer.NFM(dataframe, testing_data, test_index, users, items)\n",
    "    clear_output()\n",
    "    print(f\"NFM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def dcn(dataframe, testing_data, test_index, users, items, dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"DCN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result = deer.DCN(dataframe, testing_data, test_index, users, items)\n",
    "    clear_output()\n",
    "    print(f\"DCN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def wd(dataframe, testing_data, test_index, users, items, dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"W&D\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result = deer.WD(dataframe, testing_data, test_index, users, items)\n",
    "    clear_output()\n",
    "    print(f\"W&D={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def ccpm(dataframe, testing_data, test_index, users, items, dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'],\n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"CCPM\",\n",
    "                        reinit=True)\n",
    "    # no surppot dense\n",
    "    deer = DeepCTRModel(sparse, y=y)\n",
    "    result = deer.CCPM(dataframe, testing_data, test_index, users, items)\n",
    "    clear_output()\n",
    "    print(f\"CCPM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def fnn(dataframe, testing_data, test_index, users, items, dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"FNN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result, _ = deer.FNN(dataframe, testing_data, test_index, users, items)\n",
    "    clear_output()\n",
    "    print(f\"FNN={result}\")\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "def ipnn(dataframe, testing_data, test_index, users, items, dataset='movielens', inner=True, outter=False,\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"IPNN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result, _ = deer.PNN(dataframe, testing_data, test_index, users, items, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"IPNN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def opnn(dataframe, testing_data, test_index, users, items, dataset='movielens', inner=False, outter=True,\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"OPNN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result, _ = deer.PNN(dataframe, testing_data, test_index, users, items, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"OPNN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def pin(dataframe, testing_data, test_index, users, items, dataset='movielens', inner=True, outter=True,\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"PIN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result = deer.PNN(dataframe, testing_data, test_index, users, items, inner=inner, outter=outter)\n",
    "    clear_output()\n",
    "    print(f\"PIN={result}\")\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efb099-3ab2-443d-a617-f8f967e03c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7981ea-1c09-4332-9a81-bd52e6f3fbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFM={'rmse': 1.5167813022881063, 'recall@10': 0.6753682608125133, 'ndcg@10': 0.9097272173482424}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ndcg@10</td><td>0.90973</td></tr><tr><td>recall@10</td><td>0.67537</td></tr><tr><td>rmse</td><td>1.51678</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">old-force-29</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_douban/runs/11hnfsgm\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_douban/runs/11hnfsgm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220505_143118-11hnfsgm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# # 1. FM-supported Neural Networks\n",
    "# fnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 2. Product-based Neural Networks\n",
    "# ipnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# opnn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# pin(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 3. Convolutional Click Prediction Model \n",
    "# ccpm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 4. neumf\n",
    "# # 5. Wide&Deep\n",
    "# wd(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 6. Deep Drossing\n",
    "# dcn(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 7. Neural Factorization Machine\n",
    "# nfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 8. Deep Factorization Machine\n",
    "# deepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# # 1. FM-supported Neural Networks\n",
    "# fnn(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# # 2. Product-based Neural Networks\n",
    "# ipnn(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# opnn(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# pin(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "\n",
    "# 3. Convolutional Click Prediction Model \n",
    "# ccpm(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category', 'user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# 4. neumf\n",
    "# 5. Wide&Deep\n",
    "# wd(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# # 6. Deep Drossing\n",
    "# dcn(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# # 7. Neural Factorization Machine\n",
    "# nfm(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# # 8. Deep Factorization Machine\n",
    "# deepfm(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "# # 1. FM-supported Neural Networks\n",
    "# fnn(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "#     sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "#     dense=['book_year'],\n",
    "#     y = ['rating'])\n",
    "# # 2. Product-based Neural Networks\n",
    "# ipnn(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "#     sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "#     dense=['book_year'],\n",
    "#     y = ['rating'])\n",
    "# opnn(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "#     sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "#     dense=['book_year'],\n",
    "#     y = ['rating'])\n",
    "# pin(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "#     sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "#     dense=['book_year'],\n",
    "#     y = ['rating'])\n",
    "\n",
    "# 3. Convolutional Click Prediction Model \n",
    "ccpm(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "    sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "    y = ['rating'])\n",
    "# 4. neumf\n",
    "# 5. Wide&Deep\n",
    "wd(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "    sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "    dense=['book_year'],\n",
    "    y = ['rating'])\n",
    "# 6. Deep Drossing\n",
    "dcn(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "    sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "    dense=['book_year'],\n",
    "    y = ['rating'])\n",
    "# 7. Neural Factorization Machine\n",
    "nfm(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "    sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "    dense=['book_year'],\n",
    "    y = ['rating'])\n",
    "# 8. Deep Factorization Machine\n",
    "deepfm(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "    sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "    dense=['book_year'],\n",
    "    y = ['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f83b5-a90d-4570-b030-70937d006dfe",
   "metadata": {},
   "source": [
    "## 10. Recent NN-based RecSys Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5eaab9-115a-4f2b-bd71-7bb692e6c27d",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9cdc365-b79b-4928-9119-e1eb0723d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nn_based_models import DeepCTRModel\n",
    "\n",
    "def din(train_df, test_df, test_index, users, items, watch_history = ['movie', 'movie_genre'], target=\"rating\", dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"DIN\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result = deer.DIN(train_df, test_df, test_index, users, items, watch_history, target)\n",
    "    clear_output()\n",
    "    print(f\"DIN={result}\")\n",
    "    run.finish()\n",
    "\n",
    "def xdeepfm(dataframe, testing_data, test_index, users, items, dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"xDeepFM\",\n",
    "                        reinit=True)\n",
    "    deer = DeepCTRModel(sparse, dense, y)\n",
    "    result = deer.xDeepFM(dataframe, testing_data, test_index, users, items)\n",
    "    clear_output()\n",
    "    print(f\"xDeepFM={result}\")\n",
    "    run.finish()\n",
    "    \n",
    "def afm(dataframe, testing_data, test_index, users, items, dataset='movielens',\n",
    "            sparse=['user', 'movie', 'movie_genre', 'user_occupation'], \n",
    "            dense=['user_age'], \n",
    "            y=['rating']):\n",
    "    run = wandb.init(project=config['general'][dataset],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"AFM\",\n",
    "                        reinit=True)\n",
    "    # no dense\n",
    "    deer = DeepCTRModel(sparse, y=y)\n",
    "    result = deer.AFM(dataframe, testing_data, test_index, users, items)\n",
    "    clear_output()\n",
    "    print(f\"AFM={result}\")\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765641d-6037-4366-9e63-a8fb1cbb78c4",
   "metadata": {},
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71aa6504-66b8-49ab-829b-f4cb09ddd022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xDeepFM={'rmse': 1.5175224400558918, 'recall@10': 0.6753067660073391, 'ndcg@10': 0.9095881425164742}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ndcg@10</td><td>0.90959</td></tr><tr><td>recall@10</td><td>0.67531</td></tr><tr><td>rmse</td><td>1.51752</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">tusken-cruiser-31</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_douban/runs/2706y8xz\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_douban/runs/2706y8xz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220505_150803-2706y8xz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 15:42:37.593302: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-05 15:42:37.593342: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220505_154235-1deqfqcj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_douban/runs/1deqfqcj\" target=\"_blank\">galactic-bantha-32</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_douban\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trasfer history items: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 383100/383100 [00:22<00:00, 17317.89it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 63.8 GiB for an array with shape (383100, 22347) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m xdeepfm(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouban\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     32\u001b[0m     sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_location\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_author\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_publisher\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_group\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     33\u001b[0m     dense\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_year\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     34\u001b[0m     y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 4. Deep Interest Network\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mdin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdouban_training_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouban_testing_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index_douban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouban_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatch_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_publisher\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdouban\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_location\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_author\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_publisher\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_group\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mdin\u001b[0;34m(train_df, test_df, test_index, users, items, watch_history, target, dataset, sparse, dense, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][dataset],\n\u001b[1;32m      8\u001b[0m                     entity\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m                     group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m                     reinit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m deer \u001b[38;5;241m=\u001b[39m DeepCTRModel(sparse, dense, y)\n\u001b[0;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdeer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDIN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatch_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m clear_output()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIN=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/HW/Recommender_System/models/nn_based_models.py:460\u001b[0m, in \u001b[0;36mDeepCTRModel.DIN\u001b[0;34m(self, dataframe, test_dataframe, test_index, users, items, history, target)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mDIN\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataframe, test_dataframe, test_index, users, items, history, target):\n\u001b[0;32m--> 460\u001b[0m     test_x, test_y, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_din_xy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# init evaluation\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/HW/Recommender_System/models/nn_based_models.py:60\u001b[0m, in \u001b[0;36mDeepCTRModel.get_din_xy\u001b[0;34m(self, dataframe, users, items, history, target)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_din_xy\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataframe, users, items, history, target):\n\u001b[0;32m---> 60\u001b[0m     data_dict, y \u001b[38;5;241m=\u001b[39m \u001b[43mget_din_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatch_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     feature_columns \u001b[38;5;241m=\u001b[39m [SparseFeat(feat, vocabulary_size\u001b[38;5;241m=\u001b[39mdataframe[feat]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     62\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i,feat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sparse_features)] \u001b[38;5;241m+\u001b[39m [DenseFeat(feat, \u001b[38;5;241m1\u001b[39m,)\n\u001b[1;32m     63\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dense_features]\n\u001b[1;32m     64\u001b[0m     feature_columns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     65\u001b[0m         VarLenSparseFeat(SparseFeat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhist_movie\u001b[39m\u001b[38;5;124m'\u001b[39m, vocabulary_size\u001b[38;5;241m=\u001b[39mdataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, embedding_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     66\u001b[0m                         maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1682\u001b[39m, length_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     67\u001b[0m         VarLenSparseFeat(SparseFeat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhist_movie_genre\u001b[39m\u001b[38;5;124m'\u001b[39m, dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_genre\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, embedding_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_genre\u001b[39m\u001b[38;5;124m'\u001b[39m), maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1682\u001b[39m,\n\u001b[1;32m     68\u001b[0m                         length_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/HW/Recommender_System/dataaccessframeworks/data_preprocessing.py:343\u001b[0m, in \u001b[0;36mget_din_data\u001b[0;34m(df, users, items, watch_history, target)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# 轉換成array\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dis \u001b[38;5;129;01min\u001b[39;00m output_history\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 343\u001b[0m     output_history[dis] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_history\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# 返回合併後的字典\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdf_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_history}, y\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 63.8 GiB for an array with shape (383100, 22347) and data type float64"
     ]
    }
   ],
   "source": [
    "# # 1. Attentional Factorization Machines\n",
    "# afm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 3. xDeepFM\n",
    "# xdeepfm(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "# # 4. Deep Interest Network\n",
    "# din(movielens_training_df, movielens_testing_df, test_index, len_users, movies)\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# # 1. Attentional Factorization Machines\n",
    "# afm(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category', 'user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# # 3. xDeepFM\n",
    "# xdeepfm(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "# # 4. Deep Interest Network\n",
    "# din(yelp_training_df, yelp_testing_df, test_index_yelp, yelp_users, business, watch_history = ['business', 'business_category'], dataset='yelp', \n",
    "#     sparse=['user', 'business', 'business_city', 'business_category'],\n",
    "#     dense=['user_compliment'],\n",
    "#     y = ['rating'])\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# 1. Attentional Factorization Machines\n",
    "afm(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "    sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "    dense=['book_year'],\n",
    "    y = ['rating'])\n",
    "# 3. xDeepFM\n",
    "xdeepfm(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, dataset='douban', \n",
    "    sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "    dense=['book_year'],\n",
    "    y = ['rating'])\n",
    "# 4. Deep Interest Network\n",
    "din(douban_training_df, douban_testing_df, test_index_douban, douban_users, books, watch_history = ['book', 'book_publisher'], dataset='douban', \n",
    "    sparse=['user', 'book', 'user_location', 'book_author', 'book_publisher', 'user_group'],\n",
    "    dense=['book_year'],\n",
    "    y = ['rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e206e4c-16f6-4591-a7ab-14bec64fe102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
