{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9d29f8d-13fb-4094-ac45-cf49ef5e22b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/HW/Recommender_System/util/utility.py:9: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if sim is 'cos':\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from imp import reload\n",
    "from dataaccessframeworks.read_data import get_movielens, user_filter, training_testing, get_yelp, get_douban, training_testing_XY\n",
    "from dataaccessframeworks.data_preprocessing import get_one_hot_feature, generate_eval_array\n",
    "from models.collaborative_filtering import get_user_item_matrix, predict\n",
    "from models.evaluation import recall_k\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "import configparser\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from util.mywandb import WandbLog\n",
    "import util.utility as util\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.path.dirname(os.getcwd()), 'config.ini'))\n",
    "LIBFM_PATH = '/home/baron/libfm/bin/'\n",
    "os.environ['LIBFM_PATH'] = LIBFM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0225cdb-d647-405f-8c1d-d0f7e1e8ff60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reload(\u001b[43mdata_preprocessing\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "reload(data_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4011be-f3e9-46b9-a80e-ae726fc3e542",
   "metadata": {},
   "source": [
    "## 0. Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbec23-20b2-4987-985f-8fc797ec5fff",
   "metadata": {},
   "source": [
    "### MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3457ef2-20bb-41c1-84d1-0f98490d7fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_movie:[['196' '242' '3']\n",
      " ['186' '302' '3']\n",
      " ['22' '377' '1']]\n",
      "movie_genre:[['1' '3']\n",
      " ['1' '4']\n",
      " ['1' '5']]\n",
      "user_age:[['1' '3']\n",
      " ['2' '6']\n",
      " ['3' '3']]\n",
      "user_occupation:[['1' '1']\n",
      " ['2' '2']\n",
      " ['3' '3']]\n",
      "使用者評分大於三次的共有：(100000, 3)\n",
      "[0/943] uij_pos: (204, 4), uij_neg: (200, 4)\n",
      "[300/943] uij_pos: (28345, 4), uij_neg: (28158, 4)\n",
      "[600/943] uij_pos: (58906, 4), uij_neg: (58516, 4)\n",
      "[900/943] uij_pos: (85434, 4), uij_neg: (84866, 4)\n",
      "uij_positive: (88873, 4), uij_negative: (88274, 4)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from random import sample\n",
    "\n",
    "data = get_movielens()\n",
    "# str to int\n",
    "user_movie = np.array([list(map(int, data)) for data in data['user_movie']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_movie, 0)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 取得電影個數及電影個數\n",
    "len_users, movies = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "training_data,  testing_data = training_testing(filter_data)\n",
    "# get uij index\n",
    "uij_pos, uij_neg = get_uij(training_data, len_users, movies)\n",
    "print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "train_uij = np.vstack((uij_pos, uij_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc7fb39-e567-42b6-b8a5-b5fdb61739b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataaccessframeworks.data_preprocessing import get_feature_map\n",
    "\n",
    "def get_uij_one_hot_feature(data, user_item_col, uij_data, y_col=3, time_col=3, batch_size=10000):\n",
    "    # 取得user及items feature map \n",
    "    users_dict, items_dict, features = get_feature_map(data, user_item_col)\n",
    "\n",
    "    # 將user item 數值轉為integer\n",
    "    # user_items = np.array([list(map(int, data))for data in data[user_item_col]])\n",
    "    # 使用者評分次數小於三筆則剔除\n",
    "    filter_data = user_filter(uij_data, 0)\n",
    "    print(filter_data.shape)\n",
    "    print(filter_data[:5])\n",
    "    # user label encoder\n",
    "    le = LabelEncoder()\n",
    "    filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "    # item label encoder\n",
    "    ile = LabelEncoder()\n",
    "    filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "    filter_data[:, 2] = ile.fit_transform(filter_data[:, 2])\n",
    "    \n",
    "    # 做特徵的onehot encoding \n",
    "    one_hot_encoder_data, y, concat_data = get_uij_onehot_encoding(filter_data, users_dict, items_dict, features, le, ile, batch_size, y_col)\n",
    "\n",
    "    return one_hot_encoder_data, y, concat_data\n",
    "\n",
    "# 取得user及items的one hot encoding map\n",
    "def get_uij_onehot_encoding(data, users_dict, items_dict, features, le, ile, batch_size, y_col):\n",
    "    #users_onehot = get_users_onehot(data)\n",
    "    sparse_, dense = get_uij_feature_onehot(data, users_dict, items_dict, features, le, ile, batch_size)\n",
    "    \n",
    "    # 取得y\n",
    "    y = data[:,y_col].reshape(-1,1)\n",
    "    \n",
    "    # return np.concatenate((sparse_, dense), axis=1), y, concat_data\n",
    "    return sparse.hstack((sparse_, dense), format='csr'), y, data\n",
    "\n",
    "# 取得feature one hot\n",
    "def get_uij_feature_onehot(data, users_feature, items_feature, features_map, le, ile, batch_size):\n",
    "    # 取得user & item個數\n",
    "    user_number = np.max(data[:,0]) + 1\n",
    "    item_number = np.max(data[:,1]) + 1\n",
    "    i_feature = items_feature[1].keys()\n",
    "    # one hot encoding\n",
    "    for b in range(0, data.shape[0], batch_size):\n",
    "        user_one_hot = np.eye(user_number)[data[b:b+batch_size,0]]\n",
    "        itemi_one_hot = np.eye(item_number)[data[b:b+batch_size,1]]\n",
    "        itemj_one_hot = np.eye(item_number)[data[b:b+batch_size,2]]\n",
    "        sparse_ = np.concatenate((user_one_hot, itemi_one_hot, itemj_one_hot), axis=1)\n",
    "        dense = np.empty((user_one_hot.shape[0], 1), int)\n",
    "\n",
    "        # create items feature \n",
    "        i_feature = items_feature[1].keys()\n",
    "        for fe in i_feature:\n",
    "            # sparse\n",
    "            if fe.split(\"_\")[1] != 'year':\n",
    "                f_map = features_map[fe]\n",
    "                feature_lengh = f_map[list(f_map.keys())[0]].shape[1]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), feature_lengh*2))\n",
    "                for i, item_ij in enumerate(data[b:b+batch_size, 1:3]):\n",
    "                    item_i, item_j = item_ij\n",
    "                    item_i = ile.inverse_transform(np.array([item_i])).item()\n",
    "                    item_j = ile.inverse_transform(np.array([item_j])).item()\n",
    "                    if item_i not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, :feature_lengh] = features_map[fe][item_i].toarray()\n",
    "                    if item_j not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, feature_lengh:] = features_map[fe][item_j].toarray()\n",
    "                # sparse_ = np.concatenate((sparse_, tmp), axis=1)\n",
    "                sparse_ = np.hstack((sparse_, tmp))\n",
    "            # dense\n",
    "            else:\n",
    "                # i = 0\n",
    "                f_map = features_map[fe]\n",
    "                feature_lengh = f_map[list(f_map.keys())[0]].shape[1]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), feature_lengh*2))\n",
    "                for i, item_ij in enumerate(data[b:b+batch_size, 1:3]):\n",
    "                    item_i, item_j = item_ij\n",
    "                    item_i = ile.inverse_transform(np.array([item_i])).item()\n",
    "                    item_j = ile.inverse_transform(np.array([item_j])).item()\n",
    "                    if item_i not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, :feature_lengh] = features_map[fe][item_i].toarray()\n",
    "                    if item_j not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # item_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        # item_feature_onehot = features_map[fe][item].toarray()\n",
    "                        tmp[i, feature_lengh:] = features_map[fe][item_j].toarray()\n",
    "                # dense = np.concatenate((dense, tmp), axis=1)\n",
    "                dense = np.hstack((dense, tmp))\n",
    "\n",
    "        # create user feature\n",
    "        u_feature = users_feature[1].keys()\n",
    "        for fe in u_feature:\n",
    "            # sparse\n",
    "            if fe.split(\"_\")[1] != 'age':\n",
    "                f_map = features_map[fe]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                for i, user in enumerate(data[b:b+batch_size, 0]):\n",
    "                    # i = 0\n",
    "                    user = le.inverse_transform(np.array([user])).item()\n",
    "                    if user not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # user_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp[i] = features_map[fe][user].toarray()\n",
    "                # sparse_ = np.concatenate((sparse_, tmp), axis=1)\n",
    "                sparse_ = np.hstack((sparse_, tmp))\n",
    "                \n",
    "            # dense\n",
    "            else:\n",
    "                f_map = features_map[fe]\n",
    "                tmp = np.zeros((len(data[b:b+batch_size, 1]), f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                for i, user in enumerate(data[b:b+batch_size, 0]):\n",
    "                    # i = 0\n",
    "                    user = le.inverse_transform(np.array([user])).item()\n",
    "                    if user not in features_map[fe].keys():\n",
    "                        # 取第一個鍵值得長度\n",
    "                        # f_map = features_map[fe]\n",
    "                        # user_feature_onehot = np.zeros((f_map[list(f_map.keys())[0]].shape[1]))\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp[i] = features_map[fe][user].toarray()\n",
    "                # dense = np.concatenate((dense, tmp), axis=1)\n",
    "                dense = np.hstack((dense, tmp))\n",
    "        if b==0:\n",
    "            sparse_matrix = csr_matrix(sparse_)\n",
    "            dense_matrix = dense\n",
    "        else:\n",
    "            sparse_matrix = sparse.vstack((sparse_matrix, csr_matrix(sparse_)))\n",
    "            dense_matrix = np.vstack((dense_matrix, dense))\n",
    "        print(\"[{}/{}] sparse_matrix shape is {}\".format(b, data.shape[0], sparse_matrix.shape))\n",
    "    \n",
    "    return sparse_matrix, dense_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de72e1bc-5f21-45ac-aeb9-272d3b19aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_movie:[['196' '242' '3']\n",
      " ['186' '302' '3']\n",
      " ['22' '377' '1']]\n",
      "movie_genre:[['1' '3']\n",
      " ['1' '4']\n",
      " ['1' '5']]\n",
      "user_age:[['1' '3']\n",
      " ['2' '6']\n",
      " ['3' '3']]\n",
      "user_occupation:[['1' '1']\n",
      " ['2' '2']\n",
      " ['3' '3']]\n",
      "使用者評分大於三次的共有：(100000, 3)\n",
      "[0/943] uij_pos: (227, 4), uij_neg: (226, 4)\n",
      "[300/943] uij_pos: (28204, 4), uij_neg: (27978, 4)\n",
      "[600/943] uij_pos: (58930, 4), uij_neg: (58508, 4)\n",
      "[900/943] uij_pos: (85758, 4), uij_neg: (85158, 4)\n",
      "uij_positive: (89138, 4), uij_negative: (88516, 4)\n",
      "[0/943] uij_pos: (67, 4), uij_neg: (66, 4)\n",
      "[300/943] uij_pos: (7192, 4), uij_neg: (7018, 4)\n",
      "[600/943] uij_pos: (15267, 4), uij_neg: (14948, 4)\n",
      "[900/943] uij_pos: (22326, 4), uij_neg: (21848, 4)\n",
      "testing uij_positive: (23273, 4), testing uij_negative: (22774, 4)\n",
      "users:  943\n",
      "items:  1682\n",
      "(177654, 4)\n",
      "[[   1  238 1418    1]\n",
      " [   1  176 1504    1]\n",
      " [   1  163   54    1]\n",
      " [   1  252  795    1]\n",
      " [   1  228  179    1]]\n",
      "[0/177654] sparse_matrix shape is (100000, 4367)\n",
      "[100000/177654] sparse_matrix shape is (177654, 4367)\n",
      "(142123, 4377) (35531, 4377)\n",
      "(177654, 4377)\n"
     ]
    }
   ],
   "source": [
    "data = get_movielens()\n",
    "# str to int\n",
    "user_movie = np.array([list(map(int, data)) for data in data['user_movie']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_movie, 0)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 取得電影個數及電影個數\n",
    "len_users, movies = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "training_data,  testing_data = training_testing(filter_data)\n",
    "# get uij index\n",
    "uij_pos, uij_neg = get_uij(training_data, len_users, movies)\n",
    "print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "train_uij = np.vstack((uij_pos, uij_neg))\n",
    "test_uij_pos, test_uij_neg = get_uij(testing_data, len_users, movies)\n",
    "print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "\n",
    "# normalize rating value\n",
    "# training_data[:, 2:3] = normalize(training_data[:, 2:3], axis=0)\n",
    "# testing_data[:, 2:3] = normalize(testing_data[:, 2:3], axis=0)\n",
    "# train_min = training_data[:, 2:3].min()\n",
    "# train_max = training_data[:, 2:3].max()\n",
    "# training_rating = (training_data[:, 2] - train_min)/(train_max-train_min)\n",
    "# test_min = testing_data[:, 2:3].min()\n",
    "# test_max = testing_data[:, 2:3].max()\n",
    "# testing_rating = (testing_data[:, 2:3] - test_min)/(test_max-test_min)\n",
    "print(\"users: \", len(len_users))\n",
    "print(\"items: \", len(movies))\n",
    "\n",
    "# generarte one hot encoding\n",
    "run = 'bpr'\n",
    "if run=='bpr':\n",
    "    one_hot_x, y, add_fake_data = get_uij_one_hot_feature(data,  'user_movie', train_uij, batch_size=100000)\n",
    "else:\n",
    "    one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_movie', batch_size=100000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index, test_index, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(one_hot_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140b967-0b12-4efb-8752-75b341b6018f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9291be6-6e04-47d6-af61-f92487ace79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_category:[['1', '334', '1'], ['1', '426', '1'], ['2', '211', '1']]\n",
      "business_city:[['1', '31', '1'], ['2', '35', '1'], ['3', '35', '1']]\n",
      "user_business:[['1', '8391', '5'], ['1', '8971', '5'], ['2', '186', '5']]\n",
      "user_compliment:[['2', '1', '1'], ['2', '2', '1'], ['2', '3', '1']]\n",
      "使用者評分大於三次的共有：(184835, 3)\n",
      "users:  7326\n",
      "items:  14127\n",
      "(184835, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 21453 and the array at index 1 has size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(business))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# generarte one hot encoding\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m one_hot_x, y, add_fake_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_one_hot_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_business\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp \u001b[38;5;241m=\u001b[39m training_testing_XY(one_hot_x, y, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     28\u001b[0m training_index_yelp, test_index_yelp, _, _ \u001b[38;5;241m=\u001b[39m training_testing_XY(add_fake_data, y, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/HW/Recommender_System/dataaccessframeworks/data_preprocessing.py:63\u001b[0m, in \u001b[0;36mget_one_hot_feature\u001b[0;34m(data, user_item_col, y_col, time_col)\u001b[0m\n\u001b[1;32m     60\u001b[0m filter_data[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ile\u001b[38;5;241m.\u001b[39mfit_transform(filter_data[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# 做特徵的onehot encoding \u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m one_hot_encoder_data, y, concat_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_onehot_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m one_hot_encoder_data, y, concat_data\n",
      "File \u001b[0;32m~/HW/Recommender_System/dataaccessframeworks/data_preprocessing.py:73\u001b[0m, in \u001b[0;36mget_onehot_encoding\u001b[0;34m(data, users_dict, items_dict, features, le, ile, y_col)\u001b[0m\n\u001b[1;32m     70\u001b[0m concat_data \u001b[38;5;241m=\u001b[39m get_norating_data(data)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#users_onehot = get_users_onehot(data)\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m sparse_, dense \u001b[38;5;241m=\u001b[39m \u001b[43mget_feature_onehot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# 取得y\u001b[39;00m\n\u001b[1;32m     76\u001b[0m y \u001b[38;5;241m=\u001b[39m concat_data[:,y_col]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/HW/Recommender_System/dataaccessframeworks/data_preprocessing.py:138\u001b[0m, in \u001b[0;36mget_feature_onehot\u001b[0;34m(data, users_feature, items_feature, features_map, le, ile)\u001b[0m\n\u001b[1;32m    136\u001b[0m             tmp[i] = features_map[fe][item].toarray()\n\u001b[1;32m    137\u001b[0m     sparse_ = np.concatenate((sparse_, tmp), axis=1)\n\u001b[0;32m--> 138\u001b[0m # dense\n\u001b[1;32m    139\u001b[0m else:\n\u001b[1;32m    140\u001b[0m     # i = 0\n\u001b[1;32m    141\u001b[0m     f_map = features_map[fe]\n\u001b[1;32m    142\u001b[0m     tmp = np.zeros((data[b:b+batch_size, 1], f_map[list(f_map.keys())[0]].shape[1]))\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 21453 and the array at index 1 has size 512"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = get_yelp()\n",
    "# str to int\n",
    "user_business = np.array([list(map(int, data)) for data in data['user_business']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_business, 0)\n",
    "# user label encoder\n",
    "le = LabelEncoder()\n",
    "filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "filter_data[:, 0] += 1\n",
    "# item label encoder\n",
    "ile = LabelEncoder()\n",
    "filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "filter_data[:, 1] += 1\n",
    "# if want to inverse label \n",
    "# le.inverse_transform(yelp_training_encoder)\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 取得business個數及users個數\n",
    "yelp_users, business = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "# yelp_training_data,  yelp_testing_data = training_testing(filter_data)\n",
    "print(\"users: \", len(yelp_users))\n",
    "print(\"items: \", len(business))\n",
    "# generarte one hot encoding\n",
    "one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_business')\n",
    "X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index_yelp, test_index_yelp, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ba6de-2893-4a33-b9c2-e5a77953b129",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Douban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ad654-29d3-4430-9469-ba209cc7b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uij index for bpr\n",
    "uij_pos, uij_neg = get_uij(np.hstack((X_train, y_train)), users, items)\n",
    "print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "uij = np.vstack(uij_pos, uij_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01661c74-3b39-49c2-a121-7002680d2ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_author:[['12131', '3871'], ['20995', '10690'], ['9905', '3845']]\n",
      "book_publisher:[['12131', '108'], ['20995', '1470'], ['9905', '1696']]\n",
      "book_year:[['9905', '16'], ['21153', '15'], ['12823', '15']]\n",
      "user_book:[['10855', '938', '4'], ['10027', '3', '3'], ['741', '2426', '5']]\n",
      "user_group:[['3587', '232'], ['3587', '666'], ['3587', '226']]\n",
      "user_location:[['3587', '33'], ['3210', '179'], ['7993', '394']]\n",
      "使用者評分大於三次的共有：(788898, 3)\n",
      "users:  11266\n",
      "items:  22347\n",
      "(788898, 3)\n",
      "[0/1915498] sparse_matrix shape is (10000, 49626)\n",
      "[10000/1915498] sparse_matrix shape is (20000, 49626)\n",
      "[20000/1915498] sparse_matrix shape is (30000, 49626)\n",
      "[30000/1915498] sparse_matrix shape is (40000, 49626)\n",
      "[40000/1915498] sparse_matrix shape is (50000, 49626)\n",
      "[50000/1915498] sparse_matrix shape is (60000, 49626)\n",
      "[60000/1915498] sparse_matrix shape is (70000, 49626)\n",
      "[70000/1915498] sparse_matrix shape is (80000, 49626)\n",
      "[80000/1915498] sparse_matrix shape is (90000, 49626)\n",
      "[90000/1915498] sparse_matrix shape is (100000, 49626)\n",
      "[100000/1915498] sparse_matrix shape is (110000, 49626)\n",
      "[150000/1915498] sparse_matrix shape is (160000, 49626)\n",
      "[160000/1915498] sparse_matrix shape is (170000, 49626)\n",
      "[170000/1915498] sparse_matrix shape is (180000, 49626)\n",
      "[180000/1915498] sparse_matrix shape is (190000, 49626)\n",
      "[190000/1915498] sparse_matrix shape is (200000, 49626)\n",
      "[200000/1915498] sparse_matrix shape is (210000, 49626)\n",
      "[210000/1915498] sparse_matrix shape is (220000, 49626)\n",
      "[220000/1915498] sparse_matrix shape is (230000, 49626)\n",
      "[230000/1915498] sparse_matrix shape is (240000, 49626)\n",
      "[240000/1915498] sparse_matrix shape is (250000, 49626)\n",
      "[250000/1915498] sparse_matrix shape is (260000, 49626)\n",
      "[260000/1915498] sparse_matrix shape is (270000, 49626)\n",
      "[270000/1915498] sparse_matrix shape is (280000, 49626)\n",
      "[280000/1915498] sparse_matrix shape is (290000, 49626)\n",
      "[290000/1915498] sparse_matrix shape is (300000, 49626)\n",
      "[300000/1915498] sparse_matrix shape is (310000, 49626)\n",
      "[310000/1915498] sparse_matrix shape is (320000, 49626)\n",
      "[320000/1915498] sparse_matrix shape is (330000, 49626)\n",
      "[330000/1915498] sparse_matrix shape is (340000, 49626)\n",
      "[340000/1915498] sparse_matrix shape is (350000, 49626)\n",
      "[350000/1915498] sparse_matrix shape is (360000, 49626)\n",
      "[360000/1915498] sparse_matrix shape is (370000, 49626)\n",
      "[370000/1915498] sparse_matrix shape is (380000, 49626)\n",
      "[380000/1915498] sparse_matrix shape is (390000, 49626)\n",
      "[390000/1915498] sparse_matrix shape is (400000, 49626)\n",
      "[400000/1915498] sparse_matrix shape is (410000, 49626)\n",
      "[410000/1915498] sparse_matrix shape is (420000, 49626)\n",
      "[420000/1915498] sparse_matrix shape is (430000, 49626)\n",
      "[430000/1915498] sparse_matrix shape is (440000, 49626)\n",
      "[440000/1915498] sparse_matrix shape is (450000, 49626)\n",
      "[450000/1915498] sparse_matrix shape is (460000, 49626)\n",
      "[460000/1915498] sparse_matrix shape is (470000, 49626)\n",
      "[470000/1915498] sparse_matrix shape is (480000, 49626)\n",
      "[480000/1915498] sparse_matrix shape is (490000, 49626)\n",
      "[490000/1915498] sparse_matrix shape is (500000, 49626)\n",
      "[500000/1915498] sparse_matrix shape is (510000, 49626)\n",
      "[510000/1915498] sparse_matrix shape is (520000, 49626)\n",
      "[520000/1915498] sparse_matrix shape is (530000, 49626)\n",
      "[530000/1915498] sparse_matrix shape is (540000, 49626)\n",
      "[540000/1915498] sparse_matrix shape is (550000, 49626)\n",
      "[550000/1915498] sparse_matrix shape is (560000, 49626)\n",
      "[560000/1915498] sparse_matrix shape is (570000, 49626)\n",
      "[570000/1915498] sparse_matrix shape is (580000, 49626)\n",
      "[580000/1915498] sparse_matrix shape is (590000, 49626)\n",
      "[590000/1915498] sparse_matrix shape is (600000, 49626)\n",
      "[600000/1915498] sparse_matrix shape is (610000, 49626)\n",
      "[610000/1915498] sparse_matrix shape is (620000, 49626)\n",
      "[620000/1915498] sparse_matrix shape is (630000, 49626)\n",
      "[630000/1915498] sparse_matrix shape is (640000, 49626)\n",
      "[640000/1915498] sparse_matrix shape is (650000, 49626)\n",
      "[650000/1915498] sparse_matrix shape is (660000, 49626)\n",
      "[660000/1915498] sparse_matrix shape is (670000, 49626)\n",
      "[670000/1915498] sparse_matrix shape is (680000, 49626)\n",
      "[680000/1915498] sparse_matrix shape is (690000, 49626)\n",
      "[690000/1915498] sparse_matrix shape is (700000, 49626)\n",
      "[700000/1915498] sparse_matrix shape is (710000, 49626)\n",
      "[710000/1915498] sparse_matrix shape is (720000, 49626)\n",
      "[720000/1915498] sparse_matrix shape is (730000, 49626)\n",
      "[730000/1915498] sparse_matrix shape is (740000, 49626)\n",
      "[740000/1915498] sparse_matrix shape is (750000, 49626)\n",
      "[750000/1915498] sparse_matrix shape is (760000, 49626)\n",
      "[760000/1915498] sparse_matrix shape is (770000, 49626)\n",
      "[770000/1915498] sparse_matrix shape is (780000, 49626)\n",
      "[780000/1915498] sparse_matrix shape is (790000, 49626)\n",
      "[790000/1915498] sparse_matrix shape is (800000, 49626)\n",
      "[800000/1915498] sparse_matrix shape is (810000, 49626)\n",
      "[810000/1915498] sparse_matrix shape is (820000, 49626)\n",
      "[820000/1915498] sparse_matrix shape is (830000, 49626)\n",
      "[830000/1915498] sparse_matrix shape is (840000, 49626)\n",
      "[840000/1915498] sparse_matrix shape is (850000, 49626)\n",
      "[850000/1915498] sparse_matrix shape is (860000, 49626)\n",
      "[860000/1915498] sparse_matrix shape is (870000, 49626)\n",
      "[870000/1915498] sparse_matrix shape is (880000, 49626)\n",
      "[880000/1915498] sparse_matrix shape is (890000, 49626)\n",
      "[890000/1915498] sparse_matrix shape is (900000, 49626)\n",
      "[900000/1915498] sparse_matrix shape is (910000, 49626)\n",
      "[910000/1915498] sparse_matrix shape is (920000, 49626)\n",
      "[920000/1915498] sparse_matrix shape is (930000, 49626)\n",
      "[930000/1915498] sparse_matrix shape is (940000, 49626)\n",
      "[940000/1915498] sparse_matrix shape is (950000, 49626)\n",
      "[950000/1915498] sparse_matrix shape is (960000, 49626)\n",
      "[960000/1915498] sparse_matrix shape is (970000, 49626)\n",
      "[970000/1915498] sparse_matrix shape is (980000, 49626)\n",
      "[980000/1915498] sparse_matrix shape is (990000, 49626)\n",
      "[990000/1915498] sparse_matrix shape is (1000000, 49626)\n",
      "[1000000/1915498] sparse_matrix shape is (1010000, 49626)\n",
      "[1010000/1915498] sparse_matrix shape is (1020000, 49626)\n",
      "[1020000/1915498] sparse_matrix shape is (1030000, 49626)\n",
      "[1030000/1915498] sparse_matrix shape is (1040000, 49626)\n",
      "[1040000/1915498] sparse_matrix shape is (1050000, 49626)\n",
      "[1050000/1915498] sparse_matrix shape is (1060000, 49626)\n",
      "[1060000/1915498] sparse_matrix shape is (1070000, 49626)\n",
      "[1070000/1915498] sparse_matrix shape is (1080000, 49626)\n",
      "[1080000/1915498] sparse_matrix shape is (1090000, 49626)\n",
      "[1090000/1915498] sparse_matrix shape is (1100000, 49626)\n",
      "[1100000/1915498] sparse_matrix shape is (1110000, 49626)\n",
      "[1110000/1915498] sparse_matrix shape is (1120000, 49626)\n",
      "[1120000/1915498] sparse_matrix shape is (1130000, 49626)\n",
      "[1130000/1915498] sparse_matrix shape is (1140000, 49626)\n",
      "[1140000/1915498] sparse_matrix shape is (1150000, 49626)\n",
      "[1150000/1915498] sparse_matrix shape is (1160000, 49626)\n",
      "[1160000/1915498] sparse_matrix shape is (1170000, 49626)\n",
      "[1170000/1915498] sparse_matrix shape is (1180000, 49626)\n",
      "[1180000/1915498] sparse_matrix shape is (1190000, 49626)\n",
      "[1190000/1915498] sparse_matrix shape is (1200000, 49626)\n",
      "[1200000/1915498] sparse_matrix shape is (1210000, 49626)\n",
      "[1210000/1915498] sparse_matrix shape is (1220000, 49626)\n",
      "[1220000/1915498] sparse_matrix shape is (1230000, 49626)\n",
      "[1230000/1915498] sparse_matrix shape is (1240000, 49626)\n",
      "[1240000/1915498] sparse_matrix shape is (1250000, 49626)\n",
      "[1250000/1915498] sparse_matrix shape is (1260000, 49626)\n",
      "[1260000/1915498] sparse_matrix shape is (1270000, 49626)\n",
      "[1270000/1915498] sparse_matrix shape is (1280000, 49626)\n",
      "[1280000/1915498] sparse_matrix shape is (1290000, 49626)\n",
      "[1290000/1915498] sparse_matrix shape is (1300000, 49626)\n",
      "[1300000/1915498] sparse_matrix shape is (1310000, 49626)\n",
      "[1310000/1915498] sparse_matrix shape is (1320000, 49626)\n",
      "[1320000/1915498] sparse_matrix shape is (1330000, 49626)\n",
      "[1330000/1915498] sparse_matrix shape is (1340000, 49626)\n",
      "[1340000/1915498] sparse_matrix shape is (1350000, 49626)\n",
      "[1350000/1915498] sparse_matrix shape is (1360000, 49626)\n",
      "[1360000/1915498] sparse_matrix shape is (1370000, 49626)\n",
      "[1370000/1915498] sparse_matrix shape is (1380000, 49626)\n",
      "[1380000/1915498] sparse_matrix shape is (1390000, 49626)\n",
      "[1390000/1915498] sparse_matrix shape is (1400000, 49626)\n",
      "[1400000/1915498] sparse_matrix shape is (1410000, 49626)\n",
      "[1410000/1915498] sparse_matrix shape is (1420000, 49626)\n",
      "[1420000/1915498] sparse_matrix shape is (1430000, 49626)\n",
      "[1430000/1915498] sparse_matrix shape is (1440000, 49626)\n",
      "[1440000/1915498] sparse_matrix shape is (1450000, 49626)\n",
      "[1450000/1915498] sparse_matrix shape is (1460000, 49626)\n",
      "[1460000/1915498] sparse_matrix shape is (1470000, 49626)\n",
      "[1470000/1915498] sparse_matrix shape is (1480000, 49626)\n",
      "[1480000/1915498] sparse_matrix shape is (1490000, 49626)\n",
      "[1490000/1915498] sparse_matrix shape is (1500000, 49626)\n",
      "[1500000/1915498] sparse_matrix shape is (1510000, 49626)\n",
      "[1510000/1915498] sparse_matrix shape is (1520000, 49626)\n",
      "[1520000/1915498] sparse_matrix shape is (1530000, 49626)\n",
      "[1530000/1915498] sparse_matrix shape is (1540000, 49626)\n",
      "[1540000/1915498] sparse_matrix shape is (1550000, 49626)\n",
      "[1550000/1915498] sparse_matrix shape is (1560000, 49626)\n",
      "[1560000/1915498] sparse_matrix shape is (1570000, 49626)\n",
      "[1570000/1915498] sparse_matrix shape is (1580000, 49626)\n",
      "[1580000/1915498] sparse_matrix shape is (1590000, 49626)\n",
      "[1590000/1915498] sparse_matrix shape is (1600000, 49626)\n",
      "[1600000/1915498] sparse_matrix shape is (1610000, 49626)\n",
      "[1610000/1915498] sparse_matrix shape is (1620000, 49626)\n",
      "[1620000/1915498] sparse_matrix shape is (1630000, 49626)\n",
      "[1630000/1915498] sparse_matrix shape is (1640000, 49626)\n",
      "[1640000/1915498] sparse_matrix shape is (1650000, 49626)\n",
      "[1650000/1915498] sparse_matrix shape is (1660000, 49626)\n",
      "[1660000/1915498] sparse_matrix shape is (1670000, 49626)\n",
      "[1670000/1915498] sparse_matrix shape is (1680000, 49626)\n",
      "[1680000/1915498] sparse_matrix shape is (1690000, 49626)\n",
      "[1690000/1915498] sparse_matrix shape is (1700000, 49626)\n",
      "[1700000/1915498] sparse_matrix shape is (1710000, 49626)\n",
      "[1710000/1915498] sparse_matrix shape is (1720000, 49626)\n",
      "[1720000/1915498] sparse_matrix shape is (1730000, 49626)\n",
      "[1730000/1915498] sparse_matrix shape is (1740000, 49626)\n",
      "[1740000/1915498] sparse_matrix shape is (1750000, 49626)\n",
      "[1750000/1915498] sparse_matrix shape is (1760000, 49626)\n",
      "[1760000/1915498] sparse_matrix shape is (1770000, 49626)\n",
      "[1770000/1915498] sparse_matrix shape is (1780000, 49626)\n",
      "[1780000/1915498] sparse_matrix shape is (1790000, 49626)\n",
      "[1790000/1915498] sparse_matrix shape is (1800000, 49626)\n",
      "[1800000/1915498] sparse_matrix shape is (1810000, 49626)\n",
      "[1810000/1915498] sparse_matrix shape is (1820000, 49626)\n",
      "[1820000/1915498] sparse_matrix shape is (1830000, 49626)\n",
      "[1830000/1915498] sparse_matrix shape is (1840000, 49626)\n",
      "[1840000/1915498] sparse_matrix shape is (1850000, 49626)\n",
      "[1850000/1915498] sparse_matrix shape is (1860000, 49626)\n",
      "[1860000/1915498] sparse_matrix shape is (1870000, 49626)\n",
      "[1870000/1915498] sparse_matrix shape is (1880000, 49626)\n",
      "[1880000/1915498] sparse_matrix shape is (1890000, 49626)\n",
      "[1890000/1915498] sparse_matrix shape is (1900000, 49626)\n",
      "[1900000/1915498] sparse_matrix shape is (1910000, 49626)\n",
      "[1910000/1915498] sparse_matrix shape is (1915498, 49626)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = get_douban()\n",
    "# str to int\n",
    "user_book = np.array([list(map(int, data)) for data in data['user_book']])\n",
    "# 濾除使用者評分小於三筆的資料\n",
    "filter_data = user_filter(user_book, 0)\n",
    "# user label encoder\n",
    "le = LabelEncoder()\n",
    "filter_data[:, 0] = le.fit_transform(filter_data[:, 0])\n",
    "filter_data[:, 0] += 1\n",
    "# item label encoder\n",
    "ile = LabelEncoder()\n",
    "filter_data[:, 1] = ile.fit_transform(filter_data[:, 1])\n",
    "filter_data[:, 1] += 1\n",
    "print(f\"使用者評分大於三次的共有：{filter_data.shape}\")\n",
    "# 取得business個數及users個數\n",
    "douban_users, books = np.unique(filter_data[:,0]), np.unique(filter_data[:,1])\n",
    "# 取得訓練資料及測試資料\n",
    "# douban_training_data,  douban_testing_data = training_testing(filter_data)\n",
    "print(\"users: \", len(douban_users))\n",
    "print(\"items: \", len(books))\n",
    "# generarte one hot encoding\n",
    "one_hot_x, y, add_fake_data = get_one_hot_feature(data,  'user_book')\n",
    "X_train_douban, X_test_douban, y_train_douban, y_test_douban = training_testing_XY(one_hot_x, y, random_state=int(config['model']['random_state']))\n",
    "training_index_douban, test_index_douban, _, _ = training_testing_XY(add_fake_data, y, random_state=int(config['model']['random_state']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100878b3-d40a-4d90-a916-6cdde0b47339",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. User-based Collaborative Filtering (U-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f550a514-2c1a-4a63-933f-ec3210319308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/baron/HW/Recommender_System/config.ini']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7207d08-17cf-46cc-836c-2a2928438f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Movielens:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1s4aqyjn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pious-energy-147</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/1s4aqyjn\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/1s4aqyjn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220426_002008-1s4aqyjn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1s4aqyjn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220426_002027-230u1y75</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/230u1y75\" target=\"_blank\">faithful-sky-148</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_movielens\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:00<00:00, 2077.16it/s]\n",
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:00<00:00, 8128.81it/s]\n",
      "UCF predicting cos score with 20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:03<00:00, 261.71it/s]\n",
      "UCF predicting pcc score with 20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:03<00:00, 270.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cos_rmse': 1.185866232216183, 'cos_recall@10': 0.5325477503204228, 'cos_NDCG@10': 0.7979352460670519, 'pcc_rmse': 1.1891346691404043, 'pcc_recall@10': 0.5237494417388817, 'pcc_NDCG@10': 0.7901763769411583}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cos_NDCG@10</td><td>0.79794</td></tr><tr><td>cos_recall@10</td><td>0.53255</td></tr><tr><td>cos_rmse</td><td>1.18587</td></tr><tr><td>pcc_NDCG@10</td><td>0.79018</td></tr><tr><td>pcc_recall@10</td><td>0.52375</td></tr><tr><td>pcc_rmse</td><td>1.18913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">faithful-sky-148</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/230u1y75\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/230u1y75</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220426_002027-230u1y75/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Yelp:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220426_002050-36089rkr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/36089rkr\" target=\"_blank\">eager-violet-7</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_yelp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7326/7326 [00:05<00:00, 1231.18it/s]\n",
      "data transfer user matrix: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7326/7326 [00:01<00:00, 5190.59it/s]\n",
      "UCF predicting cos score with 20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 7326/7326 [01:05<00:00, 111.44it/s]\n",
      "UCF predicting pcc score with 20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 7326/7326 [01:07<00:00, 108.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cos_rmse': 1.221072135063134, 'cos_recall@10': 0.05844828752340614, 'cos_NDCG@10': 0.094093591112611, 'pcc_rmse': 1.286335779093395, 'pcc_recall@10': 0.05850365429801504, 'pcc_NDCG@10': 0.09332834327436929}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cos_NDCG@10</td><td>0.09409</td></tr><tr><td>cos_recall@10</td><td>0.05845</td></tr><tr><td>cos_rmse</td><td>1.22107</td></tr><tr><td>pcc_NDCG@10</td><td>0.09333</td></tr><tr><td>pcc_recall@10</td><td>0.0585</td></tr><tr><td>pcc_rmse</td><td>1.28634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">eager-violet-7</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/36089rkr\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_yelp/runs/36089rkr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220426_002050-36089rkr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Douban Book:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220426_002521-21k1mk5i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_douban/runs/21k1mk5i\" target=\"_blank\">zesty-morning-16</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_douban\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11266/11266 [00:40<00:00, 276.23it/s]\n",
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 11266/11266 [00:09<00:00, 1237.99it/s]\n",
      "/home/baron/HW/Recommender_System/util/utility.py:95: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.reshape(total/exist_number, (-1, 1))\n",
      "UCF predicting cos score with 20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 11266/11266 [02:43<00:00, 68.92it/s]\n",
      "/home/baron/.local/lib/python3.8/site-packages/numpy/lib/function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/baron/.local/lib/python3.8/site-packages/numpy/lib/function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "UCF predicting pcc score with 20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 11266/11266 [02:44<00:00, 68.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cos_rmse': 0.9581459914947442, 'cos_recall@10': 0.17468548559149782, 'cos_NDCG@10': 0.314405097518533, 'pcc_rmse': 0.9643837375362581, 'pcc_recall@10': 0.1729472623095187, 'pcc_NDCG@10': 0.31003647129290945}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cos_NDCG@10</td><td>0.31441</td></tr><tr><td>cos_recall@10</td><td>0.17469</td></tr><tr><td>cos_rmse</td><td>0.95815</td></tr><tr><td>pcc_NDCG@10</td><td>0.31004</td></tr><tr><td>pcc_recall@10</td><td>0.17295</td></tr><tr><td>pcc_rmse</td><td>0.96438</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zesty-morning-16</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_douban/runs/21k1mk5i\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_douban/runs/21k1mk5i</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220426_002521-21k1mk5i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def user_sim_score(users, items, train_data, test_data, k=int(config['CF']['user_K'])):\n",
    "    # make matrix\n",
    "    user_matrix = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "    # 計算bias\n",
    "    bias_matrix = util.get_bias(user_matrix, users, items)\n",
    "    # 計算相似度\n",
    "#     cos, pcc = util.get_sim_array(user_matrix)\n",
    "#     cosine_dis = cos -  np.identity(len(users))\n",
    "#     pcc_dis = pcc -  np.identity(len(users))\n",
    "    \n",
    "#     sim = {\"cos\":cosine_dis, \"pcc\":pcc_dis}\n",
    "    sim = [\"cos\", \"pcc\"]\n",
    "    evaluation = dict()\n",
    "    for s in sim:\n",
    "        delta_list = list()\n",
    "        predict_array = np.zeros((test_matrix.shape))\n",
    "        # sim_dis = sim[s]\n",
    "        sim_array = util.get_sim_array(user_matrix, sim=s)\n",
    "        sim_dis = sim_array -  np.identity(len(users))\n",
    "        for i in tqdm(range(len(users)), desc=f\"UCF predicting {s} score with {k}\"):\n",
    "            # Suv: 取出前K個最相似的使用者相似度 ex:K=3, output=[0.378, 0.353, 0.336]\n",
    "            Suv = heapq.nlargest(k ,sim_dis[i])\n",
    "            # 若i不存在，則跳過\n",
    "            if np.isnan(sim_dis[i]).all():\n",
    "                continue\n",
    "            # top_sim_index: 取出與使用者i最為相似的前K個使用者 ex:K=3, output=[915, 406, 214]\n",
    "            sim_dis_idx = sim_dis[i].tolist()\n",
    "            top_sim_index = list(map(sim_dis_idx.index, heapq.nlargest(k,sim_dis[i])))\n",
    "            # recall\n",
    "            prediction = list()\n",
    "            # 計算相似使用者與使用者i的評分誤差\n",
    "            for item_idx in range(len(items)):\n",
    "                # 取得使用者i的評分(ground truth)\n",
    "                rth = test_matrix[i, item_idx]\n",
    "                # 如果使用者i有進行評分，則才納入計算RMSE\n",
    "                if rth != 0:\n",
    "                    # 之後需剔除對電影m未評分的相似使用者，因此先進行複製，才不會影響下一部電影的計算\n",
    "                    copy_Suv = copy.deepcopy(Suv)\n",
    "                    # R: 若相似使用者對電影 m 有評分則進行調整\n",
    "                    R = list()\n",
    "                    # 判斷相似使用者是否對電影ｍ有評分，若有評分則將原始評分減去該使用者對電影m的bias\n",
    "                    for c, j in enumerate(top_sim_index):\n",
    "                        if  test_matrix[j, item_idx] == 0:\n",
    "                            R.append(0)\n",
    "                            copy_Suv[c] = 0\n",
    "                        else:\n",
    "                            R.append(test_matrix[j, item_idx] - bias_matrix[j, item_idx])\n",
    "                    # 如果所有相似使用者都沒評分則跳過此次計算\n",
    "                    if sum(R) != 0:\n",
    "                        # 預測使用者i對於第m部電影的評分 + 使用者i對電影m的偏差\n",
    "                        Rui = predict(copy_Suv, R) + bias_matrix[i, item_idx]\n",
    "                        # 計算square error\n",
    "                        delta_list.append(util.se(rth, Rui))\n",
    "                        # 儲存預測結果, 並取四捨五入\n",
    "                        predict_array[i, item_idx] = Rui\n",
    "        # 各評估指標\n",
    "        evaluation[f'{s}_rmse']= util.rmse(delta_list)\n",
    "        evaluation[f'{s}_recall@10'] = recall_k(test_matrix, predict_array) \n",
    "        evaluation[f'{s}_NDCG@10']=ndcg_score(test_matrix, predict_array, k=10)\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = user_sim_score(len_users, movies, training_data, testing_data)\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "print(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = user_sim_score(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"UCF\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = user_sim_score(douban_users, books, douban_training_data, douban_testing_data)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76ff5f-9a67-4ed0-9a0f-f4f94add8de9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Item-based Collaborative Filtering (I-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "262b2f05-9860-4be9-b6be-88fb7e345438",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Yelp:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220426_001106-3ah7ff0w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/3ah7ff0w\" target=\"_blank\">twilight-brook-5</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_yelp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7326/7326 [00:05<00:00, 1263.44it/s]\n",
      "data transfer user matrix: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7326/7326 [00:01<00:00, 5283.99it/s]\n",
      "ICF predicting cos score with 20:   0%|                                                                                                       | 11/14127 [00:00<02:14, 104.91it/s]/home/baron/.local/lib/python3.8/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "ICF predicting cos score with 20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 14127/14127 [02:15<00:00, 103.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cos_rmse': 1.27345583713731, 'cos_recall@10': 0.00029988668138726505, 'cos_NDCG@10': 0.04081184719369428}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/.local/lib/python3.8/site-packages/numpy/lib/function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/baron/.local/lib/python3.8/site-packages/numpy/lib/function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "ICF predicting pcc score with 20:   0%|                                                                                                                 | 0/14127 [00:00<?, ?it/s]/home/baron/.local/lib/python3.8/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "ICF predicting pcc score with 20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 14127/14127 [02:30<00:00, 93.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cos_rmse': 1.27345583713731, 'cos_recall@10': 0.00029988668138726505, 'cos_NDCG@10': 0.04081184719369428, 'pcc_rmse': 1.2473677581038891, 'pcc_recall@10': 0.00029988668138726505, 'pcc_NDCG@10': 0.03952111283588812}\n",
      "{'cos_rmse': 1.27345583713731, 'cos_recall@10': 0.00029988668138726505, 'cos_NDCG@10': 0.04081184719369428, 'pcc_rmse': 1.2473677581038891, 'pcc_recall@10': 0.00029988668138726505, 'pcc_NDCG@10': 0.03952111283588812}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cos_NDCG@10</td><td>0.04081</td></tr><tr><td>cos_recall@10</td><td>0.0003</td></tr><tr><td>cos_rmse</td><td>1.27346</td></tr><tr><td>pcc_NDCG@10</td><td>0.03952</td></tr><tr><td>pcc_recall@10</td><td>0.0003</td></tr><tr><td>pcc_rmse</td><td>1.24737</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">twilight-brook-5</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/3ah7ff0w\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_yelp/runs/3ah7ff0w</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220426_001106-3ah7ff0w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "def item_sim_score(users, items, train_data, test_data, k=int(config['CF']['user_K'])):\n",
    "    # make matrix\n",
    "    user_matrix = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "    item_matrix = user_matrix.T \n",
    "    item_test = test_matrix.T\n",
    "    #item_test = sparse.csr_matrix(item_test)\n",
    "    del test_matrix\n",
    "    \n",
    "    # 計算bias\n",
    "    bias_matrix = util.get_bias(user_matrix, users, items)\n",
    "    item_bias = bias_matrix.T\n",
    "    del bias_matrix\n",
    "    del user_matrix\n",
    "    \n",
    "    # 計算相似度\n",
    "    #cos, pcc = util.get_sim_array(item_matrix)\n",
    "    #cosine_dis = cos -  np.identity(len(items))\n",
    "    #cosine_dis = sparse.csr_matrix(cosine_dis)\n",
    "    #pcc_dis = pcc -  np.identity(len(items))\n",
    "    #pcc_dis = sparse.csr_matrix(pcc_dis)\n",
    "    #sim = {\"cos\":cosine_dis, \"pcc\":pcc_dis}\n",
    "    sim = [\"cos\", \"pcc\"]\n",
    "    evaluation = dict()\n",
    "    for s in sim:\n",
    "        delta_list = list()\n",
    "        predict_array = np.zeros((item_test.shape))\n",
    "        # predict array to spase\n",
    "        predict_array = sparse.csr_matrix(predict_array)\n",
    "        sim_array = util.get_sim_array(item_matrix, sim=s)\n",
    "        sim_dis = sim_array -  np.identity(len(items))\n",
    "        # sim_dis = sim[s]\n",
    "        for i in tqdm(range(len(items)), desc=f\"ICF predicting {s} score with {k}\"):\n",
    "            # Siv: 取出前K個最相似的使用者相似度 ex:K=3, output=[0.378, 0.353, 0.336]\n",
    "            Siv = heapq.nlargest(k ,sim_dis[i])\n",
    "            # 若i不存在，則跳過\n",
    "            if np.isnan(sim_dis[i]).all():\n",
    "                continue\n",
    "            sim_dis[i][np.isnan(sim_dis[i])] = 0\n",
    "            # top_sim_index: 取出與使用者i最為相似的前K個使用者 ex:K=3, output=[915, 406, 214]\n",
    "            sim_dis_idx = sim_dis[i].tolist()\n",
    "            top_sim_index = list(map(sim_dis_idx.index, heapq.nlargest(k,sim_dis[i])))\n",
    "            # recall\n",
    "            prediction = list()\n",
    "            # 計算相似電影與電影i的評分誤差\n",
    "            for user_idx in range(len(users)):\n",
    "                # 取得項目i的評分(ground truth)\n",
    "                rth = item_test[i, user_idx]\n",
    "                # 如果使用者i有進行評分，則才納入計算RMSE\n",
    "                if rth != 0:\n",
    "                    # 之後需剔除對電影m未評分的相似使用者，因此先進行複製，才不會影響下一部電影的計算\n",
    "                    copy_Siv = copy.deepcopy(Siv)\n",
    "                    # R: 若相似使用者對電影 m 有評分則進行調整\n",
    "                    R = list()\n",
    "                    # 判斷相似使用者是否對電影ｍ有評分，若有評分則將原始評分減去該使用者對電影m的bias\n",
    "                    for c, j in enumerate(top_sim_index):\n",
    "                        if  item_test[j, user_idx] == 0:\n",
    "                            R.append(0)\n",
    "                            copy_Siv[c] = 0\n",
    "                        else:\n",
    "                            R.append(item_test[j, user_idx] - item_bias[j, user_idx])\n",
    "                    # 如果所有相似使用者都沒評分則跳過此次計算\n",
    "                    if sum(R) != 0:\n",
    "                        # 預測使用者i對於第m部電影的評分 + 使用者i對電影m的偏差\n",
    "                        Rui = predict(copy_Siv, R) + item_bias[i, user_idx]\n",
    "                        # 計算square error\n",
    "                        delta_list.append(util.se(rth, Rui))\n",
    "                        # 儲存預測結果, 並取四捨五入\n",
    "                        if np.isnan(Rui):\n",
    "                            Rui=0\n",
    "                        predict_array[i, user_idx] = Rui\n",
    "        \n",
    "        \n",
    "        # 各評估指標\n",
    "        delta_list = pd.Series(delta_list, dtype=object).fillna(0).tolist()\n",
    "        evaluation[f'{s}_rmse']= util.rmse(delta_list)\n",
    "        evaluation[f'{s}_recall@10'] = recall_k(item_test, predict_array) \n",
    "        evaluation[f'{s}_NDCG@10']=ndcg_score(item_test, predict_array.toarray(), k=10)\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"ICF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = item_sim_score(len_users, movies, training_data, testing_data)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"ICF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = item_sim_score(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"ICF\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = item_sim_score(douban_users, books, douban_training_data, douban_testing_data)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555705a2-5424-4878-8f2d-d0c87a5de4d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e38937-3e80-412a-9a71-4e68c459cf54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Yelp:\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: baron (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220426_142153-3bo2mes7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/3bo2mes7\" target=\"_blank\">polished-wood-11</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_yelp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7326/7326 [00:05<00:00, 1229.44it/s]\n",
      "data transfer user matrix: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7326/7326 [00:01<00:00, 5267.94it/s]\n",
      "/home/baron/HW/Recommender_System/util/utility.py:97: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.reshape(total/exist_number, (-1, 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100] gu=-17.692211953214663, bu=-0.35316409338309057, bi=-0.42891212553119235, testing error=7.955810027961804\n",
      "[9/100] gu=0.22247643735875575, bu=-0.5249549769093081, bi=-0.5294334345443557, testing error=2.7856859516406733\n",
      "[18/100] gu=0.5645280880312763, bu=-0.3987602788849377, bi=-0.47826749349173453, testing error=2.4400391999324773\n",
      "[27/100] gu=0.6296471911712056, bu=-0.29607102872930735, bi=-0.43956784560268547, testing error=2.225455054150513\n",
      "[36/100] gu=0.6675271534961927, bu=-0.21054304945703303, bi=-0.40919342149140897, testing error=2.069421622171588\n",
      "[45/100] gu=0.696019974791983, bu=-0.1367402157513489, bi=-0.3840343421814533, testing error=1.9492368710803363\n",
      "[54/100] gu=0.718561503467009, bu=-0.07161811549490976, bi=-0.3624030291507676, testing error=1.85335723796497\n",
      "[63/100] gu=0.7366663332675585, bu=-0.013315825634311385, bi=-0.34329892350712354, testing error=1.774947082203483\n",
      "[72/100] gu=0.7513500769514576, bu=0.03940763190864139, bi=-0.32607865411043097, testing error=1.7096103320303975\n",
      "[81/100] gu=0.763392387496884, bu=0.08743587855561198, bi=-0.31030391373975563, testing error=1.6543581934325322\n",
      "[90/100] gu=0.7734030885008962, bu=0.1314301261058962, bi=-0.2956634918190986, testing error=1.607071953528475\n",
      "[99/100] gu=0.7818479170712321, bu=0.17190486084726647, bi=-0.28192950214620266, testing error=1.5661992523383992\n",
      "{'rmse': 1.5661992523383992, 'recall@10': 0.00010830989091858658, 'NDCG@10': 0.10017196036954251}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.10017</td></tr><tr><td>recall@10</td><td>0.00011</td></tr><tr><td>rmse</td><td>1.5662</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polished-wood-11</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_yelp/runs/3bo2mes7\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_yelp/runs/3bo2mes7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220426_142153-3bo2mes7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Douban Book:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220426_143116-19nr3zxm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_douban/runs/19nr3zxm\" target=\"_blank\">daily-universe-17</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_douban\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11266/11266 [00:41<00:00, 271.45it/s]\n",
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 11266/11266 [00:09<00:00, 1218.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100] gu=-9.597066824455768, bu=-0.5293391024078583, bi=-0.3726751178259644, testing error=8.962208379542602\n",
      "[9/100] gu=1.1956220314470676, bu=-0.06118100359872818, bi=-0.21403802680386308, testing error=1.6258202692057764\n",
      "[27/100] gu=0.9814044789512671, bu=0.2622510980715556, bi=-0.07807743223896506, testing error=1.1731004316425557\n",
      "[36/100] gu=0.9607706421336191, bu=0.33401442293151506, bi=-0.04050527084410636, testing error=1.0610577338859197\n",
      "[45/100] gu=0.9478580213330579, bu=0.38749323080660947, bi=-0.009205749343726458, testing error=0.9795769773050196\n",
      "[54/100] gu=0.9380314766234154, bu=0.4300271302218055, bi=0.01858431125483541, testing error=0.9182242672602638\n",
      "[63/100] gu=0.9304860229270434, bu=0.46550845869612617, bi=0.044563510148037735, testing error=0.870964974358609\n",
      "[72/100] gu=0.925087542232725, bu=0.4963265653534839, bi=0.0699132232794492, testing error=0.8341293347715502\n",
      "[81/100] gu=0.921647147049031, bu=0.5240367233080105, bi=0.09546750191204248, testing error=0.8053684226212028\n",
      "[90/100] gu=0.9198582695337938, bu=0.5496450087455109, bi=0.12177737841447049, testing error=0.7830365499365523\n",
      "[99/100] gu=0.9193771057577214, bu=0.5737663204137037, bi=0.14914706695855026, testing error=0.7658581218753574\n",
      "{'rmse': 0.7658581218753574, 'recall@10': 0.00020347094198639262, 'NDCG@10': 0.14307665602323782}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.14308</td></tr><tr><td>recall@10</td><td>0.0002</td></tr><tr><td>rmse</td><td>0.76586</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">daily-universe-17</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_douban/runs/19nr3zxm\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_douban/runs/19nr3zxm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220426_143116-19nr3zxm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from util.mywandb import WandbLog\n",
    "\n",
    "# 進行測試資料驗證評估\n",
    "def test(test_data, p, q, gu=False, bu=False, bi=False):\n",
    "    rmse_test = list()\n",
    "\n",
    "    for test in test_data:\n",
    "        user = test[0] - 1\n",
    "        movie = test[1] - 1\n",
    "        # 判斷是否有bias\n",
    "        if gu and bu.any() and bi.any():\n",
    "            rmse_test.append(util.se(test[2], (np.dot(p[user], q[movie]) + gu + bu[user] + bi[movie])))\n",
    "        else:\n",
    "            rmse_test.append(util.se(test[2], (np.dot(p[user], q[movie]))))\n",
    "    return util.rmse(rmse_test)\n",
    "\n",
    "def execute_matrix_factorization(users, items, train_data, test_data):\n",
    "    # 存放測試資料集的rmse結果\n",
    "    MF_bias_testing = list()\n",
    "    # init evaluation\n",
    "    evaluation = dict()\n",
    "    user_item = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "\n",
    "    # init setting global mean\n",
    "    gu= util.get_u(user_item)\n",
    "    # init setting user mean as bias\n",
    "    bu = np.array([util.get_ubias(user_item, i) - gu for i in range(len(users))])\n",
    "    # init setting items mean as bias\n",
    "    bi = np.array([util.get_ibias(user_item, m) - gu for m in range(len(items))])\n",
    "\n",
    "    # init lentent vector\n",
    "    K = int(config[\"MF\"][\"latent_vector_number\"])\n",
    "    # init user lentent matrix\n",
    "    P = np.random.uniform(low=0, high=3, size=(users.max(), K))\n",
    "    # init items lentent matrix\n",
    "    Q = np.random.uniform(low=0, high=3, size=(items.max(), K))\n",
    "\n",
    "    # parameter\n",
    "    epochs = int(config[\"MF\"][\"epochs\"])\n",
    "    alpha = float(config[\"MF\"][\"alpha\"])\n",
    "    l = float(config[\"MF\"][\"learning_rate\"])\n",
    "\n",
    "    # 更新次數, init=100\n",
    "    for epoch in range(epochs):\n",
    "        # 存放 spuare error 結果\n",
    "        se_list = list()\n",
    "        # 針對user有評分過的rating位置進行更新(User Latent Matrix)\n",
    "        for j in range(len(users)):\n",
    "            # 找出被使用者j評分過的電影\n",
    "            # movie_index = [i for i, e in enumerate(user_item[j]) if e != 0]\n",
    "            movie_index = np.nonzero(user_item[j])[0]\n",
    "            for m in movie_index:\n",
    "                # 對u 做偏微分進行ＳＧＤ更新\n",
    "                tmp_gu = gu - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(gu))\n",
    "                # 對bu 做偏微分進行ＳＧＤ更新\n",
    "                tmp_bu = bu[j] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(bu[j]))\n",
    "                # 對bi 做偏微分進行ＳＧＤ更新\n",
    "                tmp_bi = bi[m] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) + l*(bi[m]))\n",
    "                # 若user item 有值則對Q的相對欄位進行SGD更新, 將更新後user latent matrix先暫存\n",
    "                tmp = Q[m] - alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) * P[j] + l*(Q[m]))\n",
    "                # 更新 movie latent matrix\n",
    "                P[j] -= alpha * (((np.dot(P[j], Q[m]) + gu + bu[j] + bi[m]) - user_item[j,m]) * Q[m] + l*(P[j]))\n",
    "                # 更新 user latent matrix\n",
    "                Q[m] = tmp\n",
    "                # 更新bias\n",
    "                gu = tmp_gu\n",
    "                bu[j] = tmp_bu\n",
    "                bi[m] = tmp_bi\n",
    "                # 計算ＳＥ\n",
    "                se_list.append(util.se(user_item[j, m], (np.dot(P[j], Q[m]) + gu + bu[j] + bi[m])))\n",
    "                \n",
    "        # 進行驗證資料測試\n",
    "        MF_bias_testing.append(test(test_data, P, Q, gu, bu, bi))\n",
    "        if epoch % 9 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] gu={gu}, bu={np.mean(bu)}, bi={np.mean(bi)}, testing error={MF_bias_testing[-1]}\")\n",
    "\n",
    "    # 各評估指標\n",
    "    evaluation['rmse']= MF_bias_testing[-1]\n",
    "    evaluation['recall@10'] = recall_k(test_matrix, np.dot(P, Q.T))\n",
    "    evaluation['NDCG@10'] = ndcg_score(test_matrix, np.dot(P, Q.T))\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_matrix_factorization(len_users, movies, training_data, testing_data)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nYelp:\\n==========\")\n",
    "wandb.init(project=config['general']['yelp'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"MF\")\n",
    "wandb_log = WandbLog()\n",
    "yelp_reuslt = execute_matrix_factorization(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "wandb_log.log_evaluation(yelp_reuslt)\n",
    "print(yelp_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"MF\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = execute_matrix_factorization(douban_users, books, douban_training_data, douban_testing_data)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345f5f0-cb98-436a-bc18-3d33e399f6cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. BPR-MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bd60d8-f070-4b2b-b73a-34d25167da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data input:\n",
    "[[user, item, rank], .....]\n",
    "'''\n",
    "def get_uij(data, users, items, sample_rate=1000):\n",
    "    for ii, user in enumerate(users):\n",
    "        items_data = data[data[:, 0]==user]\n",
    "        item_compare = list()\n",
    "        item_neg = list()\n",
    "        neg_count = 0\n",
    "        items_iter =[i for i in itertools.combinations(items, 2)]\n",
    "        items_iter = sample(items_iter, sample_rate)\n",
    "        for i, j in items_iter:\n",
    "            # if i exist items, but j not exsit items, i>j\n",
    "            if i in items_data[:, 1] and j not in items_data[:, 1]:\n",
    "                item_compare.append([user, i, j, 1])\n",
    "            # if j exist items, but i not exsit items, j>i\n",
    "            elif i not in items_data[:, 1] and j in items_data[:, 1]:\n",
    "                item_compare.append([user, j, i, 1])\n",
    "            # if i exist items, and also j exsit items, compare i and j\n",
    "            elif i in items_data[:, 1] and j in items_data[:, 1]:\n",
    "                ri = items_data[(items_data[:, 0]==user) & (items_data[:, 1]==i)][0, 2]\n",
    "                rj = items_data[(items_data[:, 0]==user) & (items_data[:, 1]==j)][0, 2]\n",
    "                if ri > rj:\n",
    "                    item_compare.append([user, i, j, 1])\n",
    "                elif ri < rj:\n",
    "                    item_compare.append([user, j, i, 1])\n",
    "                else:\n",
    "                    if neg_count < len(item_compare)//2:\n",
    "                        item_neg.append([user, j, i, 0])\n",
    "                        item_neg.append([user, i, j, 0])\n",
    "                        neg_count+=1\n",
    "            else:\n",
    "                if neg_count < len(item_compare)//2:\n",
    "                    item_neg.append([user, j, i, 0])\n",
    "                    item_neg.append([user, i, j, 0])\n",
    "                    neg_count+=1\n",
    "        if ii==0:\n",
    "            uij = np.array(item_compare)\n",
    "            uij_neg = np.array(item_neg)\n",
    "        else:\n",
    "            if len(item_compare)!= 0:\n",
    "                uij = np.vstack((uij, np.array(item_compare)))\n",
    "            if len(item_neg)!= 0:\n",
    "                uij_neg = np.vstack((uij_neg, np.array(item_neg)))\n",
    "        \n",
    "        if ii%300==0:\n",
    "            print(\"[{}/{}] uij_pos: {}, uij_neg: {}\".format(ii, len(users), uij.shape, uij_neg.shape))\n",
    "    \n",
    "    return uij, uij_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed929932-c906-4d20-b84c-3c8329c6d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Movielens:\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:151mibp2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polished-plant-158</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/151mibp2\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/151mibp2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_091110-151mibp2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:151mibp2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/Recommender_System/notebook/wandb/run-20220429_092021-gxyqs2fk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/gxyqs2fk\" target=\"_blank\">smooth-wood-159</a></strong> to <a href=\"https://wandb.ai/baron/recommendation-system_movielens\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:00<00:00, 2279.79it/s]\n",
      "data transfer user matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 943/943 [00:00<00:00, 9318.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/943] uij_pos: (217, 4), uij_neg: (216, 4)\n",
      "[300/943] uij_pos: (28474, 4), uij_neg: (28262, 4)\n",
      "[600/943] uij_pos: (58943, 4), uij_neg: (58548, 4)\n",
      "[900/943] uij_pos: (85906, 4), uij_neg: (85332, 4)\n",
      "uij_positive: (89328, 4), uij_negative: (88726, 4)\n",
      "[0/943] uij_pos: (66, 4), uij_neg: (66, 4)\n",
      "[300/943] uij_pos: (7351, 4), uij_neg: (7186, 4)\n",
      "[600/943] uij_pos: (15536, 4), uij_neg: (15218, 4)\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from util.mywandb import WandbLog\n",
    "import itertools\n",
    "from random import sample\n",
    "\n",
    "# 進行測試資料驗證評估\n",
    "def test(test_uij, users, items, p, q, gu=False, bu=False, bi=False):\n",
    "    rmse_test = list()\n",
    "    \n",
    "    \n",
    "    for u, i, j, rank in test_uij:\n",
    "        u_idx = u - 1\n",
    "        i_idx = i - 1\n",
    "        j_idx = j - 1\n",
    "        rui = np.dot(p[u_idx], q[i_idx])\n",
    "        ruj = np.dot(p[u_idx], q[j_idx])\n",
    "        x_uij =  rui - ruj\n",
    "        # sigmoid\n",
    "        exp_x = np.exp(-x_uij)\n",
    "        y_hat = 1/(1 + np.exp(exp_x))\n",
    "        \n",
    "        rmse_test.append(util.se(y_hat, rank))\n",
    "        \n",
    "    return util.rmse(rmse_test)\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "def execute_bpr_matrix_factorization(users, items, train_data, test_data):\n",
    "    # 存放測試資料集的rmse結果\n",
    "    MF_bias_testing = list()\n",
    "    # init evaluation\n",
    "    evaluation = dict()\n",
    "    user_item = get_user_item_matrix(train_data, users, items)\n",
    "    test_matrix = get_user_item_matrix(test_data, users, items)\n",
    "\n",
    "    # init setting global mean\n",
    "    gu= util.get_u(user_item)\n",
    "    # init setting user mean as bias\n",
    "    bu = np.array([util.get_ubias(user_item, i) - gu for i in range(len(users))])\n",
    "    # init setting items mean as bias\n",
    "    bi = np.array([util.get_ibias(user_item, m) - gu for m in range(len(items))])\n",
    "\n",
    "    # init lentent vector\n",
    "    K = int(config[\"MF\"][\"latent_vector_number\"])\n",
    "    # init user lentent matrix\n",
    "    # P = np.random.uniform(low=0, high=3, size=(users.max(), K))\n",
    "    P = np.random.randn(users.max(), K)/10\n",
    "    # init items lentent matrix\n",
    "    # Q = np.random.uniform(low=0, high=3, size=(items.max(), K))\n",
    "    Q = np.random.randn(items.max(), K)/10\n",
    "    \n",
    "    # get uij index\n",
    "    uij_pos, uij_neg = get_uij(train_data, users, items)\n",
    "    print(\"uij_positive: {}, uij_negative: {}\".format(uij_pos.shape, uij_neg.shape))\n",
    "    # uij = np.vstack(uij_pos, uij_neg)\n",
    "    test_uij_pos, test_uij_neg = get_uij(test_data, users, items)\n",
    "    print(\"testing uij_positive: {}, testing uij_negative: {}\".format(test_uij_pos.shape, test_uij_neg.shape))\n",
    "    test_uij = np.vstack((test_uij_pos, test_uij_neg))\n",
    "\n",
    "    # parameter\n",
    "    epochs = int(config[\"MF\"][\"epochs\"])\n",
    "    alpha = float(config[\"MF\"][\"alpha\"])\n",
    "    l = float(config[\"MF\"][\"learning_rate\"])\n",
    "\n",
    "    # 更新次數, init=100\n",
    "    for epoch in range(epochs):\n",
    "        # 針對user有評分過的rating位置進行更新(User Latent Matrix)\n",
    "        for u, i, j, rank in uij_pos:\n",
    "            # 計算x_uij\n",
    "            # rui = np.dot(P[u], Q[i]) + gu + bu[u] + bi[i]\n",
    "            # ruj = np.dot(P[u], Q[j]) + gu + bu[u] + bi[j]\n",
    "            i_idx = i-1\n",
    "            j_idx = j-1\n",
    "            u_idx = u-1\n",
    "            rui = np.dot(P[u_idx], Q[i_idx])\n",
    "            ruj = np.dot(P[u_idx], Q[j_idx])\n",
    "            x_uij =  rui - ruj\n",
    "            \n",
    "            # sigmoid\n",
    "            exp_x = np.exp(-x_uij)\n",
    "            partial_BPR = 1/(1 + np.exp(exp_x))\n",
    "            print(\"partial_BPR\", partial_BPR)\n",
    "            \n",
    "            # 更新 user latent matrix\n",
    "            print(P[u_idx])\n",
    "            print(P[u_idx].shape)\n",
    "            print(Q[i_idx]-Q[j_idx])\n",
    "            New_P= alpha * (partial_BPR*(Q[i_idx]-Q[j_idx]) + l*(P[u_idx]))\n",
    "            # 若user item 有值則對Q的相對欄位進行SGD更新, 將更新後user latent matrix先暫存\n",
    "            Q[i_idx] -=  alpha * (partial_BPR*P[u_idx] + l*(Q[i_idx]))\n",
    "            Q[j_idx] -=  alpha * (partial_BPR*-P[u_idx] + l*(Q[j_idx]))\n",
    "            # 更新 user latent matrix\n",
    "            print(New_P)\n",
    "            P[u_idx] = New_P\n",
    "            # # 更新bias\n",
    "            # # 對u 做偏微分進行ＳＧＤ更新\n",
    "            # gu = gu - alpha * ((rui - user_item[j,m]) + l*(gu))\n",
    "            # # 對bu 做偏微分進行ＳＧＤ更新\n",
    "            # bu[j] = bu[j] - alpha * ((rui - user_item[j,m]) + l*(bu[j]))\n",
    "            # # 對bi 做偏微分進行ＳＧＤ更新\n",
    "            # bi[m] = bi[m] - alpha * ((rui - user_item[j,m]) + l*(bi[m]))\n",
    "            break\n",
    "        break\n",
    "\n",
    "                \n",
    "        # 進行驗證資料測試\n",
    "        MF_bias_testing.append(test(test_uij, users, items, P, Q))\n",
    "        if epoch % 9 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] testing error={MF_bias_testing[-1]}\")\n",
    "    \n",
    "    rui = np.dot(P[u_idx], Q[i_idx])\n",
    "    ruj = np.dot(P[u_idx], Q[j_idx])\n",
    "    x_uij =  rui - ruj\n",
    "    print(\"{} user like item{} more than item{}, score is {}: \".format(u, i, j, x_uij))\n",
    "    # 各評估指標\n",
    "    evaluation['rmse']= MF_bias_testing[-1]\n",
    "    evaluation['recall@10'] = recall_k(test_matrix, np.dot(P, Q.T))\n",
    "    evaluation['NDCG@10'] = ndcg_score(test_matrix, np.dot(P, Q.T))\n",
    "    \n",
    "    return evaluation, P, Q\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-MF\")\n",
    "# wandb_log = WandbLog()\n",
    "movie_reuslt, P, Q = execute_bpr_matrix_factorization(len_users, movies, training_data, testing_data)\n",
    "print(movie_reuslt)\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_matrix_factorization(yelp_users, business, yelp_training_data, yelp_testing_data)\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"MF\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_matrix_factorization(douban_users, books, douban_training_data, douban_testing_data)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282af117-a92e-423b-9046-37d7649708f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac852f2d-998e-4559-a13e-6918187bde75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 1 FM Cross-Validation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouban\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     67\u001b[0m                         entity\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     68\u001b[0m                         group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m wandb_log \u001b[38;5;241m=\u001b[39m WandbLog()\n\u001b[0;32m---> 70\u001b[0m douban_reuslt \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_factorization_machine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_douban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_douban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_douban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_douban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_index_douban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index_douban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouban_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m wandb_log\u001b[38;5;241m.\u001b[39mlog_evaluation(douban_reuslt)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(douban_reuslt)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mexecute_factorization_machine\u001b[0;34m(X, y, X_test, y_test, training_index, test_index, users, items)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# define model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m fm \u001b[38;5;241m=\u001b[39m pywFM\u001b[38;5;241m.\u001b[39mFM(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m predict_values \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m     30\u001b[0m predict \u001b[38;5;241m=\u001b[39m generate_eval_array(predict_values, val_index, users, items)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pywFM/__init__.py:149\u001b[0m, in \u001b[0;36mFM.run\u001b[0;34m(self, x_train, y_train, x_test, y_test, x_validation_set, y_validation_set)\u001b[0m\n\u001b[1;32m    146\u001b[0m model_fd,model_path \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkstemp(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__temp_path)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# converts train and test data to libSVM format\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[43mdump_svmlight_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m dump_svmlight_file(x_test, y_test, test_path)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# builds arguments array\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/datasets/_svmlight_format_io.py:535\u001b[0m, in \u001b[0;36mdump_svmlight_file\u001b[0;34m(X, y, f, zero_based, comment, query_id, multilabel)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 535\u001b[0m         \u001b[43m_dump_svmlight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultilabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_based\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/datasets/_svmlight_format_io.py:405\u001b[0m, in \u001b[0;36m_dump_svmlight\u001b[0;34m(X, y, f, multilabel, one_based, comment, query_id)\u001b[0m\n\u001b[1;32m    402\u001b[0m     nz \u001b[38;5;241m=\u001b[39m X[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    403\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(nz)[\u001b[38;5;241m0\u001b[39m], X[i, nz])\n\u001b[0;32m--> 405\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_pattern\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mone_based\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multilabel:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_is_sp:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/datasets/_svmlight_format_io.py:405\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    402\u001b[0m     nz \u001b[38;5;241m=\u001b[39m X[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    403\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(nz)[\u001b[38;5;241m0\u001b[39m], X[i, nz])\n\u001b[0;32m--> 405\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(value_pattern \u001b[38;5;241m%\u001b[39m (\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mone_based\u001b[49m, x) \u001b[38;5;28;01mfor\u001b[39;00m j, x \u001b[38;5;129;01min\u001b[39;00m row)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multilabel:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_is_sp:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pywFM\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_factorization_machine(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} FM Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "\n",
    "        # reshape y\n",
    "        y_train = y_train.reshape(1, -1)[0]\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        fm = pywFM.FM(task='regression')\n",
    "\n",
    "        model = fm.run(X_train, y_train, X_val, y_val)\n",
    "        predict_values = model.predictions\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        kfold.append(util.rmse(predict_values - y_val))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# print(\"==========\\nMovielens:\\n==========\")\n",
    "# wandb.init(project=config['general']['movielens'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# movie_reuslt = execute_factorization_machine(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "\n",
    "# wandb_log.log_evaluation(movie_reuslt)\n",
    "# print(movie_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"==========\\nDouban Book:\\n==========\")\n",
    "wandb.init(project=config['general']['douban'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"FM\")\n",
    "wandb_log = WandbLog()\n",
    "douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "wandb_log.log_evaluation(douban_reuslt)\n",
    "print(douban_reuslt)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa178ba6-8d9b-49d5-8ffc-c6df7c4190b6",
   "metadata": {},
   "source": [
    "## 6. BPR-FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d92308f-b439-47b8-8d2f-47936b751813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.5321011130501097, 'recall@10': 0.03858693938782245, 'NDCG@10': 0.3017260812416944}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>NDCG@10</td><td>0.30173</td></tr><tr><td>recall@10</td><td>0.03859</td></tr><tr><td>rmse</td><td>0.5321</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">legendary-vortex-156</strong>: <a href=\"https://wandb.ai/baron/recommendation-system_movielens/runs/3ij0azoi\" target=\"_blank\">https://wandb.ai/baron/recommendation-system_movielens/runs/3ij0azoi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220429_015950-3ij0azoi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pywFM\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_bpr_factorization_machine(X, y, X_test, y_test, training_index, test_index, users, items):\n",
    "    rating_testing_array = generate_eval_array(y_test, test_index, users, items)\n",
    "    \n",
    "    # kfold = 5\n",
    "    kfold = list()\n",
    "    recall = list()\n",
    "    ndcg = list()\n",
    "    result = dict()\n",
    "    sum_predict_values = 0 \n",
    "    for i in range(5):\n",
    "        print(f\"Start {i} FM Cross-Validation\")\n",
    "        random_state = random.randint(0, 50)\n",
    "        X_train, X_val, y_train, y_val = training_testing_XY(X, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        _, val_index, _, _ = training_testing_XY(training_index, y, test_size=float(config[\"model\"][\"val_rate\"]), random_state=random_state)\n",
    "        \n",
    "        # reshape y\n",
    "        y_train = y_train.reshape(1, -1)[0]\n",
    "        y_test = y_test.reshape(1, -1)[0]\n",
    "        y_val = y_val.reshape(1, -1)[0]\n",
    "\n",
    "        # define model\n",
    "        fm = pywFM.FM(task='classification')\n",
    "\n",
    "        model = fm.run(X_train, y_train, X_val, y_val)\n",
    "        predict_values = model.predictions\n",
    "        predict = generate_eval_array(predict_values, val_index, users, items)\n",
    "        print(predict_values - y_val)\n",
    "        kfold.append(util.rmse(list(map(abs, predict_values - y_val))))\n",
    "        recall.append(recall_k(rating_testing_array, predict))\n",
    "        ndcg.append(ndcg_score(rating_testing_array, predict))\n",
    "        #sum_predict_values += predict_values\n",
    "        clear_output()\n",
    "\n",
    "    result['rmse'] = sum(kfold)/len(kfold) \n",
    "    result['recall@10'] = sum(recall)/len(recall)\n",
    "    result['NDCG@10'] = sum(ndcg)/len(ndcg)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"==========\\nMovielens:\\n==========\")\n",
    "wandb.init(project=config['general']['movielens'],\n",
    "                        entity=config['general']['entity'],\n",
    "                        group=\"BPR-FM\")\n",
    "wandb_log = WandbLog()\n",
    "movie_reuslt = execute_bpr_factorization_machine(X_train, y_train, X_test, y_test, training_index, test_index, len_users, movies)\n",
    "\n",
    "wandb_log.log_evaluation(movie_reuslt)\n",
    "print(movie_reuslt)\n",
    "wandb.finish()\n",
    "\n",
    "# print(\"==========\\nYelp:\\n==========\")\n",
    "# wandb.init(project=config['general']['yelp'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# yelp_reuslt = execute_factorization_machine(X_train_yelp, y_train_yelp, X_test_yelp, y_test_yelp, training_index_yelp, test_index_yelp, yelp_users, business)\n",
    "\n",
    "# wandb_log.log_evaluation(yelp_reuslt)\n",
    "# print(yelp_reuslt)\n",
    "# wandb.finish()\n",
    "\n",
    "# print(\"==========\\nDouban Book:\\n==========\")\n",
    "# wandb.init(project=config['general']['douban'],\n",
    "#                         entity=config['general']['entity'],\n",
    "#                         group=\"BPR-FM\")\n",
    "# wandb_log = WandbLog()\n",
    "# douban_reuslt = execute_factorization_machine(X_train_douban, y_train_douban, X_test_douban, y_test_douban, training_index_douban, test_index_douban, douban_users, books)\n",
    "# wandb_log.log_evaluation(douban_reuslt)\n",
    "# print(douban_reuslt)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0f5d2-7573-4e5a-a13c-ed3292d147c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
